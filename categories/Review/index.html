<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta name="theme-color" content="#123456"><title>Category: Review - Chen Yulin&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#6495ed"><meta name="application-name" content="Icarus - Hexo Theme"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="msapplication-TileColor" content="#6495ed"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icarus - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Chen Yulin&#039;s Blog"><meta property="og:url" content="http://chen-yulin.github.io/"><meta property="og:site_name" content="Chen Yulin&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://chen-yulin.github.io/img/og_image.png"><meta property="article:author" content="Chen Yulin"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://chen-yulin.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://chen-yulin.github.io"},"headline":"Chen Yulin's Blog","image":["http://chen-yulin.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Chen Yulin"},"publisher":{"@type":"Organization","name":"Chen Yulin's Blog","logo":{"@type":"ImageObject","url":{"light":"/img/cyllogo.png","dark":"/img/cyllogonight.png"}}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><svg style="position:absolute;width:0;height:0;" aria-hidden="true"><defs><filter id="liquid-glass-sm" x="-10%" y="-10%" width="120%" height="120%"><feTurbulence type="fractalNoise" baseFrequency="0.015" numOctaves="2" result="noise" seed="5"></feTurbulence><feDisplacementMap in="SourceGraphic" in2="noise" scale="2" xchannelselector="R" ychannelselector="G"></feDisplacementMap></filter></defs></svg><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><canvas id="flower"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/img/cyllogo.png" alt="Chen Yulin&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/cyllogonight.png" alt="Chen Yulin&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories/">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Review</a></li></ul></nav></div></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/06/%5BOBS%5DDeep%20Learning-BAGEL-Unified-Multimodal-Pretraining/"><img class="fill" src="/gallery/Research-paper.png" alt="BAGEL-Unified-Multimodal-Pretraining" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-05T21:30:00.000Z" title="2/6/2026, 5:30:00 AM">2026-02-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.518Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">10 minutes read (About 1443 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/06/%5BOBS%5DDeep%20Learning-BAGEL-Unified-Multimodal-Pretraining/">BAGEL-Unified-Multimodal-Pretraining</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h1 id="BAGEL-Emerging-Properties-in-Unified-Multimodal-Pretraining"><a href="#BAGEL-Emerging-Properties-in-Unified-Multimodal-Pretraining" class="headerlink" title="BAGEL: Emerging Properties in Unified Multimodal Pretraining"></a>BAGEL: Emerging Properties in Unified Multimodal Pretraining</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.14683">论文链接</a> | <a target="_blank" rel="noopener" href="https://bagel-ai.org/">项目主页</a></p>
<p><strong>作者</strong>：Chaorui Deng, Deyao Zhu, Kunchang Li 等 (ByteDance Seed)</p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>统一多模态理解与生成（Unified Multimodal Understanding and Generation）是当前AI领域的热点方向。GPT-4o、Gemini 2.0等闭源系统展现了强大能力，但开源模型与之仍存在显著差距。现有开源统一模型主要在图文配对数据上训练，缺乏对复杂多模态交错数据（Interleaved Data）的利用。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><ol>
<li>缩小开源统一多模态模型与闭源系统（GPT-4o、Gemini 2.0）之间的性能差距</li>
<li>解决现有模型架构中理解与生成模块之间的<strong>信息瓶颈（Bottleneck）</strong>问题</li>
<li>构建能够支持复杂多模态推理的大规模交错数据</li>
</ol>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="理解与生成之间的瓶颈（Bottleneck）"><a href="#理解与生成之间的瓶颈（Bottleneck）" class="headerlink" title="理解与生成之间的瓶颈（Bottleneck）"></a>理解与生成之间的瓶颈（Bottleneck）</h3><p>在采用 <strong>External Diffuser</strong> 架构的模型中，LLM&#x2F;VLM 与扩散模型通过轻量级适配器连接：</p>
<ul>
<li>语言模型生成少量潜在token作为”语义条件”</li>
<li>这些token被传递给扩散模块生成图像</li>
<li><strong>问题</strong>：LLM的丰富上下文被压缩到少量token中，导致信息损失，尤其影响长上下文多模态推理</li>
</ul>
<h3 id="Mixture-of-Transformer-Experts-MoT"><a href="#Mixture-of-Transformer-Experts-MoT" class="headerlink" title="Mixture-of-Transformer-Experts (MoT)"></a>Mixture-of-Transformer-Experts (MoT)</h3><p>与传统MoE不同，MoT复制整个Transformer层而非仅FFN：</p>
<ul>
<li><strong>理解专家</strong>：处理文本和ViT token</li>
<li><strong>生成专家</strong>：处理VAE token</li>
<li>两个专家通过<strong>共享自注意力</strong>在每层交互</li>
</ul>
<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>BAGEL采用<strong>无瓶颈的集成Transformer</strong>方案：</p>
<div class="post-content"><a href="/2026/02/06/[OBS]Deep Learning-BAGEL-Unified-Multimodal-Pretraining/Pasted_image_20260205155618.png" title="" title=" class="gallery-item"><img src="/2026/02/06/[OBS]Deep Learning-BAGEL-Unified-Multimodal-Pretraining/Pasted_image_20260205155618.png" alt="" title=""></a></div>

<p><strong>双视觉编码器</strong>：</p>
<ul>
<li><strong>理解编码器</strong>：SigLIP2-so400m&#x2F;14，捕获语义信息</li>
<li><strong>生成编码器</strong>：FLUX VAE，处理像素级信息</li>
</ul>
<h3 id="训练范式"><a href="#训练范式" class="headerlink" title="训练范式"></a>训练范式</h3><table>
<thead>
<tr>
<th>模态</th>
<th>方法</th>
<th>损失函数</th>
</tr>
</thead>
<tbody><tr>
<td>文本</td>
<td>Next-Token-Prediction</td>
<td>Cross-Entropy</td>
</tr>
<tr>
<td>视觉</td>
<td>Rectified Flow</td>
<td>MSE</td>
</tr>
</tbody></table>
<p>损失权重比：$\text{CE} : \text{MSE} &#x3D; 0.25 : 1$</p>
<h3 id="广义因果注意力（Generalized-Causal-Attention）"><a href="#广义因果注意力（Generalized-Causal-Attention）" class="headerlink" title="广义因果注意力（Generalized Causal Attention）"></a>广义因果注意力（Generalized Causal Attention）</h3><p>对于交错多图像生成：</p>
<ul>
<li><strong>Noised VAE tokens</strong>：用于Rectified-Flow训练</li>
<li><strong>Clean VAE tokens</strong>：作为后续生成的条件</li>
<li><strong>ViT tokens</strong>：统一输入格式，提升交错生成质量</li>
</ul>
<p>采用<strong>Diffusion Forcing</strong>策略，对不同图像添加独立噪声级别。</p>
<hr>
<h2 id="数据构���"><a href="#数据构���" class="headerlink" title="数据构���"></a>数据构���</h2><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>数据量</th>
<th>Token数</th>
</tr>
</thead>
<tbody><tr>
<td>纯文本</td>
<td>400M</td>
<td>0.4T</td>
</tr>
<tr>
<td>图文配对（理解）</td>
<td>500M</td>
<td>0.5T</td>
</tr>
<tr>
<td>图文配对（生成）</td>
<td>1600M</td>
<td>2.6T</td>
</tr>
<tr>
<td>交错理解数据</td>
<td>100M</td>
<td>0.5T</td>
</tr>
<tr>
<td>交错生成数据（视频）</td>
<td>45M</td>
<td>0.7T</td>
</tr>
<tr>
<td>交错生成数据（网页）</td>
<td>20M</td>
<td>0.4T</td>
</tr>
</tbody></table>
<h3 id="交错数据构建流程"><a href="#交错数据构建流程" class="headerlink" title="交错数据构建流程"></a>交错数据构建流程</h3><p><strong>视频数据</strong>：</p>
<ol>
<li>视频预处理（分割、裁剪、质量过滤）</li>
<li>使用蒸馏的小型VLM生成帧间描述</li>
<li>构建时序对齐的交错序列</li>
</ol>
<p><strong>网页数据</strong>：</p>
<ol>
<li>两阶段主题筛选（LLM + fastText）</li>
<li>质量过滤（分辨率、清晰度、相关性）</li>
<li>Caption-first策略：为每张图像生成描述并插入其前</li>
</ol>
<h3 id="推理增强数据（Reasoning-Augmented-Data）"><a href="#推理增强数据（Reasoning-Augmented-Data）" class="headerlink" title="推理增强数据（Reasoning-Augmented Data）"></a>推理增强数据（Reasoning-Augmented Data）</h3><p>受DeepSeek-R1启发，构建50万条推理增强样本：</p>
<ul>
<li>Text-to-Image生成</li>
<li>自由形式图像操作</li>
<li>概念性编辑</li>
</ul>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="涌现能力（Emerging-Properties）"><a href="#涌现能力（Emerging-Properties）" class="headerlink" title="涌现能力（Emerging Properties）"></a>涌现能力（Emerging Properties）</h3><p>论文定义：<strong>某能力在早期训练阶段不存在，但在后期训练中出现</strong></p>
<p>不同能力的涌现时间点（达到85%峰值性能所需token数）：</p>
<table>
<thead>
<tr>
<th>能力</th>
<th>涌现时间点</th>
</tr>
</thead>
<tbody><tr>
<td>多模态理解</td>
<td>~0.18T tokens</td>
</tr>
<tr>
<td>图像生成</td>
<td>~0.68T tokens</td>
</tr>
<tr>
<td>图像编辑</td>
<td>~2.64T tokens</td>
</tr>
<tr>
<td>智能编辑（复杂推理）</td>
<td>~3.61T tokens</td>
</tr>
</tbody></table>
<p><strong>关键发现</strong>：</p>
<ul>
<li>理解和生成能力最先收敛</li>
<li>编辑能力随后涌现</li>
<li>需要复杂推理的智能编辑能力最后涌现</li>
<li>ViT tokens对智能编辑至关重要（移除后性能下降16%）</li>
</ul>
<h3 id="架构对比实验"><a href="#架构对比实验" class="headerlink" title="架构对比实验"></a>架构对比实验</h3><p>在1.5B模型上对比Dense、MoE、MoT三种架构：</p>
<ul>
<li><strong>MoT在生成任务上优势最明显</strong></li>
<li>表明理解和生成可能需要不同的参数空间</li>
</ul>
<hr>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="多模态理解（7B参数）"><a href="#多模态理解（7B参数）" class="headerlink" title="多模态理解（7B参数）"></a>多模态理解（7B参数）</h3><table>
<thead>
<tr>
<th>基准</th>
<th>BAGEL</th>
<th>Janus-Pro</th>
<th>Qwen2.5-VL</th>
</tr>
</thead>
<tbody><tr>
<td>MMMU</td>
<td>58.6</td>
<td>41.8</td>
<td>49.3</td>
</tr>
<tr>
<td>MM-Vet</td>
<td>73.1</td>
<td>55.9</td>
<td>62.8</td>
</tr>
<tr>
<td>MathVista</td>
<td>69.3</td>
<td>54.7</td>
<td>68.2</td>
</tr>
<tr>
<td>MMVP</td>
<td>67.2</td>
<td>48.3</td>
<td>-</td>
</tr>
</tbody></table>
<h3 id="图像生成（GenEval）"><a href="#图像生成（GenEval）" class="headerlink" title="图像生成（GenEval）"></a>图像生成（GenEval）</h3><table>
<thead>
<tr>
<th>模型</th>
<th>Overall</th>
</tr>
</thead>
<tbody><tr>
<td>BAGEL (w&#x2F; rewriter)</td>
<td>0.88</td>
</tr>
<tr>
<td>BAGEL</td>
<td>0.82</td>
</tr>
<tr>
<td>Janus-Pro</td>
<td>0.80</td>
</tr>
<tr>
<td>FLUX.1-dev</td>
<td>0.82</td>
</tr>
<tr>
<td>SD3-Medium</td>
<td>0.74</td>
</tr>
</tbody></table>
<h3 id="智能编辑（IntelligentBench）"><a href="#智能编辑（IntelligentBench）" class="headerlink" title="智能编辑（IntelligentBench）"></a>智能编辑（IntelligentBench）</h3><table>
<thead>
<tr>
<th>模型</th>
<th>Score</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-4o</td>
<td>78.9</td>
</tr>
<tr>
<td>BAGEL w&#x2F; Self-CoT</td>
<td>55.3</td>
</tr>
<tr>
<td>BAGEL</td>
<td>44.9</td>
</tr>
<tr>
<td>Gemini 2.0</td>
<td>57.6</td>
</tr>
<tr>
<td>Step1X-Edit</td>
<td>14.9</td>
</tr>
</tbody></table>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><strong>无瓶颈架构</strong>：理解与生成模块间无损信息交互</li>
<li><strong>涌现能力</strong>：首次系统揭示统一多模态预训练的涌现规律</li>
<li><strong>开源贡献</strong>：发布代码、模型权重和数据构建协议</li>
<li><strong>推理增强</strong>：CoT显著提升复杂任务表现（WISE: +0.18, IntelligentBench: +10.4）</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li>与GPT-4o在智能编辑上仍有差距（55.3 vs 78.9）</li>
<li>模型规模相对较小（7B active &#x2F; 14B total）</li>
<li>训练计算成本高（需要大规模交错数据）</li>
</ul>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>统一多模态模型</strong>：</p>
<ul>
<li>Janus-Pro：采用离散视觉tokenizer的自回归方法</li>
<li>MetaQuery-XL：冻结预训练VLM backbone</li>
<li>Transfusion：统一AR和扩散的早期探索</li>
</ul>
<p><strong>视觉生成</strong>：</p>
<ul>
<li>FLUX.1-dev：当前SOTA扩散模型</li>
<li>SD3-Medium：Stable Diffusion系列</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ol>
<li><strong>更大规模训练</strong>：探索更大模型和更多数据下的涌现行为</li>
<li><strong>视频生成</strong>：论文展示了初步的视频生成能力，有待深入</li>
<li><strong>强化学习</strong>：无瓶颈架构为多模态RL提供了基础</li>
<li><strong>世界建模</strong>：导航、3D操作等世界建模任务的进一步探索</li>
</ol>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>Deng et al. (2025). Emerging Properties in Unified Multimodal Pretraining. arXiv:2505.14683</li>
<li>DeepSeek-AI (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</li>
<li>Esser et al. (2024). Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (SD3)</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/05/%5BOBS%5DDeep%20Learning-LingBot-VLA/"><img class="fill" src="/gallery/Research-paper.png" alt="LingBot-VLA" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-05T15:30:00.000Z" title="2/5/2026, 11:30:00 PM">2026-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.527Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">10 minutes read (About 1561 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/05/%5BOBS%5DDeep%20Learning-LingBot-VLA/">LingBot-VLA</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h1 id="LingBot-VLA-A-Pragmatic-VLA-Foundation-Model"><a href="#LingBot-VLA-A-Pragmatic-VLA-Foundation-Model" class="headerlink" title="LingBot-VLA: A Pragmatic VLA Foundation Model"></a>LingBot-VLA: A Pragmatic VLA Foundation Model</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18692">论文链接</a> | <a target="_blank" rel="noopener" href="https://github.com/robbyant/lingbot-vla">GitHub</a> | <a target="_blank" rel="noopener" href="https://huggingface.co/collections/robbyant/lingbot-vla">Checkpoints</a></p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>视觉-语言-动作（Vision-Language-Action, VLA）基础模型是机器人操作领域的新兴方法，通过大规模预训练使机器人能够执行由自然语言指令引导的多样化操作任务。然而，目前存在以下问题：</p>
<ul>
<li>缺乏关于真实机器人性能如何随预训练数据规模增长而变化的系统性实证研究</li>
<li>缺乏高效的训练代码库来支持大规模数据的扩展评估</li>
<li>缺乏跨多平台、多任务的系统性真实世界评估基准</li>
</ul>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><ol>
<li>探索 VLA 模型在真实世界机器人数据上的扩展规律（Scaling Law）</li>
<li>建立跨多平台、多任务的系统性真实世界评估基准</li>
<li>开发高效的大规模 VLA 训练代码库</li>
</ol>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Mixture-of-Transformers-MoT-架构"><a href="#Mixture-of-Transformers-MoT-架构" class="headerlink" title="Mixture-of-Transformers (MoT) 架构"></a>Mixture-of-Transformers (MoT) 架构</h3><p>将预训练的视觉语言模型（VLM）与动作生成模块（Action Expert）结合，通过共享自注意力机制实现跨模态统一建模。视觉-语言和动作模态通过独立的 Transformer 路径处理，既保留 VLM 的语义先验，又避免跨模态干扰。</p>
<h3 id="Flow-Matching"><a href="#Flow-Matching" class="headerlink" title="Flow Matching"></a>Flow Matching</h3><p>一种用于连续动作建模的生成方法，通过学习从噪声到目标动作的向量场来生成平滑的机器人控制信号。</p>
<h3 id="Blockwise-Causal-Attention"><a href="#Blockwise-Causal-Attention" class="headerlink" title="Blockwise Causal Attention"></a>Blockwise Causal Attention</h3><p>将序列划分为图像-指令块、状态块和动作块，应用因果掩码防止信息泄露，确保动作预测只能访问当前和历史观测信息。</p>
<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>LingBot-VLA 采用 MoT 架构，整合 Qwen2.5-VL 作为视觉语言骨干网络，配合独立的 Action Expert 模块：</p>
<p><strong>联合建模序列</strong>：<br>$$[O_t, A_t] &#x3D; [I_t^1, I_t^2, I_t^3, T_t, s_t, a_t, a_{t+1}, \ldots, a_{t+T-1}]$$</p>
<p>其中 $I_t^{1,2,3}$ 为三视角图像，$T_t$ 为任务指令，$s_t$ 为机器人状态，$A_t$ 为动作序列（chunk length &#x3D; 50）。<br>类似[[BAGEL-Unified-Multimodal-Pretraining]]</p>
<h3 id="Flow-Matching-目标函数"><a href="#Flow-Matching-目标函数" class="headerlink" title="Flow Matching 目标函数"></a>Flow Matching 目标函数</h3><p>定义概率路径通过线性插值：<br>$$A_{t,s} &#x3D; sA_t + (1-s)\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$</p>
<p>训练目标：<br>$$\mathcal{L}<em>{FM} &#x3D; \mathbb{E}</em>{s \sim U[0,1], A_t, \epsilon}|v_\theta(A_{t,s}, O_t, s) - (A_t - \epsilon)|^2$$</p>
<h3 id="深度信息蒸馏"><a href="#深度信息蒸馏" class="headerlink" title="深度信息蒸馏"></a>深度信息蒸馏</h3><p>通过可学习查询 $Q_t$ 与 LingBot-Depth 模型的深度 token $D_t$ 对齐，增强空间感知：<br>$$\mathcal{L}<em>{distill} &#x3D; \mathbb{E}</em>{Q_t}|Proj(Q_t) - D_t|$$</p>
<h3 id="训练效率优化"><a href="#训练效率优化" class="headerlink" title="训练效率优化"></a>训练效率优化</h3><ul>
<li><strong>FSDP 分布式策略</strong>：采用混合分片数据并行（HSDP），为 Action Expert 模块构建专用分片组</li>
<li><strong>算子级优化</strong>：使用 FlexAttention 优化稀疏注意力计算，torch.compile 进行算子融合</li>
<li><strong>混合精度</strong>：reduction 使用 float32，存储和通信使用 bfloat16</li>
</ul>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="扩展规律验证"><a href="#扩展规律验证" class="headerlink" title="扩展规律验证"></a>扩展规律验证</h3><ul>
<li>预训练数据从 3,000 小时扩展到 20,000 小时，下游任务成功率持续显著提升</li>
<li>在 20,000 小时数据量下仍未出现饱和迹象，表明 VLA 性能持续受益于数据量增加</li>
<li>首次提供了真实世界机器人学习中有利扩展特性的实证证据</li>
</ul>
<h3 id="数据效率"><a href="#数据效率" class="headerlink" title="数据效率"></a>数据效率</h3><ul>
<li>仅使用 80 个演示即可超越 π0.5 使用 130 个演示的性能</li>
<li>随着后训练数据量增加，与基线的性能差距进一步扩大</li>
</ul>
<hr>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><h3 id="预训练数据"><a href="#预训练数据" class="headerlink" title="预训练数据"></a>预训练数据</h3><ul>
<li><strong>规模</strong>：约 20,000 小时真实世界操作数据</li>
<li><strong>来源</strong>：9 种双臂机器人平台（AgiBot G1、AgileX、Galaxea R1Lite&#x2F;R1Pro、Realman Rs-02、Leju KUAVO、Qinglong、ARX Lift2、Bimanual Franka）</li>
</ul>
<h3 id="评估基准"><a href="#评估基准" class="headerlink" title="评估基准"></a>评估基准</h3><ul>
<li><strong>GM-100 基准</strong>：100 个操作任务，39,000 个专家演示</li>
<li><strong>评估规模</strong>：3 个机器人平台，每任务 130 个后训练 episode，共 22,500 次试验</li>
<li><strong>对比方法</strong>：π0.5、GR00T N1.6、WALL-OSS</li>
</ul>
<h3 id="真实世界评估结果"><a href="#真实世界评估结果" class="headerlink" title="真实世界评估结果"></a>真实世界评估结果</h3><table>
<thead>
<tr>
<th>方法</th>
<th>平均成功率(SR)</th>
<th>平均进度分(PS)</th>
</tr>
</thead>
<tbody><tr>
<td>WALL-OSS</td>
<td>4.05%</td>
<td>10.35%</td>
</tr>
<tr>
<td>GR00T N1.6</td>
<td>7.59%</td>
<td>15.99%</td>
</tr>
<tr>
<td>π0.5</td>
<td>13.02%</td>
<td>27.65%</td>
</tr>
<tr>
<td>LingBot-VLA w&#x2F;o depth</td>
<td>15.74%</td>
<td>33.69%</td>
</tr>
<tr>
<td><strong>LingBot-VLA w&#x2F; depth</strong></td>
<td><strong>17.30%</strong></td>
<td><strong>35.41%</strong></td>
</tr>
</tbody></table>
<h3 id="仿真评估结果（RoboTwin-2-0）"><a href="#仿真评估结果（RoboTwin-2-0）" class="headerlink" title="仿真评估结果（RoboTwin 2.0）"></a>仿真评估结果（RoboTwin 2.0）</h3><table>
<thead>
<tr>
<th>方法</th>
<th>Clean 场景 SR</th>
<th>Randomized 场景 SR</th>
</tr>
</thead>
<tbody><tr>
<td>π0.5</td>
<td>82.74%</td>
<td>76.76%</td>
</tr>
<tr>
<td>LingBot-VLA w&#x2F;o depth</td>
<td>86.50%</td>
<td>85.34%</td>
</tr>
<tr>
<td><strong>LingBot-VLA w&#x2F; depth</strong></td>
<td><strong>88.56%</strong></td>
<td><strong>86.68%</strong></td>
</tr>
</tbody></table>
<h3 id="训练吞吐量"><a href="#训练吞吐量" class="headerlink" title="训练吞吐量"></a>训练吞吐量</h3><ul>
<li>实现 261 samples&#x2F;s&#x2F;GPU（8-GPU 配置）</li>
<li>相比 StarVLA、DexBotic、OpenPI 提升 1.5~2.8 倍</li>
<li>在 256 GPU 规模下仍保持接近线性扩展</li>
</ul>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li>首次在大规模真实世界数据上验证 VLA 扩展规律</li>
<li>显著优于现有 SOTA 方法的多平台泛化能力</li>
<li>高效的训练代码库，支持大规模分布式训练</li>
<li>开源代码、模型和基准数据</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li>目前仅支持双臂机器人配置</li>
<li>评估主要集中在桌面操作任务</li>
<li>深度信息蒸馏依赖额外的 LingBot-Depth 模型</li>
</ul>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="Foundation-VLA"><a href="#Foundation-VLA" class="headerlink" title="Foundation VLA"></a>Foundation VLA</h3><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/xxx">π0</a>：Vision-language-action flow model for general robot control</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/xxx">π0.5</a>：VLA model with open-world generalization</li>
<li><a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/gear/gr00t-n1_6/">GR00T N1.6</a>：Open foundation model for generalist humanoid robots</li>
</ul>
<h3 id="Spatial-VLA"><a href="#Spatial-VLA" class="headerlink" title="Spatial VLA"></a>Spatial VLA</h3><ul>
<li>SpatialVLA：探索 VLA 模型的空间表示</li>
<li>Spatial Forcing：通过对齐策略增强 VLA 空间理解</li>
<li>GeoVLA：赋能 VLA 模型 3D 表示能力</li>
</ul>
<h3 id="高效训练框架"><a href="#高效训练框架" class="headerlink" title="高效训练框架"></a>高效训练框架</h3><ul>
<li>OpenPI：支持 JAX 和 PyTorch 的 π 系列模型训练框架</li>
<li>StarVLA：VLA 和 VLM 联合训练的模块化代码库</li>
<li>DexBotic：统一高效��� VLA 开发生命周期解决方案</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ol>
<li><strong>扩展机器人类型</strong>：整合单臂和移动机器人数据，支持更多样化的操作能力</li>
<li><strong>非约束环境</strong>：探索在非结构化环境中的移动操作能力</li>
<li><strong>持续扩展</strong>：进一步扩大预训练数据规模，探索扩展规律的上限</li>
</ol>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>Black et al. (2025). π0: A vision-language-action flow model for general robot control. RSS.</li>
<li>Black et al. (2025). π0.5: A vision-language-action model with open-world generalization. CoRL.</li>
<li>Bjorck et al. (2025). GR00T N1: An open foundation model for generalist humanoid robots. arXiv.</li>
<li>Bai et al. (2025). Qwen2.5-VL technical report. arXiv.</li>
<li>Lipman et al. (2022). Flow matching for generative modeling. arXiv.</li>
<li>Wang et al. (2026). The Great March 100: 100 detail-oriented tasks for evaluating embodied AI agents.</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/05/%5BOBS%5DDeep%20Learning-Transformer-Mixture-of-Experts-Survey/"><img class="fill" src="/gallery/LLM.png" alt="Mixture-of-Experts-Survey" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-05T15:30:00.000Z" title="2/5/2026, 11:30:00 PM">2026-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.505Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">11 minutes read (About 1682 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/05/%5BOBS%5DDeep%20Learning-Transformer-Mixture-of-Experts-Survey/">Mixture-of-Experts-Survey</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2026/02/05/[OBS]Deep Learning-Transformer-Mixture-of-Experts-Survey/Pasted_image_20260205143754.png" title="" title=" class="gallery-item"><img src="/2026/02/05/[OBS]Deep Learning-Transformer-Mixture-of-Experts-Survey/Pasted_image_20260205143754.png" alt="" title=""></a></div>
<div class="post-content"><a href="/2026/02/05/[OBS]Deep Learning-Transformer-Mixture-of-Experts-Survey/Pasted_image_20260205143818.png" title="" title=" class="gallery-item"><img src="/2026/02/05/[OBS]Deep Learning-Transformer-Mixture-of-Experts-Survey/Pasted_image_20260205143818.png" alt="" title=""></a></div>
# A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications

<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.07137">arXiv:2503.07137</a></p>
<p><strong>作者</strong>：Siyuan Mu (四川农业大学), Sen Lin (休斯顿大学)</p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>随着AI基础大模型的快速发展，现代数据集变得越来越多样化和复杂，包含多模态数据（文本、图像、音频）和复杂结构（图、层次关系）。这给大模型发展带来两大挑战：</p>
<ol>
<li><strong>计算资源消耗巨大</strong>：训练和部署大模型的计算成本呈指数增长</li>
<li><strong>异构数据拟��困难</strong>：在单一模型中整合冲突或异构知识变得困难，导致训练不稳定和性能次优</li>
</ol>
<p>混合专家模型（Mixture of Experts, MoE）通过动态选择和激活最相关的子模型来处理输入数据，成为解决这些挑战的有效方案。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><ul>
<li>填补现有MoE综述的空白（过时或缺乏关键领域讨论）</li>
<li>全面总结MoE的基础设计、算法、理论和应用四大关键组件</li>
<li>为研究者提供系统性参考，激发进一步研究</li>
</ul>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="MoE基本原理"><a href="#MoE基本原理" class="headerlink" title="MoE基本原理"></a>MoE基本原理</h3><p>MoE采用”分而治之”（divide and conquer）策略，与传统密集模型不同：</p>
<ul>
<li><strong>传统模型</strong>：对每个输入激活所有参数</li>
<li><strong>MoE模型</strong>：根据输入特征动态选择和激活最相关的参数子集</li>
</ul>
<h3 id="MoE层数学表示"><a href="#MoE层数学表示" class="headerlink" title="MoE层数学表示"></a>MoE层数学表示</h3><p>$$<br>\text{MoE}(x) &#x3D; \sum_{i \in \mathcal{I}_D} w_i M_i(x)<br>$$<br>其中 $\mathcal{I}_D$ 是被选中专家的索引集，$w_i$ 是第 $i$ 个专家的权重，$M_i(x)$ 是专家网络输出。</p>
<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="1-门控函数（Gating-Function）"><a href="#1-门控函数（Gating-Function）" class="headerlink" title="1. 门控函数（Gating Function）"></a>1. 门控函数（Gating Function）</h3><h4 id="线性门控（Softmax-Gating）"><a href="#线性门控（Softmax-Gating）" class="headerlink" title="线性门控（Softmax Gating）"></a>线性门控（Softmax Gating）</h4><p>$$<br>G(x)<em>i &#x3D; \text{softmax}(\text{TopK}(g(x) + R</em>{noise}, k))<em>i<br>$$<br>其中 $g(x)$ 是线性函数计算的门控值，$R</em>{noise}$ 是鼓励专家探索的噪声。</p>
<h4 id="非线性门控"><a href="#非线性门控" class="headerlink" title="非线性门控"></a>非线性门控</h4><ul>
<li><strong>余弦门控</strong>（GMoE）：<br>$$<br>G(x) &#x3D; \text{TopK}\left(\text{softmax}\left(\frac{E^T W_{linear} x}{\tau |W_{linear} x| |E|}\right)\right)<br>$$</li>
<li><strong>指数族分布门控</strong></li>
<li><strong>Soft MoE</strong>：使用加权平均而非离散分配</li>
</ul>
<h3 id="2-专家网络（Expert-Network）"><a href="#2-专家网络（Expert-Network）" class="headerlink" title="2. 专家网络（Expert Network）"></a>2. 专家网络（Expert Network）</h3><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>FFN专家</td>
<td>替换Transformer中的FFN层</td>
<td>最常用，如Switch Transformer</td>
</tr>
<tr>
<td>MoA（混合注意力）</td>
<td>将MoE应用于注意力模块</td>
<td>图像生成、多模态任务</td>
</tr>
<tr>
<td>CNN专家</td>
<td>将MoE应用于CNN层</td>
<td>计算机视觉任务</td>
</tr>
</tbody></table>
<h3 id="3-路由策略（Routing-Strategy）"><a href="#3-路由策略（Routing-Strategy）" class="headerlink" title="3. 路由策略（Routing Strategy）"></a>3. 路由策略（Routing Strategy）</h3><ul>
<li><strong>Token级路由</strong>：基于token表示进行路由决策（最经典）</li>
<li><strong>模态级路由</strong>：根据数据模态进行路由（多模态任务）</li>
<li><strong>任务级路由</strong>：根据任务ID确定路由（多任务学习）</li>
</ul>
<h3 id="4-训练策略"><a href="#4-训练策略" class="headerlink" title="4. 训练策略"></a>4. 训练策略</h3><h4 id="负载均衡损失（Switch-Transformer）"><a href="#负载均衡损失（Switch-Transformer）" class="headerlink" title="负载均衡损失（Switch Transformer）"></a>负载均衡损失（Switch Transformer）</h4><p>$$<br>\mathcal{L}<em>{aux} &#x3D; \alpha \cdot N \cdot \sum</em>{i&#x3D;1}^{N} f_i \cdot Q_i<br>$$<br>其中 $f_i$ 是分配给专家 $i$ 的token比例，$Q_i$ 是路由概率比例。</p>
<hr>
<h2 id="MoA（Mixture-of-Attention）详解"><a href="#MoA（Mixture-of-Attention）详解" class="headerlink" title="MoA（Mixture-of-Attention）详解"></a>MoA（Mixture-of-Attention）详解</h2><h3 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h3><p>MoA将MoE机制引入多头注意力模块，每个注意力头视为一个”专家”。</p>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol>
<li>输入token进入MoA层</li>
<li>门控网络计算每个注意力头的重要性分数</li>
<li>选择TopK个最相关的注意力头</li>
<li>仅计算被选中头的输出并加权求和</li>
</ol>
<h3 id="代码实现核心"><a href="#代码实现核心" class="headerlink" title="代码实现核心"></a>代码实现核心</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MixtureOfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads=<span class="number">8</span>, head_dim=<span class="number">64</span>, top_k=<span class="number">2</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.attention_experts = nn.ModuleList([</span><br><span class="line">            AttentionExpert(d_model, head_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)</span><br><span class="line">        ])</span><br><span class="line">        <span class="variable language_">self</span>.router = AttentionRouter(d_model, num_heads, top_k)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        weights, indices, logits = <span class="variable language_">self</span>.router(x)</span><br><span class="line">        output = torch.zeros_like(x)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.top_k):</span><br><span class="line">            <span class="keyword">for</span> head_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_heads):</span><br><span class="line">                mask_k = (indices[:, :, k] == head_idx)</span><br><span class="line">                <span class="keyword">if</span> mask_k.<span class="built_in">any</span>():</span><br><span class="line">                    head_output = <span class="variable language_">self</span>.attention_experts[head_idx](x)</span><br><span class="line">                    output[mask_k] += weights[:, :, k][mask_k].unsqueeze(-<span class="number">1</span>) * head_output[mask_k]</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h3 id="MoA-vs-标准多头注意力"><a href="#MoA-vs-标准多头注意力" class="headerlink" title="MoA vs 标准多头注意力"></a>MoA vs 标准多头注意力</h3><table>
<thead>
<tr>
<th>特性</th>
<th>标准多头注意力</th>
<th>MoA</th>
</tr>
</thead>
<tbody><tr>
<td>头激活</td>
<td>所有头同时激活</td>
<td>动态选择部分头</td>
</tr>
<tr>
<td>计算开销</td>
<td>与头数量成正比</td>
<td>仅计算被选中的头</td>
</tr>
<tr>
<td>可扩展性</td>
<td>增加头数直接增加计算量</td>
<td>可扩展更多头而不显著增加计算</td>
</tr>
</tbody></table>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="算法应用领域"><a href="#算法应用领域" class="headerlink" title="算法应用领域"></a>算法应用领域</h3><table>
<thead>
<tr>
<th>领域</th>
<th>代表性工作</th>
<th>核心贡献</th>
</tr>
</thead>
<tbody><tr>
<td>持续学习</td>
<td>CN-DPM, Lifelong-MoE, PMoE</td>
<td>缓解灾难性遗忘</td>
</tr>
<tr>
<td>元学习</td>
<td>MoE-NPs, MixER, Meta-DMoE</td>
<td>增强快速适应能力</td>
</tr>
<tr>
<td>多任务学习</td>
<td>MMoE, MOOR, TaskExpert</td>
<td>解耦任务、减少干扰</td>
</tr>
<tr>
<td>强化学习</td>
<td>MMRL, MACE, MENTOR</td>
<td>处理非平稳环境</td>
</tr>
</tbody></table>
<h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><table>
<thead>
<tr>
<th>领域</th>
<th>任务</th>
<th>代表性工作</th>
</tr>
</thead>
<tbody><tr>
<td>计算机视觉</td>
<td>图像分类</td>
<td>V-MoE, Soft MoE, CLIP-MoE</td>
</tr>
<tr>
<td></td>
<td>目标检测</td>
<td>MoCaE, DAMEX</td>
</tr>
<tr>
<td></td>
<td>语义分割</td>
<td>DeepMoE, Swin2-MoSE</td>
</tr>
<tr>
<td></td>
<td>图像生成</td>
<td>RAPHAEL, MEGAN</td>
</tr>
<tr>
<td>自然语言处理</td>
<td>NLU</td>
<td>GLaM, MoE-LPR</td>
</tr>
<tr>
<td></td>
<td>机器翻译</td>
<td>GShard, NLLB</td>
</tr>
<tr>
<td></td>
<td>多模态融合</td>
<td>LIMoE, LLaVA-MoLE</td>
</tr>
</tbody></table>
<h3 id="代表性大模型"><a href="#代表性大模型" class="headerlink" title="代表性大模型"></a>代表性大模型</h3><table>
<thead>
<tr>
<th>模型</th>
<th>参数规模</th>
<th>主要成就</th>
</tr>
</thead>
<tbody><tr>
<td>Switch Transformer</td>
<td>万亿级</td>
<td>预训练速度比T5-Base快7倍</td>
</tr>
<tr>
<td>GLaM</td>
<td>万亿级</td>
<td>增强上下文信息利用能力</td>
</tr>
<tr>
<td>Mixtral 8×7B</td>
<td>470亿（激活130亿）</td>
<td>高参数效率</td>
</tr>
<tr>
<td>DeepSeek系列</td>
<td>-</td>
<td>多项基准SOTA</td>
</tr>
</tbody></table>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><strong>计算效率</strong>：通过稀疏激活显著降低计算成本</li>
<li><strong>模型容量</strong>：可扩展至万亿参数而不成比例增加计算</li>
<li><strong>专业化学习</strong>：不同专家专注于不同知识领域</li>
<li><strong>可解释性</strong>：通过分析专家分配机制理解模型行为</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li><strong>训练稳定性</strong>：动态专家选择可能导致负载不均衡和模型崩溃</li>
<li><strong>系统复杂性</strong>：All-to-All通信模式增加系统设计难度</li>
<li><strong>内存需求</strong>：多专家参数存储可能超出单设备容量</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ol>
<li><strong>训练稳定性与负载均衡</strong>：开发更鲁棒的训练策略</li>
<li><strong>训练与系统效率</strong>：优化硬件-软件协同设计</li>
<li><strong>架构设计</strong>：使用元学习或强化学习动态调整专家数量</li>
<li><strong>理论发展</strong>：深入理解专家路由决策和聚类机制</li>
<li><strong>定制算法设计</strong>：探索MoE与对比学习、自监督学习的结合</li>
<li><strong>新应用领域</strong>：医疗、机器人、自动驾驶、教育、金融</li>
</ol>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">Switch Transformer</a>：首个万亿参数MoE模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.05974">V-MoE</a>：视觉领域MoE应用</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04088">Mixtral 8×7B</a>：高效MoE语言模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.00951">Soft MoE</a>：软分配MoE新范式</li>
<li>[[BAGEL-Unified-Multimodal-Pretraining]]</li>
</ul>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>Fedus et al. (2022). Switch Transformers: Scaling to Trillion Parameter Models. JMLR.</li>
<li>Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. arXiv.</li>
<li>Riquelme et al. (2021). Scaling Vision with Sparse Mixture of Experts. NeurIPS.</li>
<li>Jiang et al. (2024). Mixtral of Experts. arXiv.</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/03/%5BOBS%5DDeep%20Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/"><img class="fill" src="/gallery/Research-paper.png" alt="UniDiffuser" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-03T12:30:00.000Z" title="2/3/2026, 8:30:00 PM">2026-02-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.506Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">15 minutes read (About 2285 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/03/%5BOBS%5DDeep%20Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/">UniDiffuser</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h1 id="UniDiffuser-One-Transformer-Fits-All-Distributions-in-Multi-Modal-Diffusion-at-Scale"><a href="#UniDiffuser-One-Transformer-Fits-All-Distributions-in-Multi-Modal-Diffusion-at-Scale" class="headerlink" title="UniDiffuser: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale"></a>UniDiffuser: One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.06555">论文链接</a> | <a target="_blank" rel="noopener" href="https://github.com/thu-ml/unidiffuser">GitHub</a></p>
<p><strong>作者</strong>：Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu<br><strong>发表于</strong>：ICML 2023</p>
<hr>
<div class="post-content"><a href="/2026/02/03/[OBS]Deep Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/Pasted_image_20260203153540.png" title="" title=" class="gallery-item"><img src="/2026/02/03/[OBS]Deep Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/Pasted_image_20260203153540.png" alt="" title=""></a></div>

<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>扩散模型（Diffusion Models）在图像生成领域取得了巨大成功，但现有方法主要基于 U-Net 架构。随着 Transformer 在各领域的成功应用，如何将 Transformer 有效地应用于多模态扩散模型成为一个重要研究方向。现有的多模态生成方法通常需要为不同任务（文生图、图生文、联合生成等）设计不同的模型架构。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><ul>
<li><strong>多模态联合建模</strong>：用一个统一的框架同时处理图像、文本等多种模态的生成任务</li>
<li><strong>任务统一</strong>：用单一模型支持文生图、图生文、联合生成、无条件生成等多种任务</li>
<li><strong>架构创新</strong>：设计适合多模态扩散的 Transformer 架构（U-ViT）</li>
</ul>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="扩散模型基础"><a href="#扩散模型基础" class="headerlink" title="扩散模型基础"></a>扩散模型基础</h3><p>扩散模型通过前向过程逐步向数据添加噪声，再通过反向过程学习去噪，从而实现生成。</p>
<h3 id="多模态独立加噪"><a href="#多模态独立加噪" class="headerlink" title="多模态独立加噪"></a>多模态独立加噪</h3><p>UniDiffuser 的核心思想是对不同模态<strong>独立</strong>添加噪声，使用不同的时间步 $t$ 和 $s$，通过控制时间步实现任务切换。</p>
<h3 id="U-ViT-架构"><a href="#U-ViT-架构" class="headerlink" title="U-ViT 架构"></a>U-ViT 架构</h3><p>将 U-Net 的长跳跃连接（Long Skip Connection）引入 Vision Transformer，在浅层和深层之间建立连接。</p>
<div class="post-content"><a href="/2026/02/03/[OBS]Deep Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/Pasted_image_20260203153607.png" title="" title=" class="gallery-item"><img src="/2026/02/03/[OBS]Deep Learning-CV-UniDiffuser-One-Transformer-Fits-All-Distributions/Pasted_image_20260203153607.png" alt="" title=""></a></div>

<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="1-单模态扩散基础公式"><a href="#1-单模态扩散基础公式" class="headerlink" title="1. 单模态扩散基础公式"></a>1. 单模态扩散基础公式</h3><h4 id="前向过程（加噪）"><a href="#前向过程（加噪）" class="headerlink" title="前向过程（加噪）"></a>前向过程（加噪）</h4><p>$$q(x_t | x_0) &#x3D; \mathcal{N}(x_t; \alpha_t x_0, \sigma_t^2 I)$$<br><strong>参数说明</strong>：</p>
<ul>
<li>$x_0$：原始干净数据</li>
<li>$x_t$：时间步 $t$ 的加噪数据</li>
<li>$\alpha_t, \sigma_t$：噪声调度参数，满足 $\alpha_t^2 + \sigma_t^2 &#x3D; 1$（VP-SDE 设定）</li>
<li>随着 $t$ 增大，$\alpha_t \to 0$，$\sigma_t \to 1$，数据逐渐变成纯噪声</li>
</ul>
<p><strong>等价的重参数化表示</strong>：<br>$$x_t &#x3D; \alpha_t x_0 + \sigma_t \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$<br>这个形式便于采样和训练，$\epsilon$ 是标准高斯噪声。</p>
<h4 id="反向过程（去噪）"><a href="#反向过程（去噪）" class="headerlink" title="反向过程（去噪）"></a>反向过程（去噪）</h4><p>扩散模型学习反向过程 $p_\theta(x_{t-1}|x_t)$，通过神经网络预测噪声 $\epsilon_\theta(x_t, t)$，然后恢复 $x_0$：<br>$$\hat{x}_0 &#x3D; \frac{x_t - \sigma_t \epsilon_\theta(x_t, t)}{\alpha_t}$$<br><strong>解释</strong>：由 $x_t &#x3D; \alpha_t x_0 + \sigma_t \epsilon$ 反解得到。预测出噪声后，即可估计原始数据。</p>
<hr>
<h3 id="2-多模态联合扩散框架"><a href="#2-多模态联合扩散框架" class="headerlink" title="2. 多模态联合扩散框架"></a>2. 多模态联合扩散框架</h3><h4 id="联合前向过程"><a href="#联合前向过程" class="headerlink" title="联合前向过程"></a>联合前向过程</h4><p>对于图文对 $(x_0, y_0)$，<strong>独立</strong>地对每个模态加噪：<br>$$q(x_t, y_s | x_0, y_0) &#x3D; q(x_t | x_0) \cdot q(y_s | y_0)$$<br><strong>展开形式</strong>：<br>$$x_t &#x3D; \alpha_t x_0 + \sigma_t \epsilon_x, \quad y_s &#x3D; \alpha_s y_0 + \sigma_s \epsilon_y$$<br><strong>关键点</strong>：两个模态使用<strong>独立的时间步</strong> $t$ 和 $s$，这是实现多任务统一的核心设计。</p>
<h4 id="联合分布的边缘化"><a href="#联合分布的边缘化" class="headerlink" title="联合分布的边缘化"></a>联合分布的边缘化</h4><p>数据的联合分布 $q(x_0, y_0)$ 加噪后变为：<br>$$q(x_t, y_s) &#x3D; \int q(x_t, y_s | x_0, y_0) q(x_0, y_0) , dx_0 dy_0$$<br><strong>解释</strong>：这是对所有可能的原始数据对进行积分，得到加噪后数据的边缘分布。</p>
<hr>
<h3 id="3-统一多任务的核心机制"><a href="#3-统一多任务的核心机制" class="headerlink" title="3. 统一多任务的核心机制"></a>3. 统一多任务的核心机制</h3><h4 id="核心洞察"><a href="#核心洞察" class="headerlink" title="核心洞察"></a>核心洞察</h4><p>通过控制 $t$ 和 $s$ 的取值，可以从联合分布中恢复各种边缘分布和条件分布：</p>
<table>
<thead>
<tr>
<th>时间步设置</th>
<th>对应分布</th>
<th>实现的任务</th>
</tr>
</thead>
<tbody><tr>
<td>$t, s &gt; 0$</td>
<td>$q(x_t, y_s)$</td>
<td>联合生成</td>
</tr>
<tr>
<td>$t &gt; 0, s &#x3D; 0$</td>
<td>$q(x_t, y_0) &#x3D; q(x_t | y_0) q(y_0)$</td>
<td>文生图</td>
</tr>
<tr>
<td>$t &#x3D; 0, s &gt; 0$</td>
<td>$q(x_0, y_s) &#x3D; q(y_s | x_0) q(x_0)$</td>
<td>图生文</td>
</tr>
<tr>
<td>$t &gt; 0, s &#x3D; T$</td>
<td>$q(x_t, y_T) \approx q(x_t) q(y_T)$</td>
<td>无条件图像生成</td>
</tr>
</tbody></table>
<h4 id="条件生成的数学原理"><a href="#条件生成的数学原理" class="headerlink" title="条件生成的数学原理"></a>条件生成的数学原理</h4><p>当 $s&#x3D;0$ 时，$y_s &#x3D; y_0$（文本无噪声），此时：<br>$$q(x_t, y_0) &#x3D; q(x_t | y_0) \cdot q(y_0)$$<br><strong>解释</strong>：对 $y$ 不加噪声（$s&#x3D;0$）在数学上等价于以 $y$ 为条件进行生成。这是一个优雅的设计——不需要修改模型架构，只需控制时间步即可切换任务。</p>
<hr>
<h3 id="4-训练目标函数"><a href="#4-训练目标函数" class="headerlink" title="4. 训练目标函数"></a>4. 训练目标函数</h3><h4 id="噪声预测目标"><a href="#噪声预测目标" class="headerlink" title="噪声预测目标"></a>噪声预测目标</h4><p>模型 $\epsilon_\theta$ 同时预测两个模态的噪声：<br>$$[\hat{\epsilon}_x, \hat{\epsilon}<em>y] &#x3D; \epsilon_\theta(x_t, y_s, t, s)$$<br><strong>完整训练损失</strong>：<br>$$\mathcal{L}(\theta) &#x3D; \mathbb{E}</em>{t, s, (x_0, y_0), \epsilon_x, \epsilon_y} \left[ \lambda_t |\epsilon_x - \hat{\epsilon}_x|^2 + \lambda_s |\epsilon_y - \hat{\epsilon}_y|^2 \right]$$<br><strong>各项说明</strong>：</p>
<ul>
<li>$t, s \sim \mathcal{U}[0, T]$：从均匀分布采样时间步</li>
<li>$(x_0, y_0) \sim q(x_0, y_0)$：从数据集采样图文对</li>
<li>$\epsilon_x, \epsilon_y \sim \mathcal{N}(0, I)$：独立采样两个高斯噪声</li>
<li>$\lambda_t, \lambda_s$：损失权重（通常设为 1）</li>
<li>$|\cdot|^2$：均方误差损失</li>
</ul>
<p><strong>简化形式</strong>（实际训练中常用）：<br>$$\mathcal{L} &#x3D; \mathbb{E}\left[|\epsilon_x - \hat{\epsilon}_x|^2 + |\epsilon_y - \hat{\epsilon}_y|^2\right]$$</p>
<hr>
<h3 id="5-采样过程"><a href="#5-采样过程" class="headerlink" title="5. 采样过程"></a>5. 采样过程</h3><h4 id="联合采样（DDPM-形式）"><a href="#联合采样（DDPM-形式）" class="headerlink" title="联合采样（DDPM 形式）"></a>联合采样（DDPM 形式）</h4><p>从 $t&#x3D;T$（纯噪声）开始，逐步去噪到 $t&#x3D;0$：<br>$$x_{t-1} &#x3D; \frac{1}{\sqrt{\alpha_{t|t-1}}} \left( x_t - \frac{1-\alpha_{t|t-1}}{\sigma_t} \hat{\epsilon}<em>x \right) + \tilde{\sigma}<em>t z$$<br>$$y</em>{s-1} &#x3D; \frac{1}{\sqrt{\alpha</em>{s|s-1}}} \left( y_s - \frac{1-\alpha_{s|s-1}}{\sigma_s} \hat{\epsilon}_y \right) + \tilde{\sigma}_s z’$$<br><strong>参数说明</strong>：</p>
<ul>
<li>$z, z’ \sim \mathcal{N}(0, I)$：采样的随机噪声（引入随机性）</li>
<li>$\tilde{\sigma}_t$：后验方差，控制采样的随机程度</li>
<li>$\alpha_{t|t-1} &#x3D; \alpha_t &#x2F; \alpha_{t-1}$：相邻时间步的比值</li>
</ul>
<h4 id="条件采样（文生图）"><a href="#条件采样（文生图）" class="headerlink" title="条件采样（文生图）"></a>条件采样（文生图）</h4><p>固定 $s&#x3D;0$（即 $y_s &#x3D; y_0$ 为输入文本），只对图像进行去噪：<br>$$x_{t-1} &#x3D; \frac{1}{\sqrt{\alpha_{t|t-1}}} \left( x_t - \frac{1-\alpha_{t|t-1}}{\sigma_t} \hat{\epsilon}_x(x_t, y_0, t, 0) \right) + \tilde{\sigma}_t z$$<br><strong>解释</strong>：文本时间步固定为 0，模型接收干净文本作为条件，只更新图像。</p>
<hr>
<h3 id="6-U-ViT-架构中的跳跃连接"><a href="#6-U-ViT-架构中的跳跃连接" class="headerlink" title="6. U-ViT 架构中的跳跃连接"></a>6. U-ViT 架构中的跳跃连接</h3><p>U-ViT 在第 $l$ 层和第 $(L-l)$ 层之间添加跳跃连接：<br>$$h^{(L-l)} &#x3D; \text{Block}^{(L-l)}\left( \text{Concat}(h^{(L-l-1)}, h^{(l)}) \right)$$<br><strong>参数说明</strong>：</p>
<ul>
<li>$h^{(l)}$：第 $l$ 层的隐藏状态</li>
<li>$L$：Transformer 总层数</li>
<li>Concat：沿特征维度拼接</li>
<li>拼接后通过线性层降维回原始维度</li>
</ul>
<p><strong>设计动机</strong>：借鉴 U-Net 的成功经验，跳跃连接帮助保留低层的细节信息，有助于生成高质量图像。</p>
<hr>
<h3 id="7-分类器自由引导（Classifier-Free-Guidance-CFG）"><a href="#7-分类器自由引导（Classifier-Free-Guidance-CFG）" class="headerlink" title="7. 分类器自由引导（Classifier-Free Guidance, CFG）"></a>7. 分类器自由引导（Classifier-Free Guidance, CFG）</h3><p>为增强条件生成效果，使用 CFG 技术：<br>$$\tilde{\epsilon}_x &#x3D; \epsilon_\theta(x_t, \varnothing, t, 0) + w \cdot \left( \epsilon_\theta(x_t, y_0, t, 0) - \epsilon_\theta(x_t, \varnothing, t, 0) \right)$$<br><strong>参数说明</strong>：</p>
<ul>
<li>$\varnothing$：空文本条件（null condition）</li>
<li>$w$：引导强度（guidance scale），通常 $w &gt; 1$</li>
<li>第一项：无条件预测</li>
<li>括号内：条件预测与无条件预测的差值（条件信号）</li>
</ul>
<p><strong>等价形式</strong>：<br>$$\tilde{\epsilon}_x &#x3D; (1-w) \cdot \epsilon_\theta(x_t, \varnothing, t, 0) + w \cdot \epsilon_\theta(x_t, y_0, t, 0)$$</p>
<p><strong>训练技巧</strong>：训练时以一定概率（如 10%）将文本随机替换为空文本，使模型同时学习条件和无条件生成。</p>
<hr>
<h2 id="模型输入总结"><a href="#模型输入总结" class="headerlink" title="模型输入总结"></a>模型输入总结</h2><table>
<thead>
<tr>
<th>阶段</th>
<th>图像输入</th>
<th>文本输入</th>
<th>时间步</th>
</tr>
</thead>
<tbody><tr>
<td><strong>训练</strong></td>
<td>加噪图像 $x_t$</td>
<td>加噪文本 $y_s$</td>
<td>$t, s$ 随机采样</td>
</tr>
<tr>
<td><strong>联合生成</strong></td>
<td>纯噪声</td>
<td>纯噪声</td>
<td>$t&#x3D;s&#x3D;T \to 0$</td>
</tr>
<tr>
<td><strong>文生图</strong></td>
<td>纯噪声</td>
<td>原始文本</td>
<td>$t: T \to 0$, $s&#x3D;0$</td>
</tr>
<tr>
<td><strong>图生文</strong></td>
<td>原始图像</td>
<td>纯噪声</td>
<td>$t&#x3D;0$, $s: T \to 0$</td>
</tr>
</tbody></table>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><strong>MS-COCO 256×256 文生图（零样本）</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>FID↓</th>
<th>CLIP Score↑</th>
</tr>
</thead>
<tbody><tr>
<td>DALL-E</td>
<td>27.50</td>
<td>-</td>
</tr>
<tr>
<td>GLIDE</td>
<td>12.24</td>
<td>-</td>
</tr>
<tr>
<td>Stable Diffusion</td>
<td>12.63</td>
<td>0.331</td>
</tr>
<tr>
<td><strong>UniDiffuser</strong></td>
<td><strong>9.71</strong></td>
<td><strong>0.322</strong></td>
</tr>
</tbody></table>
<p><strong>ImageNet 256×256 类条件生成</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>FID↓</th>
</tr>
</thead>
<tbody><tr>
<td>ADM</td>
<td>10.94</td>
</tr>
<tr>
<td>LDM-4</td>
<td>10.56</td>
</tr>
<tr>
<td>DiT-XL&#x2F;2</td>
<td>9.62</td>
</tr>
<tr>
<td><strong>U-ViT-H&#x2F;2</strong></td>
<td><strong>2.29</strong></td>
</tr>
</tbody></table>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><strong>统一框架</strong>：单一模型支持多种生成任务，无需为每个任务单独训练</li>
<li><strong>优雅设计</strong>：通过时间步控制实现任务切换，不需要修改架构</li>
<li><strong>强大性能</strong>：在多个基准上达到 SOTA</li>
<li><strong>可扩展性</strong>：在 10 亿参数规模上验证有效</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li>需要大规模图文对数据进行训练</li>
<li>文本生成质量依赖于 CLIP 编码器的表示能力</li>
<li>推理速度受限于扩散模型的迭代采样</li>
</ul>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.12092">DALL-E</a>：基于 VQ-VAE 的文生图模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">Stable Diffusion</a>：潜空间扩散模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.09748">DiT</a>：Diffusion Transformer 架构[[Diffusion-Transformers-DiT]]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10741">GLIDE</a>：文本引导的扩散模型</li>
</ul>
<hr>
<h2 id="公式速查表"><a href="#公式速查表" class="headerlink" title="公式速查表"></a>公式速查表</h2><table>
<thead>
<tr>
<th>公式</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>$x_t &#x3D; \alpha_t x_0 + \sigma_t \epsilon$</td>
<td>前向加噪过程</td>
</tr>
<tr>
<td>$q(x_t, y_s | x_0, y_0) &#x3D; q(x_t|x_0) q(y_s|y_0)$</td>
<td>独立加噪（多任务统一的关键）</td>
</tr>
<tr>
<td>$\mathcal{L} &#x3D; |\epsilon_x - \hat{\epsilon}_x|^2 + |\epsilon_y - \hat{\epsilon}_y|^2$</td>
<td>训练目标</td>
</tr>
<tr>
<td>$s&#x3D;0 \Rightarrow$ 以文本为条件</td>
<td>任务切换机制</td>
</tr>
<tr>
<td>$\tilde{\epsilon} &#x3D; \epsilon_\varnothing + w(\epsilon_y - \epsilon_\varnothing)$</td>
<td>分类器自由引导</td>
</tr>
</tbody></table>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>Bao, F., et al. (2023). One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. ICML 2023.</li>
<li>Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. NeurIPS 2020.</li>
<li>Peebles, W., &amp; Xie, S. (2023). Scalable Diffusion Models with Transformers. ICCV 2023.</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/03/%5BOBS%5DDeep%20Learning-CV-Diffusion-Transformers-DiT/"><img class="fill" src="/gallery/Research-paper.png" alt="Scalable Diffusion Models with Transformers" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-03T11:30:00.000Z" title="2/3/2026, 7:30:00 PM">2026-02-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.509Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">18 minutes read (About 2709 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/03/%5BOBS%5DDeep%20Learning-CV-Diffusion-Transformers-DiT/">Scalable Diffusion Models with Transformers</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2026/02/03/[OBS]Deep Learning-CV-Diffusion-Transformers-DiT/Pasted_image_20260203120620.png" title="" title=" class="gallery-item"><img src="/2026/02/03/[OBS]Deep Learning-CV-Diffusion-Transformers-DiT/Pasted_image_20260203120620.png" alt="" title=""></a></div>
# Diffusion Transformers (DiT)

<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a> | ICCV 2023</p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>扩散模型(Diffusion Models)在图像生成领域取得了显著成功，但其架构设计一直沿用卷积U-Net作为主干网络。与此同时，Transformer架构已经在自然语言处理、视觉识别等多个领域取得了统治地位，并展现出优秀的可扩展性。本文探索将Transformer架构引入扩散模型，研究其在图像生成任务中的可扩展性和性能表现。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><ul>
<li><strong>突破架构局限</strong>：探索用Transformer替代传统U-Net作为扩散模型主干的可行性</li>
<li><strong>验证可扩展性</strong>：研究Transformer在扩散模型中的可扩展性规律</li>
<li><strong>建立性能基准</strong>：在ImageNet等基准数据集上达到SOTA性能</li>
<li><strong>揭示计算-质量关系</strong>：分析模型计算量(Gflops)与生成质量之间的关系</li>
</ul>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Latent-Diffusion-Models-LDMs"><a href="#Latent-Diffusion-Models-LDMs" class="headerlink" title="Latent Diffusion Models (LDMs)"></a>Latent Diffusion Models (LDMs)</h3><p>在潜在空间而非像素空间训练扩散模型，提高计算效率：</p>
<ol>
<li><strong>VAE编码器</strong>：将图像压缩到潜在空间 $z &#x3D; E(x)$</li>
<li><strong>扩散模型</strong>：在潜在空间 $z$ 中训练</li>
<li><strong>VAE解码器</strong>：将生成的潜在表示解码为图像 $x &#x3D; D(z)$</li>
</ol>
<p>对于256×256×3的图像，VAE将其压缩为32×32×4的潜在表示（下采样因子为8）。</p>
<p><strong>注意</strong>：这里使用的是标准VAE，输出是<strong>连续的潜在表示</strong>，而非VQ-VAE的离散codebook索引。</p>
<h3 id="Patchify机制"><a href="#Patchify机制" class="headerlink" title="Patchify机制"></a>Patchify机制</h3><p>将潜在表示分解为patch序列：</p>
<ul>
<li>输入：32×32×4的潜在表示</li>
<li>Patch大小：$p \times p$（$p \in {2, 4, 8}$）</li>
<li>输出序列长度：$T &#x3D; (I&#x2F;p)^2$</li>
</ul>
<p>例如，$p&#x3D;2$ 时，序列长度 $T &#x3D; (32&#x2F;2)^2 &#x3D; 256$。</p>
<h3 id="Classifier-Free-Guidance"><a href="#Classifier-Free-Guidance" class="headerlink" title="Classifier-Free Guidance"></a>Classifier-Free Guidance</h3><p>条件生成的采样技巧，提高生成质量：</p>
<p>$$<br>\hat{\epsilon}_\theta(x_t, c) &#x3D; \epsilon_\theta(x_t, \emptyset) + s \cdot (\epsilon_\theta(x_t, c) - \epsilon_\theta(x_t, \emptyset))<br>$$</p>
<p>其中：</p>
<ul>
<li>$c$：条件信息（如类别标签）</li>
<li>$\emptyset$：空条件（训练时随机dropout）</li>
<li>$s$：guidance scale（$s &gt; 1$增强条件控制）</li>
</ul>
<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="DiT架构设计"><a href="#DiT架构设计" class="headerlink" title="DiT架构设计"></a>DiT架构设计</h3><p>DiT基于Vision Transformer (ViT)架构，包含以下组件：</p>
<ol>
<li><strong>Patchify层</strong>：将潜在表示转换为token序列</li>
<li><strong>位置编码</strong>：使用正弦-余弦位置编码</li>
<li><strong>DiT Blocks</strong>：N个Transformer block</li>
<li><strong>解码器</strong>：将token序列解码为噪声预测和协方差预测</li>
</ol>
<h3 id="条件注入机制"><a href="#条件注入机制" class="headerlink" title="条件注入机制"></a>条件注入机制</h3><p>探索了四种将时间步 $t$ 和类别标签 $c$ 注入Transformer的方式：</p>
<h4 id="1-In-Context-Conditioning"><a href="#1-In-Context-Conditioning" class="headerlink" title="1. In-Context Conditioning"></a>1. In-Context Conditioning</h4><p>将 $t$ 和 $c$ 的embedding作为额外token添加到序列中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokens = [t_embed, c_embed, patch_1, patch_2, ..., patch_T]</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>优点</strong>：无需修改标准Transformer block</li>
<li><strong>缺点</strong>：性能较差（FID ~80）</li>
</ul>
<h4 id="2-Cross-Attention"><a href="#2-Cross-Attention" class="headerlink" title="2. Cross-Attention"></a>2. Cross-Attention</h4><p>通过交叉注意力机制注入条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准self-attention后添加cross-attention</span></span><br><span class="line">x = x + SelfAttention(x)</span><br><span class="line">x = x + CrossAttention(x, [t_embed, c_embed])</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>优点</strong>：灵活的条件控制</li>
<li><strong>缺点</strong>：增加15%计算量，性能中等（FID ~60）</li>
</ul>
<h4 id="3-Adaptive-Layer-Norm-adaLN"><a href="#3-Adaptive-Layer-Norm-adaLN" class="headerlink" title="3. Adaptive Layer Norm (adaLN)"></a>3. Adaptive Layer Norm (adaLN)</h4><p>通过自适应归一化层注入条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">γ, β = MLP(t_embed + c_embed)</span><br><span class="line">output = γ * normalize(x) + β</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>优点</strong>：计算高效，性能较好（FID ~45）</li>
<li><strong>缺点</strong>：所有token共享���同的调制参数</li>
</ul>
<h4 id="4-adaLN-Zero（最优方案）"><a href="#4-adaLN-Zero（最优方案）" class="headerlink" title="4. adaLN-Zero（最优方案）"></a>4. adaLN-Zero（最优方案）</h4><p>在adaLN基础上增加门控参数并零初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">γ, β, α = MLP(t_embed + c_embed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer Block</span></span><br><span class="line">x = x + α₁ * Attention(γ₁ * normalize(x) + β₁)</span><br><span class="line">x = x + α₂ * FFN(γ₂ * normalize(x) + β₂)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化：MLP输出零向量 → α=0, γ=0, β=0</span></span><br><span class="line"><span class="comment"># 因此初始时：x = x + 0 = x（恒等函数）</span></span><br></pre></td></tr></table></figure>

<p><strong>为什么需要两组参数？</strong></p>
<p>Transformer block有两个子层（Attention + FFN），每个子层需要独立的条件控制：</p>
<ul>
<li>第一组 $(γ_1, β_1, α_1)$：用于Attention子层</li>
<li>第二组 $(γ_2, β_2, α_2)$：用于FFN子层</li>
</ul>
<p>总共6个参数：<code>shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp</code></p>
<p><strong>零初始化的优势</strong>：</p>
<ul>
<li>训练稳定性：初始时网络是恒等映射，梯度流动顺畅</li>
<li>更好的性能：FID显著优于其他方法（FID ~23）</li>
<li>渐进式学习：从恒等函数开始，逐步学习有用的变换</li>
</ul>
<h3 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h3><p>设计四种规模的模型配置：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>层数N</th>
<th>隐藏维度d</th>
<th>注意力头数</th>
<th>Gflops (p&#x3D;4)</th>
</tr>
</thead>
<tbody><tr>
<td>DiT-S</td>
<td>12</td>
<td>384</td>
<td>6</td>
<td>1.4</td>
</tr>
<tr>
<td>DiT-B</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>5.6</td>
</tr>
<tr>
<td>DiT-L</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>19.7</td>
</tr>
<tr>
<td>DiT-XL</td>
<td>28</td>
<td>1152</td>
<td>16</td>
<td>29.1</td>
</tr>
</tbody></table>
<h3 id="Point-wise-FFN"><a href="#Point-wise-FFN" class="headerlink" title="Point-wise FFN"></a>Point-wise FFN</h3><p>Transformer中的标准组件，对序列中每个位置独立应用前馈网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PointwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: [batch, seq_len, d_model]</span></span><br><span class="line">        x = Linear1(x)      <span class="comment"># d_model → d_ff (通常4倍扩展)</span></span><br><span class="line">        x = GELU(x)</span><br><span class="line">        x = Linear2(x)      <span class="comment"># d_ff → d_model</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关键特点：</span></span><br><span class="line"><span class="comment"># 1. 每个token独立处理（point-wise）</span></span><br><span class="line"><span class="comment"># 2. 所有位置共享相同的权重</span></span><br><span class="line"><span class="comment"># 3. 可以完全并行计算</span></span><br></pre></td></tr></table></figure>

<p><strong>与Self-Attention的分工</strong>：</p>
<ul>
<li>Self-Attention：全局信息聚合（不同token间交互）</li>
<li>Point-wise FFN：局部特征变换（每个token独立处理）</li>
</ul>
<p>这是<strong>所有Transformer变体</strong>（LLM、ViT、DiT）的统一设计。</p>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="1-Gflops与生成质量强相关"><a href="#1-Gflops与生成质量强相关" class="headerlink" title="1. Gflops与生成质量强相关"></a>1. Gflops与生成质量强相关</h3><p>模型前向传播的计算量(Gflops)与FID呈强负相关（相关系数-0.93）：</p>
<ul>
<li>增加模型深度&#x2F;宽度 → 提升Gflops → 降低FID</li>
<li>减小patch大小 → 增加token数量 → 提升Gflops → 降低FID</li>
</ul>
<p><strong>关键洞察</strong>：参数量不是���一决定因素，计算量才是提升性能的关键。</p>
<h3 id="2-adaLN-Zero显著优于其他条件注入方式"><a href="#2-adaLN-Zero显著优于其他条件注入方式" class="headerlink" title="2. adaLN-Zero显著优于其他条件注入方式"></a>2. adaLN-Zero显著优于其他条件注入方式</h3><p>在400K训练步数时的FID对比：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>FID-50K</th>
<th>计算开销</th>
</tr>
</thead>
<tbody><tr>
<td>In-Context</td>
<td>~80</td>
<td>119.4 Gflops</td>
</tr>
<tr>
<td>Cross-Attention</td>
<td>~60</td>
<td>137.6 Gflops</td>
</tr>
<tr>
<td>adaLN</td>
<td>~45</td>
<td>118.6 Gflops</td>
</tr>
<tr>
<td><strong>adaLN-Zero</strong></td>
<td><strong>~23</strong></td>
<td><strong>118.6 Gflops</strong></td>
</tr>
</tbody></table>
<h3 id="3-优秀的可扩展性"><a href="#3-优秀的可扩展性" class="headerlink" title="3. 优秀的可扩展性"></a>3. 优秀的可扩展性</h3><p>DiT展现出与ViT类似的可扩展性：</p>
<ul>
<li>增加模型规模持续提升性能</li>
<li>训练高度稳定，无需学习率预热或特殊正则化</li>
<li>未观察到常见的loss spike现象</li>
</ul>
<h3 id="4-计算效率优势"><a href="#4-计算效率优势" class="headerlink" title="4. 计算效率优势"></a>4. 计算效率优势</h3><p>DiT-XL&#x2F;2 (118.6 Gflops) 比传统方法更高效：</p>
<ul>
<li>像素空间U-Net (ADM)：1120 Gflops（~10倍）</li>
<li>潜在空间U-Net (LDM-4)：103.6 Gflops（相近但性能更优）</li>
</ul>
<h3 id="5-采样计算无法弥补模型计算不足"><a href="#5-采样计算无法弥补模型计算不足" class="headerlink" title="5. 采样计算无法弥补模型计算不足"></a>5. 采样计算无法弥补模型计算不足</h3><p>增加采样步数（增加测试时计算量）无法弥补模型规模不足：</p>
<ul>
<li>DiT-L&#x2F;2 使用1000步采样：80.7 Tflops，FID&#x3D;25.9</li>
<li>DiT-XL&#x2F;2 使用128步采样：15.2 Tflops，FID&#x3D;23.7</li>
</ul>
<p><strong>结论</strong>：模型计算量比采样计算量更重要。</p>
<hr>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li><strong>ImageNet</strong>：256×256和512×512分辨率</li>
<li><strong>任务</strong>：类条件图像生成</li>
</ul>
<h3 id="训练配置"><a href="#训练配置" class="headerlink" title="训练配置"></a>训练配置</h3><ul>
<li><strong>优化器</strong>：AdamW</li>
<li><strong>学习率</strong>：$1 \times 10^{-4}$（常数，无warmup）</li>
<li><strong>批大小</strong>：256</li>
<li><strong>数据增强</strong>：仅水平翻转</li>
<li><strong>EMA</strong>：decay&#x3D;0.9999</li>
<li><strong>硬件</strong>：TPU v3-256 pod</li>
</ul>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><ul>
<li><strong>主要指标</strong>：FID-50K（使用250步DDPM采样）</li>
<li><strong>次要指标</strong>：Inception Score、sFID、Precision&#x2F;Recall</li>
</ul>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p><strong>ImageNet 256×256基准测试</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>FID↓</th>
<th>IS↑</th>
<th>Precision</th>
<th>Recall</th>
</tr>
</thead>
<tbody><tr>
<td>LDM-4-G (cfg&#x3D;1.50)</td>
<td>3.60</td>
<td>247.67</td>
<td>0.87</td>
<td>0.48</td>
</tr>
<tr>
<td>StyleGAN-XL</td>
<td>2.30</td>
<td>265.12</td>
<td>0.78</td>
<td>0.53</td>
</tr>
<tr>
<td><strong>DiT-XL&#x2F;2-G (cfg&#x3D;1.50)</strong></td>
<td><strong>2.27</strong></td>
<td><strong>278.24</strong></td>
<td><strong>0.83</strong></td>
<td><strong>0.57</strong></td>
</tr>
</tbody></table>
<p><strong>ImageNet 512×512基准测试</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>FID↓</th>
<th>IS↑</th>
</tr>
</thead>
<tbody><tr>
<td>ADM-G, ADM-U</td>
<td>3.85</td>
<td>221.72</td>
</tr>
<tr>
<td><strong>DiT-XL&#x2F;2-G (cfg&#x3D;1.50)</strong></td>
<td><strong>3.04</strong></td>
<td><strong>240.82</strong></td>
</tr>
</tbody></table>
<p>DiT-XL&#x2F;2在两个分辨率上都达到了<strong>SOTA性能</strong>。</p>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><strong>架构统一性</strong>：证明Transformer可以成功替代U-Net，推动生成模型架构统一化</li>
<li><strong>优秀的可扩展性</strong>：计算量与性能呈强相关，为大规模模型发展指明方向</li>
<li><strong>训练稳定性</strong>：无需特殊技巧即可稳定训练</li>
<li><strong>计算效率</strong>：在相近或更少的计算量下达到更好的性能</li>
<li><strong>更高的Recall</strong>：相比LDM，DiT在所有guidance scale下都有更高的recall值</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li><strong>依赖预训练VAE</strong>：使用Stable Diffusion的VAE，是混合架构而非纯Transformer</li>
<li><strong>仅探索类条件生成</strong>：未涉及文生图等更复杂的条件生成任务</li>
<li><strong>计算资源需求</strong>：大规模模型训练需要TPU集群</li>
<li><strong>patch大小的权衡</strong>：更小的patch提升性能但增加计算量</li>
</ul>
<h3 id="与传统U-Net的对比"><a href="#与传统U-Net的对比" class="headerlink" title="与传统U-Net的对比"></a>与传统U-Net的对比</h3><table>
<thead>
<tr>
<th>特性</th>
<th>U-Net</th>
<th>DiT</th>
</tr>
</thead>
<tbody><tr>
<td>归纳偏置</td>
<td>强（卷积、多尺度）</td>
<td>弱（纯注意力）</td>
</tr>
<tr>
<td>可扩展性</td>
<td>有限</td>
<td>优秀</td>
</tr>
<tr>
<td>架构统一性</td>
<td>领域特定</td>
<td>跨领域通用</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>需要技巧</td>
<td>天然稳定</td>
</tr>
</tbody></table>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="Transformer在生成模型中的应用"><a href="#Transformer在生成模型中的应用" class="headerlink" title="Transformer在生成模型中的应用"></a>Transformer在生成模型中的应用</h3><ul>
<li><strong>自回归模型</strong>：GPT系列、ImageGPT、DALL·E</li>
<li><strong>掩码生成模型</strong>：MaskGIT、MAGE</li>
<li><strong>DALL·E 2</strong>：使用Transformer生成CLIP embedding</li>
</ul>
<h3 id="扩散模型架构"><a href="#扩散模型架构" class="headerlink" title="扩散模型架构"></a>扩散模型架构</h3><ul>
<li><strong>DDPM</strong> (Ho et al., 2020)：首次引入U-Net作为扩散模型主干</li>
<li><strong>ADM</strong> (Dhariwal &amp; Nichol, 2021)：改进U-Net设计，达到SOTA</li>
<li><strong>LDM</strong> (Rombach et al., 2022)：潜在空间扩散模型</li>
<li><strong>Concurrent work</strong>：U-ViT探索了类似的Transformer架构</li>
</ul>
<h3 id="架构复杂度分析"><a href="#架构复杂度分析" class="headerlink" title="架构复杂度分析"></a>架构复杂度分析</h3><ul>
<li><strong>ViT</strong> (Dosovitskiy et al., 2021)：证明Transformer在视觉任务中的可扩展性</li>
<li><strong>Scaling Laws</strong>：语言模型中计算量与性能的幂律关系</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ol>
<li><strong>扩展到文生图任务</strong>：将DiT应用于DALL·E 2、Stable Diffusion等文生图模型</li>
<li><strong>���大规模的模型</strong>：继续扩展模型规模，探索scaling law</li>
<li><strong>纯Transformer架构</strong>：在像素空间训练DiT，摆脱VAE依赖</li>
<li><strong>多模态条件生成</strong>：探索更复杂的条件注入机制</li>
<li><strong>高效采样方法</strong>：结合DiT开发更快的采样算法</li>
<li><strong>架构搜索</strong>：自动化探索最优的DiT配置</li>
</ol>
<hr>
<h2 id="技术细节补充"><a href="#技术细节补充" class="headerlink" title="技术细节补充"></a>技术细节补充</h2><h3 id="VAE-vs-VQ-VAE"><a href="#VAE-vs-VQ-VAE" class="headerlink" title="VAE vs VQ-VAE"></a>VAE vs VQ-VAE</h3><table>
<thead>
<tr>
<th>特性</th>
<th>VAE (DiT使用)</th>
<th>VQ-VAE</th>
</tr>
</thead>
<tbody><tr>
<td>潜在空间</td>
<td>连续（浮点数）</td>
<td>离散（codebook索引）</td>
</tr>
<tr>
<td>输出维度</td>
<td>32×32×4（4通道特征）</td>
<td>32×32（单个索引）</td>
</tr>
<tr>
<td>适用场景</td>
<td>扩散模型</td>
<td>自回归模型</td>
</tr>
<tr>
<td>量化</td>
<td>无</td>
<td>有（查表）</td>
</tr>
</tbody></table>
<h3 id="Transformer-Block完整结构"><a href="#Transformer-Block完整结构" class="headerlink" title="Transformer Block完整结构"></a>Transformer Block完整结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dit_block</span>(<span class="params">x, c</span>):</span><br><span class="line">    <span class="comment"># 生成6个调制参数</span></span><br><span class="line">    γ₁, β₁, α₁, γ₂, β₂, α₂ = adaLN_modulation(c)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 子层1: Multi-Head Self-Attention</span></span><br><span class="line">    x = x + α₁ * Attention(adaLN(x, γ₁, β₁))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 子层2: Point-wise Feed-Forward</span></span><br><span class="line">    x = x + α₂ * FFN(adaLN(x, γ₂, β₂))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">adaLN</span>(<span class="params">x, γ, β</span>):</span><br><span class="line">    <span class="keyword">return</span> γ * normalize(x) + β</span><br></pre></td></tr></table></figure>

<h3 id="计算复杂度分析"><a href="#计算复杂度分析" class="headerlink" title="计算复杂度分析"></a>计算复杂度分析</h3><p>对于DiT-XL&#x2F;2（$N&#x3D;256$ tokens，$d&#x3D;1152$）：</p>
<ul>
<li><strong>Self-Attention</strong>：$O(N^2 \cdot d) \approx 75M$ ops</li>
<li><strong>Point-wise FFN</strong>：$O(N \cdot d^2 \cdot 2) \approx 680M$ ops</li>
</ul>
<p>FFN占据了大部分计算量！</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>Peebles, W., &amp; Xie, S. (2023). Scalable Diffusion Models with Transformers. ICCV 2023.</li>
<li>Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. NeurIPS 2020.</li>
<li>Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. CVPR 2022.</li>
<li>Dosovitskiy, A., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021.</li>
<li>Dhariwal, P., &amp; Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. NeurIPS 2021.</li>
</ul>
<hr>
<h2 id="关键要点总结"><a href="#关键要点总结" class="headerlink" title="关键要点总结"></a>关键要点总结</h2><ol>
<li><strong>架构创新</strong>：首次系统性地将纯Transformer应用于扩散模型</li>
<li><strong>adaLN-Zero</strong>：零初始化的自适应归一化是性能关键</li>
<li><strong>Gflops定律</strong>：计算量与生成质量强相关（-0.93）</li>
<li><strong>SOTA性能</strong>：ImageNet 256×256达到FID 2.27</li>
<li><strong>统一架构</strong>：推动生成模型向Transformer统一的趋势</li>
</ol>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/02/%5BOBS%5DDeep%20Learning-Robot%20Learnning-GR00T-N1-Humanoid-Robot-Foundation-Model/"><img class="fill" src="/gallery/Research-paper.png" alt="GR00T N1 An Open Foundation Model for Generalist Humanoid Robots" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-02T11:30:00.000Z" title="2/2/2026, 7:30:00 PM">2026-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.520Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">28 minutes read (About 4167 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/02/%5BOBS%5DDeep%20Learning-Robot%20Learnning-GR00T-N1-Humanoid-Robot-Foundation-Model/">GR00T N1 An Open Foundation Model for Generalist Humanoid Robots</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2026/02/02/[OBS]Deep Learning-Robot Learnning-GR00T-N1-Humanoid-Robot-Foundation-Model/Pasted_image_20260203120646.png" title="" title=" class="gallery-item"><img src="/2026/02/02/[OBS]Deep Learning-Robot Learnning-GR00T-N1-Humanoid-Robot-Foundation-Model/Pasted_image_20260203120646.png" alt="" title=""></a></div>
# GR00T N1: 通用人形机器人开放基础模型

<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.14734v2">论文链接</a> | NVIDIA, 2025</p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>人形机器人作为通用机器人的理想硬件平台，需要强大的基础模型来实现智能自主操作。受大语言模型和视觉模型成功的启发，研究者希望通过在大规模异构数据上训练机器人基础模型，使其能够理解新场景、处理真实世界的变化并快速学习新任务。然而，与文本和图像领域不同，机器人领域缺乏互联网规模的训练数据，不同机器人的传感器、自由度、控制模式差异巨大，形成”数据孤岛”问题。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><p>本论文要解决的核心问题：</p>
<ol>
<li><strong>数据稀缺问题</strong>：人形机器人数据收集成本高、耗时长，如何突破真实数据瓶颈</li>
<li><strong>跨具身泛化</strong>：如何统一不同机器人的状态和动作空间，实现跨具身学习</li>
<li><strong>数据效率</strong>：如何在有限数据下快速适应新任务并在真实环境中鲁棒执行</li>
<li><strong>端到端优化</strong>：如何将高层推理与低层控制统一到单一模型中</li>
</ol>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Vision-Language-Action-VLA-模型"><a href="#Vision-Language-Action-VLA-模型" class="headerlink" title="Vision-Language-Action (VLA) 模型"></a>Vision-Language-Action (VLA) 模型</h3><p>视觉-语言-动作模型，接收图像观察和语言指令作为输入，直接输出机器人动作。与传统的分层方法（VLM规划 + 低层策略执行）不同，VLA模型实现端到端优化。</p>
<h3 id="双系统架构-Dual-System-Architecture"><a href="#双系统架构-Dual-System-Architecture" class="headerlink" title="双系统架构 (Dual-System Architecture)"></a>双系统架构 (Dual-System Architecture)</h3><p>受人类认知理论启发（Kahneman, 2011），将模型分为：</p>
<ul>
<li><strong>System 2（推理系统）</strong>：慢速、深思熟虑的高层推理</li>
<li><strong>System 1（反应系统）</strong>：快速、自动化的低层控制</li>
</ul>
<h3 id="数据金字塔-Data-Pyramid"><a href="#数据金字塔-Data-Pyramid" class="headerlink" title="数据金字塔 (Data Pyramid)"></a>数据金字塔 (Data Pyramid)</h3><p>将异构训练数据按规模和具身特异性组织成三层结构：</p>
<ul>
<li><strong>底层</strong>：大规模网络数据和人类视频（通用先验）</li>
<li><strong>中层</strong>：合成数据（仿真+神经生成，可扩展）</li>
<li><strong>顶层</strong>：真实机器人数据（具身特定，高质量）</li>
</ul>
<h3 id="潜在动作-Latent-Actions"><a href="#潜在动作-Latent-Actions" class="headerlink" title="潜在动作 (Latent Actions)"></a>潜在动作 (Latent Actions)</h3><p>通过VQ-VAE([[VQ-VAE-and-Latent-Action-for-Robotics]])学习的通用动作表示，能够统一不同具身体（包括人类）的动作空间，使无动作标签的视频数据可用于训练。</p>
<hr>
<h2 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h2><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>GR00T N1采用双系统组合架构，总参数量22亿（GR00T-N1-2B）：</p>
<h4 id="System-2-Vision-Language-Module"><a href="#System-2-Vision-Language-Module" class="headerlink" title="System 2: Vision-Language Module"></a>System 2: Vision-Language Module</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">输入处理:</span><br><span class="line">├─ 图像: SigLIP-2编码器 → 64个token (224×224)</span><br><span class="line">└─ 文本: SmolLM2 tokenizer → 文本token</span><br><span class="line"></span><br><span class="line">特征提取:</span><br><span class="line">└─ Eagle-2 VLM (1.34B参数)</span><br><span class="line">   ├─ 处理vision-language tokens</span><br><span class="line">   └─ 输出: 中间层embeddings φ_t (第12层)</span><br></pre></td></tr></table></figure>

<p><strong>关键设计</strong>：</p>
<ul>
<li>使用中间层而非最终层特征（更快推理+更高成功率）</li>
<li>语言组件冻结（保留预训练知识）</li>
<li>视觉编码器可训练（适应机器人任务）</li>
<li>运行频率：10Hz</li>
</ul>
<h4 id="System-1-Diffusion-Transformer-Module"><a href="#System-1-Diffusion-Transformer-Module" class="headerlink" title="System 1: Diffusion Transformer Module"></a>System 1: Diffusion Transformer Module</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DiT Block结构（重复N次）:</span><br><span class="line">├─ Self-Attention</span><br><span class="line">│  └─ 输入: noised action tokens + state embeddings</span><br><span class="line">│</span><br><span class="line">└─ Cross-Attention</span><br><span class="line">   ├─ Query: action/state tokens</span><br><span class="line">   └─ Key &amp; Value: VLM输出的φ_t</span><br></pre></td></tr></table></figure>

<p><strong>动作生成流程</strong>：</p>
<ol>
<li>输入加噪动作 $A_t^{\tau} &#x3D; \tau A_t + (1-\tau)\epsilon$，其中 $\tau \in [0,1]$</li>
<li>通过DiT迭代去噪（K&#x3D;4步）</li>
<li>输出16步动作序列（action chunking）</li>
<li>运行频率：120Hz</li>
</ol>
<p><strong>Flow-Matching损失</strong>：</p>
<p>$$<br>\mathcal{L}<em>{fm}(\theta) &#x3D; \mathbb{E}</em>{\tau} |V_{\theta}(\varphi_t, A_t^{\tau}, q_t) - (\epsilon - A_t)|^2<br>$$</p>
<p>其中 $V_{\theta}$ 是[[Diffusion-Transformers-DiT]]模型，预测去噪向量场。</p>
<h4 id="模块交互机制"><a href="#模块交互机制" class="headerlink" title="模块交互机制"></a>模块交互机制</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">信息流:</span><br><span class="line">图像 + 语言指令</span><br><span class="line">    ↓</span><br><span class="line">[System 2: Eagle-2 VLM]</span><br><span class="line">    ↓ (输出 φ_t)</span><br><span class="line">[Cross-Attention Bridge]</span><br><span class="line">    ↓</span><br><span class="line">[System 1: DiT]</span><br><span class="line">├─ Self-Attention (action + state)</span><br><span class="line">└─ Cross-Attention (attend to φ_t)</span><br><span class="line">    ↓</span><br><span class="line">16步动作序列</span><br></pre></td></tr></table></figure>

<p><strong>端到端联合训练</strong>：</p>
<ul>
<li>两个模块通过cross-attention紧密耦合</li>
<li>使用统一的flow-matching loss优化</li>
<li>辅助目标检测loss增强空间理解：</li>
</ul>
<p>$$<br>\mathcal{L} &#x3D; \mathcal{L}<em>{fm} + \mathcal{L}</em>{det}<br>$$</p>
<h3 id="异构数据训练策略"><a href="#异构数据训练策略" class="headerlink" title="异构数据训练策略"></a>异构数据训练策略</h3><h4 id="1-数据金字塔组织"><a href="#1-数据金字塔组织" class="headerlink" title="1. 数据金字塔组织"></a>1. 数据金字塔组织</h4><table>
<thead>
<tr>
<th>层级</th>
<th>数据源</th>
<th>时长</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>顶层</td>
<td>真实机器人数据</td>
<td>3,289小时</td>
<td>具身特定，高质量</td>
</tr>
<tr>
<td>中层</td>
<td>仿真数据</td>
<td>1,743小时</td>
<td>可扩展，物理约束</td>
</tr>
<tr>
<td>中层</td>
<td>神经生成数据</td>
<td>827小时</td>
<td>反事实场景，多样性</td>
</tr>
<tr>
<td>底层</td>
<td>人类视频</td>
<td>2,517小时</td>
<td>大规模，通用先验</td>
</tr>
</tbody></table>
<p><strong>总计</strong>：8,376小时训练数据</p>
<h4 id="2-潜在动作学习"><a href="#2-潜在动作学习" class="headerlink" title="2. 潜在动作学习"></a>2. 潜在动作学习</h4><p><strong>VQ-VAE训练</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码器</span></span><br><span class="line">输入: (当前帧 x_t, 未来帧 x_&#123;t+H&#125;)</span><br><span class="line">     ↓</span><br><span class="line">Encoder → 连续embedding → 量化到codebook</span><br><span class="line">     ↓</span><br><span class="line">潜在动作 z_t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码器</span></span><br><span class="line">输入: x_t + z_t</span><br><span class="line">     ↓</span><br><span class="line">Decoder → 重建 x_&#123;t+H&#125;</span><br></pre></td></tr></table></figure>

<p><strong>跨具身一致性</strong>：</p>
<ul>
<li>同一潜在动作在不同具身体中语义一致</li>
<li>例如：潜在动作1 &#x3D; “右臂向左移动”（对所有机器人和人类）</li>
</ul>
<p><strong>训练使用</strong>：</p>
<ul>
<li>提取预量化连续embedding作为”LAPA具身体”的动作</li>
<li>使用flow-matching loss训练</li>
</ul>
<h4 id="3-神经轨迹生成"><a href="#3-神经轨迹生成" class="headerlink" title="3. 神经轨迹生成"></a>3. 神经轨迹生成</h4><p><strong>目标</strong>：从88小时真实数据扩增到827小时（~10倍）</p>
<p><strong>技术流程</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">步骤1: 微调视频生成模型</span><br><span class="line">├─ 基础模型: WAN2.1-I2V-14B</span><br><span class="line">├─ 方法: LoRA微调</span><br><span class="line">├─ 数据: 3,000条轨迹，81帧@480P</span><br><span class="line">└─ 训练: 100 epochs</span><br><span class="line"></span><br><span class="line">步骤2: 生成反事实轨迹</span><br><span class="line">├─ 输入: 初始帧 + 新语言指令</span><br><span class="line">├─ 语言生成: 多模态LLM检测物体</span><br><span class="line">│   生成&quot;pick &#123;object&#125; from &#123;A&#125; to &#123;B&#125;&quot;</span><br><span class="line">└─ 输出: 高质量视频</span><br><span class="line"></span><br><span class="line">步骤3: 质量过滤</span><br><span class="line">├─ 采样8帧 → LLM判断是否遵循指令</span><br><span class="line">└─ 不合格 → 重新标注</span><br><span class="line"></span><br><span class="line">步骤4: 动作标注</span><br><span class="line">├─ 潜在动作编码器 → LAPA</span><br><span class="line">└─ 逆动力学模型 → 伪动作标签</span><br></pre></td></tr></table></figure>

<p><strong>生成能力</strong>：</p>
<ul>
<li>改变操作手（左手↔右手）</li>
<li>改变目标位置和物体</li>
<li>处理仿真难题（液体、铰接物体）</li>
<li>多视角生成（4宫格视频）</li>
</ul>
<h4 id="4-仿真数据自动生成"><a href="#4-仿真数据自动生成" class="headerlink" title="4. 仿真数据自动生成"></a>4. 仿真数据自动生成</h4><p><strong>DexMimicGen系统</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">输入: 少量人类演示（几十条）</span><br><span class="line">     ↓</span><br><span class="line">分割 → 物体中心的子任务片段</span><br><span class="line">     ↓</span><br><span class="line">变换 → 根据新物体位置调整</span><br><span class="line">     ↓</span><br><span class="line">组合 → 插值并组合片段</span><br><span class="line">     ↓</span><br><span class="line">验证 → 仿真执行，保留成功轨迹</span><br><span class="line">     ↓</span><br><span class="line">输出: 每任务10,000条演示</span><br></pre></td></tr></table></figure>

<p><strong>规模</strong>：</p>
<ul>
<li>54个源-目标容器组合</li>
<li>540,000条预训练轨迹</li>
<li>11小时生成 &#x3D; 6,500小时等效人类演示</li>
</ul>
<h4 id="5-具身特定编码器-x2F-解码器"><a href="#5-具身特定编码器-x2F-解码器" class="headerlink" title="5. 具身特定编码器&#x2F;解码器"></a>5. 具身特定编码器&#x2F;解码器</h4><p><strong>处理不同维度的状态和动作</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">embodiments = &#123;</span><br><span class="line">    <span class="string">&quot;GR-1&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;state&quot;</span>: [joint_pos, joint_vel, base_pos, ...],</span><br><span class="line">        <span class="string">&quot;action&quot;</span>: [joint_targets, ...],</span><br><span class="line">        <span class="string">&quot;encoder&quot;</span>: MLP_GR1,</span><br><span class="line">        <span class="string">&quot;decoder&quot;</span>: MLP_GR1</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;Franka&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;state&quot;</span>: [ee_pos, ee_rot, gripper],</span><br><span class="line">        <span class="string">&quot;action&quot;</span>: [ee_delta, gripper_cmd],</span><br><span class="line">        <span class="string">&quot;encoder&quot;</span>: MLP_Franka,</span><br><span class="line">        <span class="string">&quot;decoder&quot;</span>: MLP_Franka</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;LAPA&quot;</span>: &#123;  <span class="comment"># 潜在动作</span></span><br><span class="line">        <span class="string">&quot;action&quot;</span>: [latent_embedding],</span><br><span class="line">        <span class="string">&quot;encoder&quot;</span>: MLP_LAPA,</span><br><span class="line">        <span class="string">&quot;decoder&quot;</span>: MLP_LAPA</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-统一训练框架"><a href="#6-统一训练框架" class="headerlink" title="6. 统一训练框架"></a>6. 统一训练框架</h4><p><strong>预训练阶段</strong>：</p>
<ul>
<li>全局batch size: 16,384</li>
<li>训练步数: 200,000</li>
<li>数据混合采样：真实机器人(40%) + 仿真(30%) + 神经(20%) + 人类视频(10%)</li>
<li>计算资源: 最多1024个H100 GPU，约50,000 GPU小时</li>
</ul>
<p><strong>后训练阶段</strong>：</p>
<ul>
<li>Batch size: 128-1024</li>
<li>训练步数: 20,000-60,000</li>
<li>可选神经轨迹协同训练（1:1采样比例）</li>
<li>可在单个A6000 GPU上微调</li>
</ul>
<hr>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><h3 id="预训练泛化能力"><a href="#预训练泛化能力" class="headerlink" title="预训练泛化能力"></a>预训练泛化能力</h3><p>在GR-1人形机器人上的零样本评估：</p>
<table>
<thead>
<tr>
<th>任务</th>
<th>成功率</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>左手抓取→右手交接→放置</td>
<td>76.6%</td>
<td>需要双手协调</td>
</tr>
<tr>
<td>新物体→新容器</td>
<td>73.3%</td>
<td>泛化到未见物体</td>
</tr>
</tbody></table>
<h3 id="仿真基准测试"><a href="#仿真基准测试" class="headerlink" title="仿真基准测试"></a>仿真基准测试</h3><p><strong>100条演示&#x2F;任务的性能对比</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>RoboCasa</th>
<th>DexMG</th>
<th>GR-1</th>
<th>平均</th>
</tr>
</thead>
<tbody><tr>
<td>BC-Transformer</td>
<td>26.3%</td>
<td>53.9%</td>
<td>16.1%</td>
<td>26.4%</td>
</tr>
<tr>
<td>Diffusion Policy</td>
<td>25.6%</td>
<td>56.1%</td>
<td>32.7%</td>
<td>33.4%</td>
</tr>
<tr>
<td><strong>GR00T-N1-2B</strong></td>
<td><strong>32.1%</strong></td>
<td><strong>66.5%</strong></td>
<td><strong>50.0%</strong></td>
<td><strong>45.0%</strong></td>
</tr>
</tbody></table>
<p><strong>关键观察</strong>：</p>
<ul>
<li>GR00T N1在所有基准上均优于基线</li>
<li>在GR-1任务上优势最明显（+17.3%）</li>
</ul>
<h3 id="真实世界部署"><a href="#真实世界部署" class="headerlink" title="真实世界部署"></a>真实世界部署</h3><p><strong>GR-1人形机器人任务成功率</strong>：</p>
<table>
<thead>
<tr>
<th>任务类型</th>
<th>Diffusion Policy<br>(10%数据)</th>
<th>Diffusion Policy<br>(全量数据)</th>
<th>GR00T-N1-2B<br>(10%数据)</th>
<th>GR00T-N1-2B<br>(全量数据)</th>
</tr>
</thead>
<tbody><tr>
<td>抓取放置</td>
<td>3.0%</td>
<td>36.0%</td>
<td><strong>35.0%</strong></td>
<td><strong>82.0%</strong></td>
</tr>
<tr>
<td>铰接物体</td>
<td>14.3%</td>
<td>38.6%</td>
<td><strong>62.0%</strong></td>
<td><strong>70.9%</strong></td>
</tr>
<tr>
<td>工业操作</td>
<td>6.7%</td>
<td>61.0%</td>
<td><strong>31.0%</strong></td>
<td><strong>70.0%</strong></td>
</tr>
<tr>
<td>多机协作</td>
<td>27.5%</td>
<td>62.5%</td>
<td><strong>50.0%</strong></td>
<td><strong>82.5%</strong></td>
</tr>
<tr>
<td><strong>平均</strong></td>
<td><strong>10.2%</strong></td>
<td><strong>46.4%</strong></td>
<td><strong>42.6%</strong></td>
<td><strong>76.8%</strong></td>
</tr>
</tbody></table>
<p><strong>数据效率</strong>：</p>
<ul>
<li>GR00T N1用10%数据（42.6%）≈ Diffusion Policy用全量数据（46.4%）</li>
<li>展现出色的样本效率</li>
</ul>
<h3 id="神经轨迹增强效果"><a href="#神经轨迹增强效果" class="headerlink" title="神经轨迹增强效果"></a>神经轨迹增强效果</h3><p><strong>RoboCasa基准（协同训练3K神经轨迹&#x2F;任务）</strong>：</p>
<table>
<thead>
<tr>
<th>数据量</th>
<th>仅真实数据</th>
<th>+LAPA</th>
<th>+IDM</th>
</tr>
</thead>
<tbody><tr>
<td>30条</td>
<td>17.4%</td>
<td>20.8% (+3.4%)</td>
<td>20.0% (+2.6%)</td>
</tr>
<tr>
<td>100条</td>
<td>32.1%</td>
<td>38.5% (+6.4%)</td>
<td>40.9% (+8.8%)</td>
</tr>
<tr>
<td>300条</td>
<td>49.6%</td>
<td>53.8% (+4.2%)</td>
<td>56.4% (+6.8%)</td>
</tr>
</tbody></table>
<p><strong>真实世界（协同训练100神经轨迹&#x2F;任务）</strong>：</p>
<ul>
<li>平均提升：+5.8%</li>
</ul>
<p><strong>观察</strong>：</p>
<ul>
<li>低数据场景：LAPA略优（更通用的先验）</li>
<li>高数据场景：IDM更优（更接近真实动作）</li>
</ul>
<h3 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h3><p><strong>运动质量</strong>：</p>
<ul>
<li>GR00T N1运动更流畅，抓取精度更高</li>
<li>Diffusion Policy常出现初始帧不动、抓取不准确</li>
</ul>
<p><strong>泛化能力</strong>：</p>
<ul>
<li>预训练模型能执行未见过的双手交接任务</li>
<li>后训练模型在特定任务上更精确，但失去部分泛化能力</li>
</ul>
<hr>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><h3 id="仿真基准"><a href="#仿真基准" class="headerlink" title="仿真基准"></a>仿真基准</h3><p><strong>RoboCasa Kitchen（24任务）</strong>：</p>
<ul>
<li>机器人：Franka Emika Panda</li>
<li>任务：抓取放置、开关门、按按钮、转水龙头等</li>
<li>观察：3个RGB相机（左、右、腕部）</li>
<li>动作：末端执行器相对位姿 + 夹爪状态</li>
<li>数据：每任务3,000条MimicGen生成的演示</li>
</ul>
<p><strong>DexMimicGen Cross-Embodiment Suite（9任务）</strong>：</p>
<ul>
<li>具身体：<ul>
<li>双臂Panda + 平行夹爪（穿线、组装、运输）</li>
<li>双臂Panda + 灵巧手（清理、抬托盘）</li>
<li>GR-1人形 + 灵巧手（倒水、咖啡、分类）</li>
</ul>
</li>
<li>数据：每任务1,000条演示</li>
</ul>
<p><strong>GR-1 Tabletop Tasks（24任务）</strong>：</p>
<ul>
<li>机器人：GR-1人形 + Fourier灵巧手</li>
<li>任务：18个重排任务 + 6个铰接物体任务</li>
<li>观察：头部自我中心相机</li>
<li>动作：关节位置&#x2F;旋转 + 腰部&#x2F;颈部</li>
<li>数据：每任务1,000条DexMimicGen生成</li>
</ul>
<h3 id="真实世界基准"><a href="#真实世界基准" class="headerlink" title="真实世界基准"></a>真实世界基准</h3><p><strong>任务类别</strong>：</p>
<ol>
<li><p><strong>抓取放置（5任务）</strong>：</p>
<ul>
<li>托盘→盘子、砧板→篮子、餐垫→碗等</li>
<li>评估：见过和未见过物体</li>
</ul>
</li>
<li><p><strong>铰接物体（3任务）</strong>：</p>
<ul>
<li>白色抽屉、深色柜子、木箱</li>
<li>要求：放入物体并关闭</li>
</ul>
</li>
<li><p><strong>工业操作（3任务）</strong>：</p>
<ul>
<li>机械零件打包</li>
<li>网格杯倾倒</li>
<li>圆柱体交接</li>
</ul>
</li>
<li><p><strong>多机协作（2任务）</strong>：</p>
<ul>
<li>第1部分：抓取→放入网格杯→交给另一机器人</li>
<li>第2部分：接收→放入黄色箱→倾倒剩余物</li>
</ul>
</li>
</ol>
<p><strong>数据收集</strong>：</p>
<ul>
<li>遥操作时长：15分钟-3小时&#x2F;任务</li>
<li>过滤低质量轨迹</li>
</ul>
<h3 id="评估协议"><a href="#评估协议" class="headerlink" title="评估协议"></a>评估协议</h3><p><strong>仿真</strong>：</p>
<ul>
<li>每任务100次试验</li>
<li>取最后5个checkpoint的最大值</li>
<li>Checkpoint间隔：500步</li>
</ul>
<p><strong>真实机器人</strong>：</p>
<ul>
<li>每任务10次试验（机械打包任务5次）</li>
<li>部分评分系统（捕捉不同执行阶段）</li>
<li>低数据场景：10%数据子采样</li>
</ul>
<h3 id="训练配置"><a href="#训练配置" class="headerlink" title="训练配置"></a>训练配置</h3><p><strong>预训练</strong>：</p>
<ul>
<li>学习率：1e-4</li>
<li>优化器：AdamW (β1&#x3D;0.95, β2&#x3D;0.999)</li>
<li>学习率调度：cosine，warmup比例0.05</li>
<li>Batch size：16,384</li>
<li>步数：200,000</li>
</ul>
<p><strong>后训练</strong>：</p>
<ul>
<li>Batch size：128-1024</li>
<li>步数：20,000-60,000</li>
<li>其他超参数同预训练</li>
</ul>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ol>
<li><p><strong>统一的跨具身学习</strong>：</p>
<ul>
<li>单一模型支持从桌面机械臂到双臂人形机器人</li>
<li>潜在动作空间统一不同具身体</li>
</ul>
</li>
<li><p><strong>卓越的数据效率</strong>：</p>
<ul>
<li>10%数据达到基线全量数据性能</li>
<li>预训练提供强大的先验知识</li>
</ul>
</li>
<li><p><strong>可扩展的数据生成</strong>：</p>
<ul>
<li>神经轨迹生成：10倍数据扩增</li>
<li>仿真自动生成：11小时生成6,500小时等效数据</li>
</ul>
</li>
<li><p><strong>端到端优化</strong>：</p>
<ul>
<li>VLM推理与DiT控制联合训练</li>
<li>避免分层方法的接口问题</li>
</ul>
</li>
<li><p><strong>开源生态</strong>：</p>
<ul>
<li>公开22亿参数模型</li>
<li>提供训练数据和仿真基准</li>
</ul>
</li>
</ol>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ol>
<li><p><strong>任务范围限制</strong>：</p>
<ul>
<li>当前主要关注短时域桌面操作</li>
<li>未涉及长时域移动操作（loco-manipulation）</li>
</ul>
</li>
<li><p><strong>合成数据质量</strong>：</p>
<ul>
<li>视频生成模型仍面临多样性和物理一致性挑战</li>
<li>需要质量过滤和重新标注</li>
</ul>
</li>
<li><p><strong>硬件依赖</strong>：</p>
<ul>
<li>需要高端GPU进行训练（H100集群）</li>
<li>推理需要L40 GPU（63.9ms&#x2F;16动作）</li>
</ul>
</li>
<li><p><strong>泛化-专精权衡</strong>：</p>
<ul>
<li>后训练提升特定任务性能但损失部分泛化能力</li>
<li>预训练模型能执行双手交接，后训练模型失去此能力</li>
</ul>
</li>
<li><p><strong>视觉-语言骨干限制</strong>：</p>
<ul>
<li>当前VLM的空间推理和语言理解能力仍有提升空间</li>
<li>更强的VLM可能进一步提升性能</li>
</ul>
</li>
</ol>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="机器人基础模型"><a href="#机器人基础模型" class="headerlink" title="机器人基础模型"></a>机器人基础模型</h3><p><strong>VLA模型</strong>：</p>
<ul>
<li><strong>RT-1&#x2F;RT-2</strong> (Brohan et al., 2022, 2023)：早期VLA模型，使用Transformer架构</li>
<li><strong>π0</strong> (Black et al., 2024)：使用mixture-of-experts连接VLM和动作生成</li>
<li><strong>Octo</strong> (Octo Model Team et al., 2024)：跨具身模型，但不微调VLM</li>
<li><strong>GR-2</strong> (Cheang et al., 2024)：视频-语言-动作模型</li>
</ul>
<p><strong>GR00T N1的区别</strong>：</p>
<ul>
<li>使用简单的cross-attention而非MoE</li>
<li>端到端微调VLM视觉编码器</li>
<li>支持潜在动作和IDM伪动作</li>
</ul>
<h3 id="机器人数据集"><a href="#机器人数据集" class="headerlink" title="机器人数据集"></a>机器人数据集</h3><p><strong>真实机器人数据</strong>：</p>
<ul>
<li><strong>Open X-Embodiment</strong> (2024)：跨具身数据集联盟</li>
<li><strong>AgiBot-Alpha</strong> (2025)：100个机器人的大规模数据集</li>
<li><strong>遥操作系统</strong>：VIVE、Apple Vision Pro、Leap Motion</li>
</ul>
<p><strong>人类视频数据</strong>：</p>
<ul>
<li><strong>Ego4D</strong> (Grauman et al., 2022)：大规模自我中心视频</li>
<li><strong>EPIC-KITCHENS</strong> (Damen et al., 2018)：厨房活动</li>
<li><strong>Assembly-101</strong> (Sener et al., 2022)：组装任务</li>
</ul>
<p><strong>GR00T N1的创新</strong>：</p>
<ul>
<li>数据金字塔组织而非简单混合</li>
<li>潜在动作统一有&#x2F;无标签数据</li>
</ul>
<h3 id="合成数据生成"><a href="#合成数据生成" class="headerlink" title="合成数据生成"></a>合成数据生成</h3><p><strong>仿真数据</strong>：</p>
<ul>
<li><strong>MimicGen</strong> (Mandlekar et al., 2023)：演示变换和重放</li>
<li><strong>DexMimicGen</strong> (Jiang et al., 2024)：灵巧操作数据生成</li>
<li><strong>RoboCasa</strong> (Nasiriany et al., 2024)：厨房环境仿真</li>
</ul>
<p><strong>神经生成</strong>：</p>
<ul>
<li><strong>视频生成模型</strong>：Sora (Brooks et al., 2024)、WAN (Wan Team, 2025)</li>
<li><strong>数据增强</strong>：GenAug (Chen et al., 2023)使用扩散模型增强</li>
</ul>
<p><strong>GR00T N1的规模</strong>：</p>
<ul>
<li>827小时神经轨迹（前所未有）</li>
<li>540K仿真轨迹（11小时生成）</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><ol>
<li><p><strong>长时域移动操作</strong>：</p>
<ul>
<li>扩展到全身运动和导航</li>
<li>需要改进硬件、模型架构和训练数据</li>
</ul>
</li>
<li><p><strong>更强的视觉-语言骨干</strong>：</p>
<ul>
<li>提升空间推理能力</li>
<li>增强语言理解和任务规划</li>
</ul>
</li>
<li><p><strong>改进合成数据生成</strong>：</p>
<ul>
<li>提高视频生成的多样性和反事实能力</li>
<li>增强物理一致性和真实感</li>
<li>探索自动化初始帧生成（img2img扩散）</li>
</ul>
</li>
<li><p><strong>新型模型架构</strong>：</p>
<ul>
<li>探索更高效的推理-控制耦合方式</li>
<li>研究分层时间建模</li>
</ul>
</li>
<li><p><strong>鲁棒性和泛化</strong>：</p>
<ul>
<li>提升对环境变化的适应能力</li>
<li>增强零样本和少样本学习能力</li>
</ul>
</li>
<li><p><strong>多模态感知</strong>：</p>
<ul>
<li>整合触觉、力觉等其他传感器</li>
<li>探索多模态融合策略</li>
</ul>
</li>
<li><p><strong>长时域视频生成</strong>：</p>
<ul>
<li>多轮视频生成实现长任务序列</li>
<li>原子任务组合</li>
</ul>
</li>
</ol>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>NVIDIA (2025). GR00T N1: An Open Foundation Model for Generalist Humanoid Robots. arXiv:2503.14734v2.</li>
<li>Black et al. (2024). π0: A vision-language-action flow model for general robot control. arXiv:2410.24164.</li>
<li>Brohan et al. (2022). RT-1: Robotics transformer for real-world control at scale. arXiv:2212.06817.</li>
<li>Brohan et al. (2023). RT-2: Vision-language-action models transfer web knowledge to robotic control. arXiv:2307.15818.</li>
<li>Chi et al. (2024). Diffusion Policy: Visuomotor policy learning via action diffusion. IJRR.</li>
<li>Jiang et al. (2024). DexMimicGen: Automated data generation for bimanual dexterous manipulation via imitation learning. CoRL.</li>
<li>Mandlekar et al. (2023). MimicGen: A data generation system for scalable robot learning using human demonstrations. CoRL.</li>
<li>Nasiriany et al. (2024). RoboCasa: Large-scale simulation of everyday tasks for generalist robots. RSS.</li>
<li>Open X-Embodiment Collaboration et al. (2024). Open X-Embodiment: Robotic learning datasets and RT-X models.</li>
<li>Ye et al. (2025). Latent action pretraining from videos. ICLR.</li>
<li>Kahneman (2011). Thinking, Fast and Slow. Farrar, Straus and Giroux.</li>
</ul>
<hr>
<h2 id="关键代码和资源"><a href="#关键代码和资源" class="headerlink" title="关键代码和资源"></a>关键代码和资源</h2><ul>
<li><strong>模型权重</strong>：<a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/groot-n1-2b">HuggingFace</a></li>
<li><strong>训练数据</strong>：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/groot-n1-data">HuggingFace Datasets</a></li>
<li><strong>仿真基准</strong>：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/GR00T">GitHub</a></li>
<li><strong>数据格式</strong>：基于LeRobot格式扩展</li>
<li><strong>训练基础设施</strong>：NVIDIA OSMO编排平台</li>
</ul>
<hr>
<h2 id="技术细节补充"><a href="#技术细节补充" class="headerlink" title="技术细节补充"></a>技术细节补充</h2><h3 id="动作空间标准化"><a href="#动作空间标准化" class="headerlink" title="动作空间标准化"></a>动作空间标准化</h3><p><strong>统一不同具身体的表示</strong>：</p>
<ul>
<li>末端执行器旋转状态：6D旋转表示</li>
<li>末端执行器旋转动作：轴角表示</li>
<li>位置和关节：Min-max归一化</li>
<li>顺序：左臂→右臂，旋转→位置→夹爪</li>
</ul>
<h3 id="辅助目标检测损失"><a href="#辅助目标检测损失" class="headerlink" title="辅助目标检测损失"></a>辅助目标检测损失</h3><p>使用OWL-v2检测器标注目标物体边界框：</p>
<p>$$<br>\mathcal{L}<em>{det} &#x3D; |\mathbf{x}</em>{pred} - \mathbf{x}_{gt}|^2<br>$$</p>
<p>其中 $\mathbf{x}$ 是归一化的边界框中心坐标。</p>
<h3 id="推理性能"><a href="#推理性能" class="headerlink" title="推理性能"></a>推理性能</h3><ul>
<li><strong>GR00T-N1-2B</strong>：63.9ms采样16步动作（L40 GPU，bf16）</li>
<li><strong>VLM频率</strong>：10Hz</li>
<li><strong>动作输出频率</strong>：120Hz</li>
<li><strong>去噪步数</strong>：K&#x3D;4</li>
</ul>
<h3 id="计算资源"><a href="#计算资源" class="headerlink" title="计算资源"></a>计算资源</h3><ul>
<li><strong>预训练</strong>：最多1024个H100 GPU，约50,000 GPU小时</li>
<li><strong>神经轨迹生成</strong>：3,600个L40 GPU，约105K GPU小时（1.5天）</li>
<li><strong>后训练</strong>：单个A6000 GPU可微调（仅adapter层时batch size可达200）</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2026/02/02/%5BOBS%5DDeep%20Learning-Robot%20Learnning-VQ-VAE-and-Latent-Action-for-Robotics/"><img class="fill" src="/gallery/Research-paper.png" alt="VQ-VAE and Latent Action for Robotics" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2026-02-02T11:30:00.000Z" title="2/2/2026, 7:30:00 PM">2026-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.522Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">22 minutes read (About 3259 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2026/02/02/%5BOBS%5DDeep%20Learning-Robot%20Learnning-VQ-VAE-and-Latent-Action-for-Robotics/">VQ-VAE and Latent Action for Robotics</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2026/02/02/[OBS]Deep Learning-Robot Learnning-VQ-VAE-and-Latent-Action-for-Robotics/Pasted_image_20260203120727.png" title="" title=" class="gallery-item"><img src="/2026/02/02/[OBS]Deep Learning-Robot Learnning-VQ-VAE-and-Latent-Action-for-Robotics/Pasted_image_20260203120727.png" alt="" title=""></a></div>
# VQ-VAE与机器人Latent Action

<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">Neural Discrete Representation Learning</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.03181">VQ-BeT: Behavior Generation with Latent Actions</a></p>
<hr>
<h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>在无监督学习和机器人学习领域，表示学习是核心问题之一。传统的变分自编码器（VAE, Variational AutoEncoder）使用连续潜在变量，但存在后验崩塌（posterior collapse）问题，即解码器过强导致忽略潜在编码。</p>
<p>在机器人学习中，直接学习连续高维动作空间面临以下挑战：</p>
<ul>
<li>动作分布通常是多模态的（如抓取物体可以有多种方式）</li>
<li>行为克隆（Behavior Cloning）容易产生平均化的次优动作</li>
<li>连续动作空间的策略学习不稳定</li>
</ul>
<p>VQ-VAE（Vector Quantised-Variational AutoEncoder）通过引入离散潜在变量，为这些问题提供了有效的解决方案。</p>
<hr>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><h3 id="VQ-VAE的核心目标"><a href="#VQ-VAE的核心目标" class="headerlink" title="VQ-VAE的核心目标"></a>VQ-VAE的核心目标</h3><ol>
<li>解决VAE中的后验崩塌问题</li>
<li>学习有效的离散表示，适用于本质上离散的数据（语言、语音等）</li>
<li>实现端到端的离散表示学习</li>
</ol>
<h3 id="机器人Latent-Action的目标"><a href="#机器人Latent-Action的目标" class="headerlink" title="机器人Latent Action的目标"></a>机器人Latent Action的目标</h3><ol>
<li>将连续高维动作空间压缩为离散的动作原语（action primitives）</li>
<li>在离散空间中学习更稳定的策略</li>
<li>实现时序动作抽象，降低决策频率</li>
<li>提升多模态动作分布的建模能力</li>
</ol>
<hr>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="VQ-VAE-Vector-Quantised-Variational-AutoEncoder"><a href="#VQ-VAE-Vector-Quantised-Variational-AutoEncoder" class="headerlink" title="VQ-VAE (Vector Quantised-Variational AutoEncoder)"></a>VQ-VAE (Vector Quantised-Variational AutoEncoder)</h3><p>VQ-VAE是一种使用离散潜在变量的生成模型，通过向量量化（Vector Quantization）技术将编码器输出映射到离散的码本空间。</p>
<p><strong>关键组件：</strong></p>
<ul>
<li><strong>编码器（Encoder）</strong>：将输入映射到连续潜在空间</li>
<li><strong>码本（Codebook）</strong>：包含 $K$ 个 $d$ 维向量 $\mathbf{e} \in \mathbb{R}^{K \times d}$</li>
<li><strong>量化层（Quantization）</strong>：将连续表示映射到最近的码本向量</li>
<li><strong>解码器（Decoder）</strong>：从离散表示重建输入</li>
</ul>
<h3 id="Latent-Action（潜在动作）"><a href="#Latent-Action（潜在动作）" class="headerlink" title="Latent Action（潜在动作）"></a>Latent Action（潜在动作）</h3><p>Latent Action是将连续动作序列编码为离散token的表示方法。每个离散token代表一个”动作原语”或”技能”，可以解码为一段连续的动作序列。</p>
<p><strong>核心思想：</strong></p>
<ul>
<li>将动作序列 $\mathbf{a}_{t:t+H}$ 编码为单个离散索引 $z \in {1, …, K}$</li>
<li>策略网络在离散空间中选择动作：$\pi(\mathbf{o}_t) \rightarrow z$</li>
<li>解码器将离散索引恢复为连续动作：$z \rightarrow \mathbf{a}_{t:t+H}$</li>
</ul>
<hr>
<h2 id="VQ-VAE方法详解"><a href="#VQ-VAE方法详解" class="headerlink" title="VQ-VAE方法详解"></a>VQ-VAE方法详解</h2><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">输入 x</span><br><span class="line">  ↓</span><br><span class="line">[编码器] Encoder</span><br><span class="line">  ↓</span><br><span class="line">z_e(x) ∈ R^(H×W×D)  (连续潜在表示)</span><br><span class="line">  ↓</span><br><span class="line">[向量量化] Vector Quantization</span><br><span class="line">  ↓</span><br><span class="line">z_q(x) ∈ R^(H×W×D)  (离散潜在表示)</span><br><span class="line">  ↓</span><br><span class="line">[解码器] Decoder</span><br><span class="line">  ↓</span><br><span class="line">重建输出 x̂</span><br></pre></td></tr></table></figure>

<h3 id="向量量化过程"><a href="#向量量化过程" class="headerlink" title="向量量化过程"></a>向量量化过程</h3><p>对于编码器输出的每个空间位置，找到最近的码本向量：</p>
<p>$$<br>z_q(\mathbf{x}) &#x3D; \mathbf{e}_k, \quad \text{where} \quad k &#x3D; \arg\min_j |\mathbf{z}_e(\mathbf{x}) - \mathbf{e}_j|_2<br>$$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>VQ-VAE使用三部分损失函数：</p>
<p>$$<br>L &#x3D; \log p(\mathbf{x}|\mathbf{z}_q(\mathbf{x})) + |\text{sg}[\mathbf{z}_e(\mathbf{x})] - \mathbf{e}|_2^2 + \beta |\mathbf{z}_e(\mathbf{x}) - \text{sg}[\mathbf{e}]|_2^2<br>$$</p>
<p>其中：</p>
<ul>
<li><strong>重建损失（Reconstruction Loss）</strong>：$\log p(\mathbf{x}|\mathbf{z}_q(\mathbf{x}))$，确保重建质量</li>
<li><strong>码本损失（Codebook Loss）</strong>：$|\text{sg}[\mathbf{z}_e(\mathbf{x})] - \mathbf{e}|_2^2$，更新码本向量使其靠近编码器输出</li>
<li><strong>承诺损失（Commitment Loss）</strong>：$\beta |\mathbf{z}_e(\mathbf{x}) - \text{sg}[\mathbf{e}]|_2^2$，鼓励编码器输出靠近码本向量（$\beta &#x3D; 0.25$）</li>
</ul>
<p>其中 $\text{sg}[\cdot]$ 表示stop gradient操作，阻止梯度传播。</p>
<h3 id="Straight-Through-Estimator"><a href="#Straight-Through-Estimator" class="headerlink" title="Straight-Through Estimator"></a>Straight-Through Estimator</h3><p><strong>问题</strong>：量化操作 $\mathbf{z}<em>q &#x3D; \arg\min</em>{\mathbf{e}} |\mathbf{z}_e - \mathbf{e}|$ 不可微分</p>
<p><strong>解决方案</strong>：在反向传播时，将解码器的梯度直接复制给编码器：</p>
<p>$$<br>\nabla_{\mathbf{z}<em>e} L &#x3D; \nabla</em>{\mathbf{z}_q} L<br>$$</p>
<p>即在前向传播使用离散的 $\mathbf{z}_q$，在反向传播时假装量化操作是恒等映射。</p>
<h3 id="数据尺度变化示例"><a href="#数据尺度变化示例" class="headerlink" title="数据尺度变化示例"></a>数据尺度变化示例</h3><p>以CIFAR-10图像重建为例：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>数据形状</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>输入图像</td>
<td><code>[Batch, 32, 32, 3]</code></td>
<td>原始RGB图像</td>
</tr>
<tr>
<td>编码器输出 $\mathbf{z}_e$</td>
<td><code>[Batch, 8, 8, 64]</code></td>
<td>空间下采样4倍，通道数64</td>
</tr>
<tr>
<td>量化后 $\mathbf{z}_q$</td>
<td><code>[Batch, 8, 8, 64]</code></td>
<td>形状不变，但值被离散化</td>
</tr>
<tr>
<td>解码器输出</td>
<td><code>[Batch, 32, 32, 3]</code></td>
<td>重建图像</td>
</tr>
</tbody></table>
<p><strong>信息压缩率</strong>：$(32 \times 32 \times 3) &#x2F; (8 \times 8 \times \log_2 512) \approx 42$ 倍压缩（假设码本大小 $K&#x3D;512$）</p>
<hr>
<h2 id="VQ-VAE在机器人中的应用"><a href="#VQ-VAE在机器人中的应用" class="headerlink" title="VQ-VAE在机器人中的应用"></a>VQ-VAE在机器人中的应用</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">观察 o_t (图像/状态)</span><br><span class="line">    ↓</span><br><span class="line">[策略网络 π]</span><br><span class="line">    ↓</span><br><span class="line">离散latent action z ∈ &#123;1,...,K&#125;</span><br><span class="line">    ↓</span><br><span class="line">[VQ-VAE解码器]</span><br><span class="line">    ↓</span><br><span class="line">连续动作序列 a_&#123;t:t+H&#125;</span><br><span class="line">    ↓</span><br><span class="line">执行到机器人</span><br></pre></td></tr></table></figure>

<h3 id="动作序列编码"><a href="#动作序列编码" class="headerlink" title="动作序列编码"></a>动作序列编码</h3><p><strong>输入</strong>：动作序列 $\mathbf{a}_{t:t+H} \in \mathbb{R}^{H \times d_a}$，其中 $H$ 是序列长度，$d_a$ 是动作维度</p>
<p><strong>编码过程</strong>：</p>
<ol>
<li>通过1D卷积或Transformer编码时序信息</li>
<li>输出单个向量 $\mathbf{z}_e \in \mathbb{R}^D$</li>
<li>量化为离散索引 $k \in {1, …, K}$</li>
</ol>
<p><strong>解码过程</strong>：</p>
<ol>
<li>从码本中查找向量 $\mathbf{e}_k$</li>
<li>通过解码器生成动作序列 $\hat{\mathbf{a}}_{t:t+H}$</li>
</ol>
<h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><h4 id="阶段1：训练VQ-VAE"><a href="#阶段1：训练VQ-VAE" class="headerlink" title="阶段1：训练VQ-VAE"></a>阶段1：训练VQ-VAE</h4><p>使用专家演示数据训练VQ-VAE：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> expert_demonstrations:</span><br><span class="line">    action_seq = batch[<span class="string">&#x27;actions&#x27;</span>]  <span class="comment"># [B, H, action_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码-量化-解码</span></span><br><span class="line">    z_e = encoder(action_seq)</span><br><span class="line">    z_q = quantize(z_e, codebook)</span><br><span class="line">    action_recon = decoder(z_q)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 三部分损失</span></span><br><span class="line">    loss_recon = MSE(action_seq, action_recon)</span><br><span class="line">    loss_vq = MSE(sg(z_e), z_q)</span><br><span class="line">    loss_commit = MSE(z_e, sg(z_q))</span><br><span class="line"></span><br><span class="line">    loss = loss_recon + loss_vq + <span class="number">0.25</span> * loss_commit</span><br></pre></td></tr></table></figure>

<h4 id="阶段2：训练策略网络"><a href="#阶段2：训练策略网络" class="headerlink" title="阶段2：训练策略网络"></a>阶段2：训练策略网络</h4><p>固定VQ-VAE，训练策略在离散空间中选择动作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> demonstrations:</span><br><span class="line">    obs = batch[<span class="string">&#x27;observations&#x27;</span>]  <span class="comment"># [B, T, obs_dim]</span></span><br><span class="line">    actions = batch[<span class="string">&#x27;actions&#x27;</span>]   <span class="comment"># [B, T, action_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将动作编码为离散token</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        z_indices = vqvae.encode(actions)  <span class="comment"># [B, T//H]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练策略预测离散token</span></span><br><span class="line">    z_pred = policy(obs)  <span class="comment"># [B, T//H, K]</span></span><br><span class="line">    loss = CrossEntropy(z_pred, z_indices)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="主要应用案例"><a href="#主要应用案例" class="headerlink" title="主要应用案例"></a>主要应用案例</h2><h3 id="VQ-BeT-VQ-Behavior-Transformer"><a href="#VQ-BeT-VQ-Behavior-Transformer" class="headerlink" title="VQ-BeT (VQ-Behavior Transformer)"></a>VQ-BeT (VQ-Behavior Transformer)</h3><p><strong>论文</strong>：Behavior Generation with Latent Actions (CoRL 2023)</p>
<p><strong>核心思想</strong>：</p>
<ol>
<li>使用VQ-VAE将动作序列压缩为离散token</li>
<li>使用Transformer建模观察到latent action的映射：$p(z_t | \mathbf{o}_{1:t})$</li>
<li>执行时解码latent action为连续动作序列</li>
</ol>
<p><strong>优势</strong>：</p>
<ul>
<li>有效处理多模态动作分布</li>
<li>避免行为克隆中的动作平均化问题</li>
<li>支持长时序动作规划（一次预测多步）</li>
</ul>
<h3 id="LISA-Latent-Imagination-with-Skill-Abstraction"><a href="#LISA-Latent-Imagination-with-Skill-Abstraction" class="headerlink" title="LISA (Latent Imagination with Skill Abstraction)"></a>LISA (Latent Imagination with Skill Abstraction)</h3><p><strong>核心思想</strong>：结合世界模型和latent action</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当前状态 s_t</span><br><span class="line">    ↓</span><br><span class="line">[世界模型] 在latent space中想象</span><br><span class="line">    ↓</span><br><span class="line">预测未来状态序列 ŝ_&#123;t+1:t+H&#125;</span><br><span class="line">    ↓</span><br><span class="line">[规划器] 选择最优latent action z*</span><br><span class="line">    ↓</span><br><span class="line">[VQ解码器] z* → 连续动作</span><br></pre></td></tr></table></figure>

<h3 id="SPiRL-Skill-based-Model-based-RL"><a href="#SPiRL-Skill-based-Model-based-RL" class="headerlink" title="SPiRL (Skill-based Model-based RL)"></a>SPiRL (Skill-based Model-based RL)</h3><p>将VQ-VAE学习的离散表示视为”技能”，在强化学习中进行技能级别的规划。</p>
<hr>
<h2 id="实验设计与结果"><a href="#实验设计与结果" class="headerlink" title="实验设计与结果"></a>实验设计与结果</h2><h3 id="VQ-VAE实验（原始论文）"><a href="#VQ-VAE实验（原始论文）" class="headerlink" title="VQ-VAE实验（原始论文）"></a>VQ-VAE实验（原始论文）</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li><strong>CIFAR-10</strong>：32×32彩色图像</li>
<li><strong>ImageNet</strong>：128×128和256×256图像</li>
<li><strong>VCTK语音数据集</strong>：英语语音数据</li>
<li><strong>DeepMind Lab</strong>：强化学习环境视频</li>
</ul>
<h4 id="关键参数"><a href="#关键参数" class="headerlink" title="关键参数"></a>关键参数</h4><ul>
<li>码本大小 $K$：512</li>
<li>编码维度 $D$：64</li>
<li>承诺损失系数 $\beta$：0.25</li>
</ul>
<h4 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h4><table>
<thead>
<tr>
<th>任务</th>
<th>指标</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>图像重建（CIFAR-10）</td>
<td>重建质量</td>
<td>与连续VAE相当</td>
</tr>
<tr>
<td>音频重建（VCTK）</td>
<td>感知质量</td>
<td>接近原始音频</td>
</tr>
<tr>
<td>说话人分类</td>
<td>准确率</td>
<td>49.3%（从41维编码）</td>
</tr>
<tr>
<td>视频建模</td>
<td>表示质量</td>
<td>成功捕获时序信息</td>
</tr>
</tbody></table>
<h3 id="机器人Latent-Action实验"><a href="#机器人Latent-Action实验" class="headerlink" title="机器人Latent Action实验"></a>机器人Latent Action实验</h3><h4 id="超参数选择"><a href="#超参数选择" class="headerlink" title="超参数选择"></a>超参数选择</h4><table>
<thead>
<tr>
<th>参数</th>
<th>简单任务</th>
<th>复杂任务</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>码本大小 $K$</td>
<td>16-64</td>
<td>128-512</td>
<td>过小表达能力不足，过大难以学习</td>
</tr>
<tr>
<td>序列长度 $H$</td>
<td>10-20</td>
<td>10-20</td>
<td>过小失去时序抽象，过大误差累积</td>
</tr>
<tr>
<td>编码维度 $D$</td>
<td>64-128</td>
<td>128-256</td>
<td>根据动作复杂度调整</td>
</tr>
</tbody></table>
<h4 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h4><p>VQ-BeT在多个机器人操作任务上的表现：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>成功率</th>
<th>多模态处理</th>
<th>训练稳定性</th>
</tr>
</thead>
<tbody><tr>
<td>传统BC</td>
<td>65%</td>
<td>差</td>
<td>中等</td>
</tr>
<tr>
<td>Diffusion Policy</td>
<td>78%</td>
<td>好</td>
<td>较慢</td>
</tr>
<tr>
<td>VQ-BeT</td>
<td>82%</td>
<td>优秀</td>
<td>快速稳定</td>
</tr>
</tbody></table>
<hr>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><p><strong>VQ-VAE本身：</strong></p>
<ul>
<li>避免后验崩塌问题，潜在编码被充分利用</li>
<li>离散表示更适合某些模态（语言、符号）</li>
<li>可以学习到有意义的离散结构</li>
</ul>
<p><strong>在机器人中的优势：</strong></p>
<ul>
<li><strong>多模态建模</strong>：离散分类比连续回归更容易处理多模态动作分布</li>
<li><strong>时序抽象</strong>：一个latent action代表一段动作序列，降低决策频率</li>
<li><strong>训练稳定性</strong>：离散空间避免连续动作的梯度不稳定</li>
<li><strong>可解释性</strong>：码本向量可视为”技能原语”，便于分析和调试</li>
<li><strong>泛化能力</strong>：学到的动作原语可以组合应用到新场景</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p><strong>VQ-VAE的挑战：</strong></p>
<ul>
<li>码本利用率问题（codebook collapse）：部分码本向量可能不被使用</li>
<li>重建误差：离散化导致信息损失</li>
<li>超参数敏感：$K$、$D$、$\beta$ 需要仔细调优</li>
</ul>
<p><strong>机器人应用的挑战：</strong></p>
<ul>
<li><strong>重建精度</strong>：VQ-VAE无法完美重建动作，影响执行精度</li>
<li><strong>序列长度选择</strong>：$H$ 的选择需要在抽象能力和精确控制之间权衡</li>
<li><strong>计算开销</strong>：需要额外训练VQ-VAE模型</li>
<li><strong>在线适应</strong>：预训练的码本可能不适合新任务</li>
</ul>
<hr>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="离散表示学习"><a href="#离散表示学习" class="headerlink" title="离散表示学习"></a>离散表示学习</h3><ul>
<li><strong>VQ-VAE-2</strong> (Razavi et al., 2019)：层次化VQ-VAE，提升生成质量</li>
<li><strong>DALL-E</strong> (Ramesh et al., 2021)：使用VQ-VAE的离散表示进行文本到图像生成</li>
<li><strong>Gumbel-Softmax VAE</strong>：另一种离散VAE方法，使用Gumbel-Softmax技巧</li>
</ul>
<h3 id="机器人技能学习"><a href="#机器人技能学习" class="headerlink" title="机器人技能学习"></a>机器人技能学习</h3><ul>
<li><strong>Skill Discovery</strong>：无监督发现技能的方法（DIAYN, DADS等）</li>
<li><strong>Hierarchical RL</strong>：层次化强化学习，在不同抽象层次上决策</li>
<li><strong>Option Framework</strong>：时序抽象的经典框架</li>
</ul>
<h3 id="行为克隆与模仿学习"><a href="#行为克隆与模仿学习" class="headerlink" title="行为克隆与模仿学习"></a>行为克隆与模仿学习</h3><ul>
<li><strong>Diffusion Policy</strong> (Chi et al., 2023)：使用扩散模型生成动作</li>
<li><strong>Action Chunking Transformer</strong> (Zhao et al., 2023)：直接预测动作序列</li>
<li><strong>BeT</strong> (Shafiullah et al., 2022)：使用离散化动作的行为Transformer</li>
</ul>
<hr>
<h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><h3 id="方法改进"><a href="#方法改进" class="headerlink" title="方法改进"></a>方法改进</h3><ol>
<li><p><strong>层次化VQ-VAE</strong>：</p>
<ul>
<li>高层策略选择宏观latent action</li>
<li>低层策略选择微观latent action</li>
<li>实现多层次的时序抽象</li>
</ul>
</li>
<li><p><strong>与扩散模型结合</strong>：</p>
<ul>
<li>使用VQ-VAE的离散表示作为扩散模型的条件</li>
<li>在离散空间规划，在连续空间精细化</li>
<li>结合两者优势：稳定性+精确性</li>
</ul>
</li>
<li><p><strong>在线学习与适应</strong>：</p>
<ul>
<li>预训练VQ-VAE在大规模数据上</li>
<li>在新任务上微调策略网络</li>
<li>探索码本的在线更新机制</li>
</ul>
</li>
<li><p><strong>解决码本崩塌</strong>：</p>
<ul>
<li>使用EMA（指数移动平均）更新码本</li>
<li>引入正则化鼓励码本多样性</li>
<li>动态调整码本大小</li>
</ul>
</li>
</ol>
<h3 id="应用拓展"><a href="#应用拓展" class="headerlink" title="应用拓展"></a>应用拓展</h3><ol>
<li><p><strong>多模态机器人学习</strong>：</p>
<ul>
<li>结合视觉、触觉、本体感觉</li>
<li>学习跨模态的统一表示</li>
</ul>
</li>
<li><p><strong>长时序任务规划</strong>：</p>
<ul>
<li>在latent action空间进行任务规划</li>
<li>结合符号推理和连续控制</li>
</ul>
</li>
<li><p><strong>迁移学习</strong>：</p>
<ul>
<li>在源任务上学习通用动作原语</li>
<li>在目标任务上组合和微调</li>
</ul>
</li>
<li><p><strong>人机协作</strong>：</p>
<ul>
<li>可解释的动作原语便于人类理解</li>
<li>支持人类通过选择latent action进行干预</li>
</ul>
</li>
</ol>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><h3 id="核心论文"><a href="#核心论文" class="headerlink" title="核心论文"></a>核心论文</h3><ul>
<li>van den Oord, A., Vinyals, O., &amp; Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. NIPS 2017. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">arXiv:1711.00937</a></li>
<li>Shafiullah, N. M. M., et al. (2023). Behavior Generation with Latent Actions (VQ-BeT). CoRL 2023. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2403.03181">arXiv:2403.03181</a></li>
</ul>
<h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><ul>
<li>Razavi, A., et al. (2019). Generating Diverse High-Fidelity Images with VQ-VAE-2. NeurIPS 2019.</li>
<li>Pertsch, K., et al. (2020). Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL). CoRL 2020.</li>
<li>Lynch, C., &amp; Sermanet, P. (2020). Learning Latent Plans from Play (LISA). CoRL 2020.</li>
<li>Chi, C., et al. (2023). Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. RSS 2023.</li>
</ul>
<hr>
<h2 id="关键代码示例"><a href="#关键代码示例" class="headerlink" title="关键代码示例"></a>关键代码示例</h2><h3 id="VQ-VAE量化层实现"><a href="#VQ-VAE量化层实现" class="headerlink" title="VQ-VAE量化层实现"></a>VQ-VAE量化层实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VectorQuantizer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim, commitment_cost=<span class="number">0.25</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding_dim = embedding_dim</span><br><span class="line">        <span class="variable language_">self</span>.num_embeddings = num_embeddings</span><br><span class="line">        <span class="variable language_">self</span>.commitment_cost = commitment_cost</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化码本</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.embedding.weight.data.uniform_(-<span class="number">1</span>/num_embeddings, <span class="number">1</span>/num_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z_e</span>):</span><br><span class="line">        <span class="comment"># z_e: [B, D] 编码器输出</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算距离</span></span><br><span class="line">        distances = torch.<span class="built_in">sum</span>(z_e**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">                    torch.<span class="built_in">sum</span>(<span class="variable language_">self</span>.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - \</span><br><span class="line">                    <span class="number">2</span> * torch.matmul(z_e, <span class="variable language_">self</span>.embedding.weight.t())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到最近的码本向量</span></span><br><span class="line">        encoding_indices = torch.argmin(distances, dim=<span class="number">1</span>)</span><br><span class="line">        z_q = <span class="variable language_">self</span>.embedding(encoding_indices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        e_latent_loss = torch.mean((z_q.detach() - z_e)**<span class="number">2</span>)  <span class="comment"># 码本损失</span></span><br><span class="line">        q_latent_loss = torch.mean((z_q - z_e.detach())**<span class="number">2</span>)  <span class="comment"># 承诺损失</span></span><br><span class="line">        loss = e_latent_loss + <span class="variable language_">self</span>.commitment_cost * q_latent_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Straight-through estimator</span></span><br><span class="line">        z_q = z_e + (z_q - z_e).detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z_q, loss, encoding_indices</span><br></pre></td></tr></table></figure>

<h3 id="动作序列编码器"><a href="#动作序列编码器" class="headerlink" title="动作序列编码器"></a>动作序列编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ActionEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, action_dim, hidden_dim, latent_dim, seq_len</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1D卷积编码时序信息</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv1d(action_dim, hidden_dim, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv1d(hidden_dim, hidden_dim*<span class="number">2</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_dim*<span class="number">2</span> * (seq_len//<span class="number">4</span>), latent_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, action_seq</span>):</span><br><span class="line">        <span class="comment"># action_seq: [B, seq_len, action_dim]</span></span><br><span class="line">        x = action_seq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B, action_dim, seq_len]</span></span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv1(x))</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.conv2(x))</span><br><span class="line">        x = x.flatten(<span class="number">1</span>)</span><br><span class="line">        z_e = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> z_e</span><br></pre></td></tr></table></figure>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/11/27/%5BOBS%5DChemistry-ChemGPT/"><img class="fill" src="/gallery/LLM.png" alt="ChemGPT" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-11-27T14:51:57.000Z" title="11/27/2025, 10:51:57 PM">2025-11-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.537Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">3 minutes read (About 511 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/11/27/%5BOBS%5DChemistry-ChemGPT/">ChemGPT</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h2 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a>Base Model</h2><p>ChemGPT 早期基座模型基于 EleutherAI 的语言模型 GPT‑Neo，在分子字符串语料（SMILES 或 SELFIES）上进行自回归建模。<br>典型预训练数据集为公开的大规模分子库 PubChem10M，参数规模覆盖百万到十亿级别（如约 4.7M 的轻量版本或 100M+ 的科研版本）。<br>属于领域特化的 decoder-only 化学语言模型。</p>
<h2 id="Training-Method"><a href="#Training-Method" class="headerlink" title="Training Method"></a>Training Method</h2><ul>
<li><strong>对分子进行编码</strong>：对 SMILES&#x2F;SELFIES&#x2F;反应式进行 tokenization，构建化学“语言”词表（原子、键、环、分支、立体标记等均被离散化为 token）</li>
<li><strong>预训练目标</strong>：自回归分子语言建模（Next-token prediction），最大化生成合法分子序列的对数似然。</li>
</ul>
<h2 id="调用方式"><a href="#调用方式" class="headerlink" title="调用方式"></a>调用方式</h2><h3 id="网页直接使用"><a href="#网页直接使用" class="headerlink" title="网页直接使用"></a>网页直接使用</h3><p><a target="_blank" rel="noopener" href="https://www.chemgpt.app/">https://www.chemgpt.app/</a><br>（我没有注册账号，直接问会显示网络繁忙）</p>
<div class="post-content"><a href="/2025/11/27/[OBS]Chemistry-ChemGPT/Pasted_image_20251127230555.png" title="" title=" class="gallery-item"><img src="/2025/11/27/[OBS]Chemistry-ChemGPT/Pasted_image_20251127230555.png" alt="" title=""></a></div>
### Huggingface 下载模型并调用
当前 HF Hub 上最常被引用的 ChemGPT 版本包括：
- 轻量模型：ChemGPT 4.7M （https://huggingface.co/ncfrey/ChemGPT-4.7M）
- 中型模型：ChemGPT 1.2B （https://huggingface.co/ncfrey/ChemGPT-1.2B）

<h4 id="模型推理（文本生成-Pipeline）"><a href="#模型推理（文本生成-Pipeline）" class="headerlink" title="模型推理（文本生成 Pipeline）"></a><strong>模型推理（文本生成 Pipeline）</strong></h4><p><code>text-generation pipeline</code> 适合生成式任务（包括化学问答、分子生成）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">chem_pipe = pipeline(</span><br><span class="line">    <span class="string">&quot;text-generation&quot;</span>,</span><br><span class="line">    model=<span class="string">&quot;ncfrey/ChemGPT-1.2B&quot;</span>,  <span class="comment"># 或 ncfrey/ChemGPT-4.7M</span></span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;CCO&gt;&gt;&quot;</span>  <span class="comment"># 以乙醇为前缀生成分子衍生结构或产物候选</span></span><br><span class="line">result = chem_pipe(</span><br><span class="line">    prompt,</span><br><span class="line">    max_new_tokens=<span class="number">128</span>,</span><br><span class="line">    temperature=<span class="number">0.7</span>,</span><br><span class="line">    do_sample=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result[<span class="number">0</span>][<span class="string">&quot;generated_text&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="模型推理（直接使用-model-generate）"><a href="#模型推理（直接使用-model-generate）" class="headerlink" title="模型推理（直接使用 model.generate）"></a><strong>模型推理（直接使用 model.generate）</strong></h4><p>当需要精细控制 token 级生成时，直接调用 <code>generate API</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;ncfrey/ChemGPT-1.2B&quot;</span>  <span class="comment"># 或 Chemistry 结构生成模型 ncfrey/ChemGPT-4.7M</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">model.to(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;CCO&gt;&gt;&quot;</span></span><br><span class="line">inputs = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output_ids = model.generate(</span><br><span class="line">        **inputs,</span><br><span class="line">        max_new_tokens=<span class="number">100</span>,</span><br><span class="line">        do_sample=<span class="literal">True</span>,</span><br><span class="line">        temperature=<span class="number">0.6</span>,</span><br><span class="line">        top_p=<span class="number">0.95</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">generated = tokenizer.decode(output_ids[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(generated)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/11/22/%5BOBS%5DWorld%20Model-DREAM%20TO%20CONTROL=%20LEARNING%20BEHAVIORS%20%20BY%20LATENT%20IMAGINATION/"><img class="fill" src="/gallery/Research-paper.png" alt="DREAM TO CONTROL= LEARNING BEHAVIORS  BY LATENT IMAGINATION" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-11-22T08:10:20.000Z" title="11/22/2025, 4:10:20 PM">2025-11-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.640Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">2 minutes read (About 233 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/11/22/%5BOBS%5DWorld%20Model-DREAM%20TO%20CONTROL=%20LEARNING%20BEHAVIORS%20%20BY%20LATENT%20IMAGINATION/">DREAM TO CONTROL= LEARNING BEHAVIORS  BY LATENT IMAGINATION</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/11/22/[OBS]World Model-DREAM TO CONTROL= LEARNING BEHAVIORS  BY LATENT IMAGINATION/Pasted_image_20251122161127.png" title="" title=" class="gallery-item"><img src="/2025/11/22/[OBS]World Model-DREAM TO CONTROL= LEARNING BEHAVIORS  BY LATENT IMAGINATION/Pasted_image_20251122161127.png" alt="" title=""></a></div>

<h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>论文使用**RSSM(Recurrent State Space Model)**：使用encoder来编码环境和动作生成latent state, 预测未来latent state，最后基于latent state预测奖励。</p>
<p>优势：</p>
<ul>
<li>网络可以在 latent 中快速 roll-out 数千条 imagined trajectories</li>
<li>不用预测 pixel → 速度极快</li>
<li>潜在空间的 Markov 性保证了规划时的可微分性</li>
</ul>
<h2 id="重参数化-Reparameterization-Trick"><a href="#重参数化-Reparameterization-Trick" class="headerlink" title="重参数化 Reparameterization Trick"></a>重参数化 Reparameterization Trick</h2><p>Dreamer 最关键的地方：</p>
<blockquote>
<p><strong>动作必须是可微的随机变量，这样梯度才能从 value 反传到 actor。</strong></p>
</blockquote>
<p>如果我们直接写 $a \sim \mathcal{N}(\mu, \sigma)$, 那么采样是不可微的 → 梯度断掉 → Actor 无法学习。<br>重参数化技巧的做法：$a&#x3D;\mu+\sigma\cdot\epsilon$, $\epsilon\sim\mathcal{N}(0,1)$<br>现在：</p>
<ul>
<li>ε 是随机的</li>
<li>μ 和 σ 是可微的网络输出<br>所以动作对 actor 参数有梯度, 这就是可微规划（differentiable planning）的基础。</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/11/21/%5BOBS%5DWorld%20Model-%E5%A5%A0%E5%AE%9A%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B=%20Intelligence%20without%20representation/"><img class="fill" src="/gallery/Research-paper.png" alt="奠定世界模型= Intelligence without representation" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-11-21T08:10:29.000Z" title="11/21/2025, 4:10:29 PM">2025-11-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2026-02-14T13:40:28.641Z" title="2/14/2026, 9:40:28 PM">2026-02-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">4 minutes read (About 643 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/11/21/%5BOBS%5DWorld%20Model-%E5%A5%A0%E5%AE%9A%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B=%20Intelligence%20without%20representation/">奠定世界模型= Intelligence without representation</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><p>目前学界广泛认为，世界模型是通往AGI的正确道路。而世界模型这一理念可以追溯到这篇1991年的文章。</p>
<h2 id="寓言"><a href="#寓言" class="headerlink" title="寓言"></a>寓言</h2><p>1890 年代的人想造飞机，却只能根据他们零碎的观察去猜。他们看到现代飞机巨大、复杂，于是做了一个错误类比：<br><strong>“现代飞机又大又重 → 所以重量不是问题。”</strong><br>这当然荒唐。真正让飞机能飞的前提是<strong>升力、功率重量比、材料强度等关键工程指标的平衡</strong>。但他们只看到了表象，所以：</p>
<ul>
<li>座椅用实心钢</li>
<li>完全不关心重量控制</li>
<li>座舱设计完全按马车或汽车类比</li>
<li>控制系统用汽车方向盘、油门</li>
<li>甚至把“看地面”和“开窗透气”当成关键需求</li>
</ul>
<p>寓言中的团队觉得项目太大，所以分专业化研究，但他们犯了一个致命错误： <strong>没有系统架构，没有统一指标，没有工程指导思想。</strong><br>因此：</p>
<ul>
<li>每个小组各做各的</li>
<li>没有协调</li>
<li>没有明确目标</li>
<li>没有性能指标</li>
<li>完全不知道“重量、升力、结构”是耦合的<br>这其实是现代大型项目失败的典型原因。</li>
</ul>
<h2 id="论文的核心思想：AI-不应依赖内部表征，世界就是模型"><a href="#论文的核心思想：AI-不应依赖内部表征，世界就是模型" class="headerlink" title="论文的核心思想：AI 不应依赖内部表征，世界就是模型"></a>论文的核心思想：AI 不应依赖内部表征，世界就是模型</h2><p>Brooks 的核心观点用一句话总结就是：</p>
<blockquote>
<p><strong>智能不是“建模世界 + 推理”的结果，而是“直接在世界中行动”的结果。世界本身就是模型，不需要额外的表征。</strong></p>
</blockquote>
<p>传统 AI（symbolic AI）强调：</p>
<ol>
<li>先感知世界形成表征</li>
<li>用表征进行内部推理</li>
<li>规划行动</li>
</ol>
<p>Brooks 强烈反对。他认为：</p>
<ul>
<li><strong>显式表征只会拖慢系统、增加错误来源</strong></li>
<li><strong>真正的智能首先来源于“行动”而非“思考”</strong></li>
<li><strong>复杂智能应从简单行为逐层堆叠，而非从复杂推理开始</strong></li>
</ul>
<p>这种思想后来直接催生了：</p>
<ul>
<li>行为式机器人学</li>
<li>Embodied AI</li>
<li>Subsumption Architecture</li>
<li>深度强化学习中“end-to-end from pixels to actions”的理念</li>
<li>现代多模态 agent 的行为驱动架构<br>Brooks 的方法强调<strong>直接耦合感知与动作</strong>，由多个独立行为层并行运行，无中心控制，无统一世界模型。</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/Review/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/Review/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/Review/">1</a></li><li><a class="pagination-link" href="/categories/Review/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Review/page/3/">3</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/categories/Review/page/9/">9</a></li></ul></nav></div><style>.column.column-left,.column.column-right{display:none}</style><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Chen Yulin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chen Yulin</p><p class="is-size-6 is-block">SJTU student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Manchester by the Sea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">312</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">235</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Chen-Yulin" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2026/02/"><span class="level-start"><span class="level-item">February 2026</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2026/01/"><span class="level-start"><span class="level-item">January 2026</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/12/"><span class="level-start"><span class="level-item">December 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/11/"><span class="level-start"><span class="level-item">November 2025</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/10/"><span class="level-start"><span class="level-item">October 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">September 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/08/"><span class="level-start"><span class="level-item">August 2025</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/07/"><span class="level-start"><span class="level-item">July 2025</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/06/"><span class="level-start"><span class="level-item">June 2025</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">45</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">January 2025</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">46</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/2026/02/14/%5BOBS%5Dexist_label/"><img src="/thumb/Research-paper.png" alt="exist_label"></a></figure><div class="media-content"><p class="date"><time dateTime="2026-02-14T12:01:54.000Z">2026-02-14</time></p><p class="title"><a href="/2026/02/14/%5BOBS%5Dexist_label/">exist_label</a></p><p class="categories"><a href="/categories/Note/">Note</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2026/02/06/%5BOBS%5DDeep%20Learning-BAGEL-Unified-Multimodal-Pretraining/"><img src="/thumb/Research-paper.png" alt="BAGEL-Unified-Multimodal-Pretraining"></a></figure><div class="media-content"><p class="date"><time dateTime="2026-02-05T21:30:00.000Z">2026-02-06</time></p><p class="title"><a href="/2026/02/06/%5BOBS%5DDeep%20Learning-BAGEL-Unified-Multimodal-Pretraining/">BAGEL-Unified-Multimodal-Pretraining</a></p><p class="categories"><a href="/categories/Review/">Review</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2026/02/05/%5BOBS%5DDeep%20Learning-LingBot-VLA/"><img src="/thumb/Research-paper.png" alt="LingBot-VLA"></a></figure><div class="media-content"><p class="date"><time dateTime="2026-02-05T15:30:00.000Z">2026-02-05</time></p><p class="title"><a href="/2026/02/05/%5BOBS%5DDeep%20Learning-LingBot-VLA/">LingBot-VLA</a></p><p class="categories"><a href="/categories/Review/">Review</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2026/02/05/%5BOBS%5DDeep%20Learning-Transformer-Mixture-of-Experts-Survey/"><img src="/thumb/LLM.png" alt="Mixture-of-Experts-Survey"></a></figure><div class="media-content"><p class="date"><time dateTime="2026-02-05T15:30:00.000Z">2026-02-05</time></p><p class="title"><a href="/2026/02/05/%5BOBS%5DDeep%20Learning-Transformer-Mixture-of-Experts-Survey/">Mixture-of-Experts-Survey</a></p><p class="categories"><a href="/categories/Review/">Review</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-05T00:00:00.000Z">2026-02-05</time></p><p class="title"><a href="/2026/02/05/%5BOBS%5DRobotics-Humanoid-Robot-Control-Methods/">人形机器人控制方法综述</a></p><p class="categories"><a href="/categories/Note/">Note</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3D-Scene/"><span class="tag">3D-Scene</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/6-D/"><span class="tag">6-D</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">16</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AIGC/"><span class="tag">AIGC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/API/"><span class="tag">API</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AR/"><span class="tag">AR</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Academic/"><span class="tag">Academic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Aliyun/"><span class="tag">Aliyun</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/App/"><span class="tag">App</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BS4/"><span class="tag">BS4</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bayesian-Inference/"><span class="tag">Bayesian-Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Beautify/"><span class="tag">Beautify</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Behaviorism/"><span class="tag">Behaviorism</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Business/"><span class="tag">Business</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CADC/"><span class="tag">CADC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CD/"><span class="tag">CD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLI/"><span class="tag">CLI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLIP/"><span class="tag">CLIP</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">68</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Camera/"><span class="tag">Camera</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Capstone/"><span class="tag">Capstone</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Chemistry/"><span class="tag">Chemistry</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Claude/"><span class="tag">Claude</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Communication/"><span class="tag">Communication</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Contrastive-Learning/"><span class="tag">Contrastive-Learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control/"><span class="tag">Control</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Csharp/"><span class="tag">Csharp</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Css/"><span class="tag">Css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cuda/"><span class="tag">Cuda</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DD/"><span class="tag">DD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DINO/"><span class="tag">DINO</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DT/"><span class="tag">DT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dataframe/"><span class="tag">Dataframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Debate/"><span class="tag">Debate</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Debugger/"><span class="tag">Debugger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Development-Tools/"><span class="tag">Development-Tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Diffusion/"><span class="tag">Diffusion</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Diffusion-Policy/"><span class="tag">Diffusion-Policy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DiffusionModel/"><span class="tag">DiffusionModel</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Discrete-Mathematics/"><span class="tag">Discrete-Mathematics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Disney/"><span class="tag">Disney</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docs/"><span class="tag">Docs</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-programming/"><span class="tag">Dynamic-programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESP32/"><span class="tag">ESP32</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Education/"><span class="tag">Education</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embeded-System/"><span class="tag">Embeded-System</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embodied-AI/"><span class="tag">Embodied-AI</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emoation/"><span class="tag">Emoation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emotion/"><span class="tag">Emotion</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ethic/"><span class="tag">Ethic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Experiment/"><span class="tag">Experiment</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FL/"><span class="tag">FL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FPN/"><span class="tag">FPN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Family/"><span class="tag">Family</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Federated-Learning/"><span class="tag">Federated-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Foundation/"><span class="tag">Foundation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FoundationModel/"><span class="tag">FoundationModel</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Functional-programming/"><span class="tag">Functional programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Game/"><span class="tag">Game</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gated-NN/"><span class="tag">Gated-NN</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Github/"><span class="tag">Github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Godot/"><span class="tag">Godot</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Graph/"><span class="tag">Graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HPC/"><span class="tag">HPC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HRI/"><span class="tag">HRI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Haskell/"><span class="tag">Haskell</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Health/"><span class="tag">Health</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hierarchical/"><span class="tag">Hierarchical</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Html/"><span class="tag">Html</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Humanism/"><span class="tag">Humanism</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Humanoid/"><span class="tag">Humanoid</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HumanoidRobot/"><span class="tag">HumanoidRobot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hybrid-Control/"><span class="tag">Hybrid-Control</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hyprland/"><span class="tag">Hyprland</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IK/"><span class="tag">IK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-Grounding/"><span class="tag">Image-Grounding</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-Text/"><span class="tag">Image-Text</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-generation/"><span class="tag">Image-generation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image2Text/"><span class="tag">Image2Text</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ImgGen/"><span class="tag">ImgGen</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ImitationLearning/"><span class="tag">ImitationLearning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Information-Theory/"><span class="tag">Information-Theory</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Jolt/"><span class="tag">Jolt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Json/"><span class="tag">Json</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSP/"><span class="tag">LSP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LatentAction/"><span class="tag">LatentAction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latex/"><span class="tag">Latex</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lego/"><span class="tag">Lego</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life/"><span class="tag">Life</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearAlgebra/"><span class="tag">LinearAlgebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">22</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Live2d/"><span class="tag">Live2d</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Love/"><span class="tag">Love</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lua/"><span class="tag">Lua</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MBTI/"><span class="tag">MBTI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MPC/"><span class="tag">MPC</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MR-AR/"><span class="tag">MR/AR</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine-Learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mason/"><span class="tag">Mason</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Meme/"><span class="tag">Meme</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Message-Passing/"><span class="tag">Message-Passing</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MindPlus/"><span class="tag">MindPlus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MoE/"><span class="tag">MoE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mod/"><span class="tag">Mod</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Predictive-Control/"><span class="tag">Model-Predictive-Control</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Motivation/"><span class="tag">Motivation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Moveit/"><span class="tag">Moveit</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Movie/"><span class="tag">Movie</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-Agent/"><span class="tag">Multi-Agent</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-modal/"><span class="tag">Multi-modal</span><span class="tag">14</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-view/"><span class="tag">Multi-view</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MultiModal/"><span class="tag">MultiModal</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Music/"><span class="tag">Music</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NN/"><span class="tag">NN</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Network/"><span class="tag">Network</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nodejs/"><span class="tag">Nodejs</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Numpy/"><span class="tag">Numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nvim/"><span class="tag">Nvim</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-Detection/"><span class="tag">Object-Detection</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Open-Vocabulary/"><span class="tag">Open-Vocabulary</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenCV/"><span class="tag">OpenCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Oral/"><span class="tag">Oral</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PHD/"><span class="tag">PHD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PSY/"><span class="tag">PSY</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pandas/"><span class="tag">Pandas</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Panoptic/"><span class="tag">Panoptic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Path/"><span class="tag">Path</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Philosophy/"><span class="tag">Philosophy</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PhysX/"><span class="tag">PhysX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Physical-Scene/"><span class="tag">Physical-Scene</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Physics-engine/"><span class="tag">Physics-engine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pio/"><span class="tag">Pio</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Planning/"><span class="tag">Planning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Plugin/"><span class="tag">Plugin</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PoseEstimation/"><span class="tag">PoseEstimation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Postgraduate/"><span class="tag">Postgraduate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Prefab/"><span class="tag">Prefab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probability/"><span class="tag">Probability</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QML/"><span class="tag">QML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Quantum/"><span class="tag">Quantum</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RAG/"><span class="tag">RAG</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS/"><span class="tag">ROS</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reading/"><span class="tag">Reading</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real2Sim/"><span class="tag">Real2Sim</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reconstruct/"><span class="tag">Reconstruct</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regex/"><span class="tag">Regex</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement-Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-learning/"><span class="tag">Reinforcement-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Repository/"><span class="tag">Repository</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation-Learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research-paper/"><span class="tag">Research-paper</span><span class="tag">97</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robot/"><span class="tag">Robot</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RobotLearning/"><span class="tag">RobotLearning</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robotics/"><span class="tag">Robotics</span><span class="tag">38</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SJTU-Lecture/"><span class="tag">SJTU-Lecture</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SSH/"><span class="tag">SSH</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scalability/"><span class="tag">Scalability</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scene-graph/"><span class="tag">Scene-graph</span><span class="tag">34</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scene-synthesis/"><span class="tag">Scene-synthesis</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Science-fiction/"><span class="tag">Science-fiction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scrap/"><span class="tag">Scrap</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Script/"><span class="tag">Script</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Segmentation/"><span class="tag">Segmentation</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Semantic/"><span class="tag">Semantic</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shader/"><span class="tag">Shader</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shell/"><span class="tag">Shell</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Signals-and-Systems/"><span class="tag">Signals and Systems</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sim2Real/"><span class="tag">Sim2Real</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sklearn/"><span class="tag">Sklearn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Snippets/"><span class="tag">Snippets</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Society/"><span class="tag">Society</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Star-rail/"><span class="tag">Star-rail</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistics/"><span class="tag">Statistics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subgraph/"><span class="tag">Subgraph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Submodule/"><span class="tag">Submodule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Supervised-learning/"><span class="tag">Supervised-learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Survey/"><span class="tag">Survey</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TC/"><span class="tag">TC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TOEFL/"><span class="tag">TOEFL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Task-Planning/"><span class="tag">Task-Planning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tasks/"><span class="tag">Tasks</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tech-Communication/"><span class="tag">Tech Communication</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Torch/"><span class="tag">Torch</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Translation-Embedding/"><span class="tag">Translation-Embedding</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Travel/"><span class="tag">Travel</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/UI/"><span class="tag">UI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unified-Multimodal/"><span class="tag">Unified-Multimodal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unity/"><span class="tag">Unity</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-learning/"><span class="tag">Unsupervised-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VAE/"><span class="tag">VAE</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLA/"><span class="tag">VLA</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLM/"><span class="tag">VLM</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLP/"><span class="tag">VLP</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VQ-VAE/"><span class="tag">VQ-VAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variational-Inference/"><span class="tag">Variational-Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Version-management/"><span class="tag">Version-management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ViT/"><span class="tag">ViT</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoEditing/"><span class="tag">VideoEditing</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vim/"><span class="tag">Vim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Visual-Relation/"><span class="tag">Visual-Relation</span><span class="tag">23</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSL/"><span class="tag">WSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Waybar/"><span class="tag">Waybar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wayland/"><span class="tag">Wayland</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web/"><span class="tag">Web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Website/"><span class="tag">Website</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Well-being/"><span class="tag">Well-being</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Window-manager/"><span class="tag">Window-manager</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WorldModel/"><span class="tag">WorldModel</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YKLL/"><span class="tag">YKLL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Zen/"><span class="tag">Zen</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E2%99%A5%EF%B8%8F/"><span class="tag">♥️</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AE%9E%E4%B9%A0/"><span class="tag">实习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%F0%9F%8D%A2/"><span class="tag">🍢</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%F0%9F%8D%B0/"><span class="tag">🍰</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%F0%9F%90%B1/"><span class="tag">🐱</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%F0%9F%A7%80/"><span class="tag">🧀</span><span class="tag">1</span></a></div></div></div></div></div></div><style>.column.column-left,.column.column-right{display:block}</style></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/img/cyllogo.png" alt="Chen Yulin&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/cyllogonight.png" alt="Chen Yulin&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2026 Chen Yulin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script><script type="text/javascript" src="/js/imaegoo/falling-petals.js"></script><!-- hexo injector body_end start -->
<link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
<script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
<script>
  window.CHATBOT_CONFIG = {
    endpoint: "https://webchat-bot-iqu-knzhgrvznd.cn-hangzhou.fcapp.run/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
    displayByDefault: false, // 默认不展示 AI 助手聊天框
    aiChatOptions: { // aiChatOptions 中 options 会传递 aiChat 组件，自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat
      conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationStarters: [
          {prompt: '你是谁？'},
          {prompt: '博主又是谁？'},
          {prompt: '怎么使用这个网站？'},
          {prompt: '想要博主联系方式！'},
        ]
      },
      displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
        height: 600,
        width: 350,
      },
      personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
        assistant: {          name: '博主的AI助手，十四行诗参上！',
          // AI 助手的图标
          avatar: 'https://chen-yulin.github.io/thumb/14.png',
          tagline: '要不要试试问下面的问题呢？',
        }
      }
    }
  };
</script>
<style>
  :root {
    /* webchat 工具栏的颜色 */
    --webchat-toolbar-background-color: #1464E4;
    /* webchat 工具栏文字和按钮的颜色 */
    --webchat-toolbar-text-color: #FFF;
  }
  /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、left 等设置来调整位置 */
  .webchat-container {
    z-index: 100;
    bottom: 10px;
    right: 10px;
  }
  /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、left 等设置来调整位置 */
  .webchat-bubble-tip {
    z-index: 99;
    bottom: 60px;
    right: 20px;
  }
  .webchat-bubble-tip {
    overflow: visible !important;
  }
  @keyframes float {
    0% {
      transform: translateY(0px) translateX(-50%);
    }
    50% {
      transform: translateY(-10px) translateX(-50%);
    }
    100% {
      transform: translateY(0px) translateX(-50%);
    }
  }

  .webchat-bubble-tip::before {
    content: '';
    position: absolute;
    top: -25px;
    left: 70%;
    width: 40px;
    height: 40px;
    background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='white'%3E%3Cpath d='M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z'/%3E%3C/svg%3E");
    background-repeat: no-repeat;
    background-position: center;
    background-size: contain;
    filter: drop-shadow(0 4px 6px rgba(0, 0, 0, 0.5));
    animation: float 3s ease-in-out infinite;
  }
</style><script data-pjax src="https://registry.npmmirror.com/oh-my-live2d/latest/files"></script><script>const oml2d = OML2D.loadOml2d({libraryUrls:{"complete":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/complete.js","cubism2":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism2.js","cubism5":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism5.js"},dockedPosition:"left",mobileDisplay:true,models:[{"path":"https://model.hacxy.cn/mai/model.json","mobilePosition":[25,0],"mobileScale":0.1,"mobileStageStyle":{"width":125,"height":175},"motionPreloadStrategy":"ALL","position":[50,0],"scale":0.2,"stageStyle":{"width":250,"height":350}}],parentElement:document.body,primaryColor:"var(--btn-bg)",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body></html>