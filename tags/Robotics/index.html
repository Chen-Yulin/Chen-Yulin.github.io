<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta name="theme-color" content="#123456"><title>Tag: Robotics - Chen Yulin&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#6495ed"><meta name="application-name" content="Icarus - Hexo Theme"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="msapplication-TileColor" content="#6495ed"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Icarus - Hexo Theme"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Chen Yulin&#039;s Blog"><meta property="og:url" content="http://chen-yulin.github.io/"><meta property="og:site_name" content="Chen Yulin&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://chen-yulin.github.io/img/og_image.png"><meta property="article:author" content="Chen Yulin"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://chen-yulin.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://chen-yulin.github.io"},"headline":"Chen Yulin's Blog","image":["http://chen-yulin.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Chen Yulin"},"publisher":{"@type":"Organization","name":"Chen Yulin's Blog","logo":{"@type":"ImageObject","url":{"light":"/img/cyllogo.png","dark":"/img/cyllogonight.png"}}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><canvas id="flower"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/img/cyllogo.png" alt="Chen Yulin&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/cyllogonight.png" alt="Chen Yulin&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags/">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Robotics</a></li></ul></nav></div></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/04/16/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-Vision-Language%20Interpreter%20for%20Robot%20Task%20Planning/"><img class="fill" src="/gallery/Research-paper.png" alt="Vision-Language Interpreter for Robot Task Planning" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-04-16T06:38:13.000Z" title="4/16/2025, 2:38:13 PM">2025-04-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.701Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Note/">Note</a></span><span class="level-item">a few seconds read (About 3 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/16/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-Vision-Language%20Interpreter%20for%20Robot%20Task%20Planning/">Vision-Language Interpreter for Robot Task Planning</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/04/16/[OBS]Reconstruct Anything-相近工作-Vision-Language Interpreter for Robot Task Planning/Pasted_image_20250416144529.png" title="" title=" class="gallery-item"><img src="/2025/04/16/[OBS]Reconstruct Anything-相近工作-Vision-Language Interpreter for Robot Task Planning/Pasted_image_20250416144529.png" alt="" title=""></a></div></div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/04/16/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-RoboEXP=%20Action-Conditioned%20Scene%20Graph%20via%20Interactive%20Exploration%20for%20Robotic%20Manipulation/"><img class="fill" src="/gallery/Research-paper.png" alt="RoboEXP" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-04-16T06:21:36.000Z" title="4/16/2025, 2:21:36 PM">2025-04-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.711Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">a few seconds read (About 3 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/16/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-RoboEXP=%20Action-Conditioned%20Scene%20Graph%20via%20Interactive%20Exploration%20for%20Robotic%20Manipulation/">RoboEXP</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/04/16/[OBS]Reconstruct Anything-相近工作-RoboEXP= Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation/Pasted_image_20250416142353.png" title="" title=" class="gallery-item"><img src="/2025/04/16/[OBS]Reconstruct Anything-相近工作-RoboEXP= Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation/Pasted_image_20250416142353.png" alt="" title=""></a></div>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/04/14/%5BOBS%5D(Mindmap)%20Part-level%20Scene%20Understanding%20for%20Robots/"><img class="fill" src="/gallery/Research-paper.png" alt="(Mindmap) Part-level Scene Understanding for Robots" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-04-14T04:12:22.000Z" title="4/14/2025, 12:12:22 PM">2025-04-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.585Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Note/">Note</a></span><span class="level-item">a few seconds read (About 95 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/04/14/%5BOBS%5D(Mindmap)%20Part-level%20Scene%20Understanding%20for%20Robots/">(Mindmap) Part-level Scene Understanding for Robots</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h2 id="概念梳理"><a href="#概念梳理" class="headerlink" title="概念梳理"></a>概念梳理</h2><h3 id="Scene-Graph"><a href="#Scene-Graph" class="headerlink" title="Scene Graph"></a>Scene Graph</h3><p>A scene graph is a structural representation, which can capture detailed semantics by explicitly Modeling:</p>
<ul>
<li>objects (‘‘man’’, ‘‘fire hydrant’’, ‘‘shorts’’)</li>
<li>attributes of objects (‘‘fire hydrant is yellow’’)</li>
<li>relations between paired objects (‘‘man jumping over fire hydrant’’)</li>
</ul>
<p>A scene graph is a set of <strong>visual relationship</strong> triplets in the form of &lt;subject, relation, object&gt; or &lt;object, is, attribute&gt;</p>
<div class="post-content"><a href="/2025/04/14/[OBS](Mindmap) Part-level Scene Understanding for Robots/Pasted_image_20250414142333.png" title="" title=" class="gallery-item"><img src="/2025/04/14/[OBS](Mindmap) Part-level Scene Understanding for Robots/Pasted_image_20250414142333.png" alt="" title=""></a></div>
Scene graphs should serve as an **objective semantic representation** of the state of the scene

</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-25T10:55:53.000Z" title="3/25/2025, 6:55:53 PM">2025-03-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.172Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Note/">Note</a></span><span class="level-item">5 minutes read (About 724 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/03/25/%5BOBS%5D%E7%A7%91%E7%A0%94-(Roadmap)%20Deeper%20Scene%20Graph%20For%20Robots/">(Roadmap) Deeper Scene Graph For Robots</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><h2 id="针对的问题（任务场景）"><a href="#针对的问题（任务场景）" class="headerlink" title="针对的问题（任务场景）"></a>针对的问题（任务场景）</h2><p>Robotic planning and execution in open-world environments is a complex problem due to the <strong>vast state spaces</strong> and <strong>high variability of task embodiment</strong>.<br>例如针对家用场景：</p>
<ul>
<li>OVMM Challenge: <a target="_blank" rel="noopener" href="https://aihabitat.org/challenge/2023_homerobot_ovmm/">https://aihabitat.org/challenge/2023_homerobot_ovmm/</a><br>想要在这样复杂场景中执行 general, long-horizon, embodied tasks 需要生成一系列离散的动作，这些动作在都拥有累计和传播错误的可能。因此需要创建一个可行的计划并在该计划出现问题时恢复，需要对物理环境进行有效的抽象以及能够完全利用该抽象的planner。应对这些挑战需要整合自然语言理解，多粒度的场景抽象和理解以及有弹性的推理。</li>
</ul>
<p>目前粗粒度（object-level）的场景抽象（场景图构建）已经有许多工作了，详见Reconstruct-Anything Literature Review，在这些工作中，重点都在于object detection和 object-level visual relationship detection</p>
<p><strong>需要聚焦的部分是多粒度的场景抽象</strong><br>需要多粒度的原因：</p>
<ul>
<li>Scalability: 如果只有一个粒度，那么输入LLM的场景图token不可控，影响扩展性</li>
<li>想要和物体进行更复杂的交互（相较于抓取），需要明确物体各个part的位置，语义性质，和父物体的<strong>parent-child relationship</strong>。这就要求场景图的生成需要考虑更细粒度。</li>
<li>针对不同复杂度的物体，需要的物体粒度层级不同</li>
<li>对于不同任务，需要的物体粒度也不同。<br>具体案例（任务需要的颗粒度层次）：</li>
<li>&lt;Task&gt;给水壶加水：<ul>
<li>&lt;object-level&gt;水壶<ul>
<li>&lt;part-level&gt;壶盖</li>
<li>&lt;part-level&gt;把手</li>
</ul>
</li>
<li>&lt;object-level&gt;饮水机<ul>
<li>&lt;part-level&gt;操作面板<ul>
<li>&lt;part-level&gt;绿色按钮（常温水）</li>
<li>&lt;part-level&gt;红色按钮（开水）</li>
<li>&lt;part-level&gt;童锁</li>
</ul>
</li>
<li>&lt;part-level&gt;水槽</li>
</ul>
</li>
<li>&lt;object-level&gt;桌子<ul>
<li>&lt;part-level&gt;桌面</li>
</ul>
</li>
</ul>
</li>
<li>&lt;Task&gt;离开房间<ul>
<li>&lt;object-level&gt;门<ul>
<li>&lt;part-level&gt;把手</li>
<li>&lt;part-level&gt;纸条：“离开房间前把玩偶放回红筐”</li>
</ul>
</li>
<li>&lt;object-level&gt;黄鸭玩偶</li>
<li>&lt;object-level&gt;红框  <div class="post-content"><a href="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328103811.png" title="" title=" class="gallery-item"><img src="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328103811.png" alt="" title=""></a></div></li>
</ul>
</li>
</ul>
<p>在更细粒度（part-level）的场景抽象中，重点在于<strong>子物体和父物体关系的识别</strong></p>
<blockquote>
<p>除此，和object-level scene graph中的object detection相对的，是part-level scene graph的子物体语义的多粒度分割和语义信息提取，可以由现有的Semantic-SAM和类似CLIP或者其他多模态模型的语义特征提取器实现。</p>
</blockquote>
<h2 id="主要的研究流程"><a href="#主要的研究流程" class="headerlink" title="主要的研究流程"></a>主要的研究流程</h2><h3 id="明确研究对象Parent-child-Relationship"><a href="#明确研究对象Parent-child-Relationship" class="headerlink" title="明确研究对象Parent-child Relationship"></a>明确研究对象<strong>Parent-child Relationship</strong></h3><p>What aspects does parent-child relationship include?</p>
<ul>
<li>语义构成关系，即这个子物体的<strong>存在与否</strong>给父物体的语义带来了什么改变 Translation in embedding space.</li>
<li>kinematic relations，也就是需要把一个物体以一个运动学树的形式构建出来  <div class="post-content"><a href="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328143957.png" title="" title=" class="gallery-item"><img src="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328143957.png" alt="" title=""></a></div>
  <div class="post-content"><a href="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328150005.png" title="" title=" class="gallery-item"><img src="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328150005.png" alt="" title=""></a></div></li>
</ul>
<h3 id="项目流程的流程"><a href="#项目流程的流程" class="headerlink" title="项目流程的流程"></a>项目流程的流程</h3><h3 id="自监督的特征提取方法"><a href="#自监督的特征提取方法" class="headerlink" title="自监督的特征提取方法"></a>自监督的特征提取方法</h3><div class="post-content"><a href="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328145911.png" title="" title=" class="gallery-item"><img src="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250328145911.png" alt="" title=""></a></div>


<div class="post-content"><a href="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250417185849.png" title="" title=" class="gallery-item"><img src="/2025/03/25/[OBS]科研-(Roadmap) Deeper Scene Graph For Robots/Pasted_image_20250417185849.png" alt="" title=""></a></div></div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/03/18/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20%20for%20Language-Grounded%20Robot%20Navigation/"><img class="fill" src="/gallery/Research-paper.png" alt="Hierarchical Open-Vocabulary 3D Scene Graphs  for Language-Grounded Robot Navigation" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-18T08:46:46.000Z" title="3/18/2025, 4:46:46 PM">2025-03-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.706Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">a few seconds read (About 3 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/03/18/%5BOBS%5DReconstruct%20Anything-%E7%9B%B8%E8%BF%91%E5%B7%A5%E4%BD%9C-Hierarchical%20Open-Vocabulary%203D%20Scene%20Graphs%20%20for%20Language-Grounded%20Robot%20Navigation/">Hierarchical Open-Vocabulary 3D Scene Graphs  for Language-Grounded Robot Navigation</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/03/18/[OBS]Reconstruct Anything-相近工作-Hierarchical Open-Vocabulary 3D Scene Graphs  for Language-Grounded Robot Navigation/Pasted_image_20250318170146.png" title="" title=" class="gallery-item"><img src="/2025/03/18/[OBS]Reconstruct Anything-相近工作-Hierarchical Open-Vocabulary 3D Scene Graphs  for Language-Grounded Robot Navigation/Pasted_image_20250318170146.png" alt="" title=""></a></div></div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/03/12/%5BOBS%5DReconstruct-Anything%20Literature%20Review/"><img class="fill" src="/gallery/Research-paper.png" alt="Reconstruct Anything Literature Review" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-12T06:17:59.000Z" title="3/12/2025, 2:17:59 PM">2025-03-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.588Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">10 minutes read (About 1524 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/03/12/%5BOBS%5DReconstruct-Anything%20Literature%20Review/">Reconstruct Anything Literature Review</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><p>涉及的文章：</p>
<ul>
<li>相近工作<ul>
<li>[[Part-level Scene Reconstruction Affords Robot Interaction]]</li>
<li>[[Scene Reconstruction with Functional Objects for Robot Autonomy]]</li>
<li>[[Reasoning with Scene Graphs for Robot Planning  under Partial Observability]]</li>
<li>[[ACDC- Automated Creation of Digital Cousins for Robust Policy Learning]]</li>
<li>[[CLIP-Fields- Weakly Supervised Semantic Fields for Robotic Memory]]</li>
<li>[[Factorizable Net&#x3D; An Efficient Subgraph-based  Framework for Scene Graph Generation]]</li>
<li>[[SayPlan&#x3D; Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning]]</li>
<li>[[ConceptGraphs&#x3D; Open-Vocabulary 3D Scene Graphs for Perception and Planning]]</li>
</ul>
</li>
<li>数据生成<ul>
<li>[[PHYSCENE- Physically Interactable 3D Scene Synthesis for Embodied AI]]</li>
</ul>
</li>
<li>数据集<ul>
<li>CLEVR</li>
<li>Visual Genome</li>
</ul>
</li>
<li>其他<ul>
<li>[[SceneGraphFusion- Incremental 3D Scene Graph Prediction from RGB-D Sequences]]</li>
<li>[[Visual Relationship Detection with Language Priors]]</li>
<li>[[Image generation from scene graphs]]</li>
<li>[[(FCSGG) Fully Convolutional Scene Graph Generation]]</li>
<li>[[RelTR&#x3D; Relation Transformer for Scene Graph Generation]]</li>
<li>[[Scene Graph Generation by Iterative Message Passing]]</li>
<li>[[From Pixels to Graphs&#x3D; Open-Vocabulary Scene Graph Generation with  Vision-Language Models]]</li>
<li>[[(VtransE) Visual Translation Embedding Network for Visual Relation Detection]]</li>
<li>[[(UVtransE) Contextual Translation Embedding for Visual Relationship Detection and Scene Graph Generation]]</li>
<li>[[(RLSV) Representation Learning for Scene Graph Completion via Jointly Structural and Visual Embedding]]</li>
<li>[[Energy-Based Learning for Scene Graph Generation]]</li>
</ul>
</li>
</ul>
<h2 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h2><p>通过构建part-level scene-graph，结合Reasoning with LLM 让机器人能够实现<strong>更复杂的交互</strong>，并以此完成<strong>更复杂的任务</strong>。</p>
<h2 id="Scene-Graph-Introduction"><a href="#Scene-Graph-Introduction" class="headerlink" title="Scene Graph Introduction"></a>Scene Graph Introduction</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p><strong>Visual scene understanding</strong>长期以来一直被认为是计算机视觉的圣杯</p>
<h4 id="Rapid-scene-understanding-at-all-levels"><a href="#Rapid-scene-understanding-at-all-levels" class="headerlink" title="Rapid scene understanding at all levels"></a>Rapid scene understanding at all levels</h4><h5 id="Generally"><a href="#Generally" class="headerlink" title="Generally"></a>Generally</h5><p>Visual scene understanding 可以被分为<strong>两块任务</strong></p>
<ul>
<li><p>recognition task</p>
<ul>
<li>image level<ul>
<li>image classification<ul>
<li>[[DINO]]</li>
<li>[[CLIP多模态预训练模型]]</li>
</ul>
</li>
</ul>
</li>
<li>pixel level<ul>
<li>semantic segmentation: classify each pixel in an image into a category<ul>
<li>Mask RCNN</li>
<li>U-Net</li>
</ul>
</li>
</ul>
</li>
<li>instance level<ul>
<li>instance segmentation: detect and delineate each individual object instance in an image (bounding boxes or segmentation masks)<ul>
<li>[[Grounding-DINO]]</li>
<li>[[Gounded-SAM]]</li>
</ul>
</li>
</ul>
</li>
<li>pixel &amp; instance level<ul>
<li>[[Panoptic Segmentation]]: takes into account both per-pixel class and instance labels<ul>
<li>[[MaskDINO]]</li>
<li>[[Semantic-SAM]]</li>
</ul>
</li>
</ul>
</li>
</ul>
  <div class="post-content"><a href="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250313113232.png" title="" title=" class="gallery-item"><img src="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250313113232.png" alt="" title=""></a></div>
</li>
<li><p>application task</p>
<ul>
<li>…</li>
</ul>
</li>
</ul>
<h5 id="Relation-amp-Interaction"><a href="#Relation-amp-Interaction" class="headerlink" title="Relation &amp; Interaction"></a>Relation &amp; Interaction</h5><p>但是以上这些<code>Generally</code>的工作注重的都是<strong>the localization of objects</strong>，更高级别的任务强调探索对象之间的丰富语义关系，以及对象与周围环境的相互作用</p>
<ul>
<li>视觉关系检测（VRD）<ul>
<li>[[GPS-Net&#x3D; Graph Property Sensing Network for Scene Graph Generation]]</li>
<li>[[Large-scale visual relationship understanding]]</li>
</ul>
</li>
<li>人类对象相互作用（HOI）<ul>
<li>…</li>
</ul>
</li>
</ul>
<h5 id="CV-amp-NLP"><a href="#CV-amp-NLP" class="headerlink" title="CV &amp; NLP"></a>CV &amp; NLP</h5><p>除此之外还有将NLP和CV结合起来的方向，主要是一些VLM</p>
<ul>
<li>image caption</li>
<li>visual question answering</li>
<li>visual dialog</li>
</ul>
<h4 id="Structured-Representation-of-Scene-Scene-Graph"><a href="#Structured-Representation-of-Scene-Scene-Graph" class="headerlink" title="Structured Representation of Scene (Scene Graph)"></a>Structured Representation of Scene (Scene Graph)</h4><p>对于总体场景的感知和信息的有效表示仍然是瓶颈。<br>所以Li Feifei 在[[Image Retrieval using Scene Graphs]]提出Scene Graph</p>
<p>与Structured Representation相对的是Latent Representation</p>
<h3 id="Scene-Graph-Definition"><a href="#Scene-Graph-Definition" class="headerlink" title="Scene Graph Definition"></a>Scene Graph Definition</h3><p>A scene graph is a structural representation, which can capture detailed semantics by explicitly Modeling</p>
<ul>
<li>objects (‘‘man’’, ‘‘fire hydrant’’, ‘‘shorts’’)</li>
<li>attributes of objects (‘‘fire hydrant is yellow’’)</li>
<li>relations between paired objects (‘‘man jumping over fire hydrant’’)</li>
</ul>
<p>A scene graph is a set of visual relationship triplets in the form of &lt;subject, relation, object&gt; or &lt;object, is, attribute&gt;</p>
<p>Scene graphs should serve as an objective semantic representation of the state of the scene</p>
<h3 id="为什么选择scene-graph"><a href="#为什么选择scene-graph" class="headerlink" title="为什么选择scene graph"></a>为什么选择scene graph</h3><p>Scene Graph具有应对和改善其他视觉任务的内在潜力。<br>可以解决的视觉任务包括：</p>
<ul>
<li>Image captioning<ul>
<li>take an image as an input and parse it into a scene graph, and then generate a reasonable text as output.</li>
</ul>
</li>
<li>Visual question answering</li>
<li>Content-based image retrieval</li>
<li>Image generation<ul>
<li>extracting scene graphs from the text description and then generate realistic images<ul>
<li>[[Image generation from scene graphs]]</li>
</ul>
</li>
</ul>
</li>
<li>referring expression comprehension</li>
</ul>
<div class="post-content"><a href="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250313201829.png" title="" title=" class="gallery-item"><img src="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250313201829.png" alt="" title=""></a></div>


<h2 id="Scene-Graph-Generation"><a href="#Scene-Graph-Generation" class="headerlink" title="Scene Graph Generation"></a>Scene Graph Generation</h2><p>场景图生成的目的是解析图像或一系列图像，并且生成结构化表示，以此弥合视觉和语义感知之间的差距，并最终达到对视觉场景的完整理解。<br><strong>任务的本质是检测视觉关系。</strong></p>
<h3 id="先驱工作"><a href="#先驱工作" class="headerlink" title="先驱工作"></a>先驱工作</h3><p>早先由Feifei [[Visual Relationship Detection with Language Priors]] 提出了视觉关系检测的方法。<br>以及Visual Genome这个包含物体关系的数据集</p>
<h3 id="生成方法"><a href="#生成方法" class="headerlink" title="生成方法"></a>生成方法</h3><h4 id="Two-stage"><a href="#Two-stage" class="headerlink" title="Two-stage"></a>Two-stage</h4><p>Detects objects first and then solves a classification task to determine the relationship between each pair of objects</p>
<div class="post-content"><a href="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250314125133.png" title="" title=" class="gallery-item"><img src="/2025/03/12/[OBS]Reconstruct-Anything Literature Review/Pasted_image_20250314125133.png" alt="" title=""></a></div>
**General:**
a) 通过图片获取 subject/object and union box proposals (ROI感兴趣区域)

<p>b) 提取每个区域的特征。包括object的appearance, spatial information, label, depth, and mask；predicate的appearance, spatial, depth, and mask。</p>
<ul>
<li>Fast&#x2F;Faster R-CNN</li>
</ul>
<p>c) 这些多模态特征被 vectorized, combined, and refined。可以通过：</p>
<ul>
<li>message passing mechanisms<ul>
<li>[[Scene Graph Generation by Iterative Message Passing]]</li>
</ul>
</li>
<li>attention mechanisms</li>
<li>visual translation embedding</li>
</ul>
<p>d) 分类器用于预测predicate的类别</p>
<p><strong>基于Visual translation embedding的</strong></p>
<ul>
<li>Translation between Subject and Object (subject+predicate ≈ object)<ul>
<li>[[(VtransE) Visual Translation Embedding Network for Visual Relation Detection]]</li>
</ul>
</li>
<li>Translation among Subject, Object and Predicate<ul>
<li>[[(UVtransE) Contextual Translation Embedding for Visual Relationship Detection and Scene Graph Generation]]</li>
<li>[[(RLSV) Representation Learning for Scene Graph Completion via Jointly Structural and Visual Embedding]]</li>
</ul>
</li>
</ul>
<h4 id="One-stage"><a href="#One-stage" class="headerlink" title="One-stage!!!"></a>One-stage!!!</h4><p>Simultaneously detects and recognizes objects and relations<br>相较于two-stage:</p>
<ul>
<li>需要更少的计算资源和参数</li>
<li>不会受到object detection的质量影响<br>Example:</li>
<li>[[(FCSGG) Fully Convolutional Scene Graph Generation]] (bottom-up + RAF)</li>
<li>[[RelTR&#x3D; Relation Transformer for Scene Graph Generation]] (bottom-up)</li>
<li>[[SGTR&#x3D; End-to-end Scene Graph Generation with Transformer]] (top-down)</li>
</ul>
<h3 id="Open-Vocabulary"><a href="#Open-Vocabulary" class="headerlink" title="Open-Vocabulary"></a>Open-Vocabulary</h3><p>基本都是基于LLM或者VLM之类的大模型</p>
<ul>
<li>[[From Pixels to Graphs&#x3D; Open-Vocabulary Scene Graph Generation with  Vision-Language Models]]</li>
<li>[[ConceptGraphs&#x3D; Open-Vocabulary 3D Scene Graphs for Perception and Planning]]</li>
</ul>
<h2 id="Scene-Graph小结"><a href="#Scene-Graph小结" class="headerlink" title="Scene Graph小结"></a>Scene Graph小结</h2><p>这里所有的工作都是关于如何判断两个独立物体之间的谓语关系（例如riding, holding…），并没有涉及part-level relationship的工作。part-level的父子关系和object-level的谓语关系是很不一样的。</p>
<h2 id="不基于Scene-Graph-的场景理解方法"><a href="#不基于Scene-Graph-的场景理解方法" class="headerlink" title="不基于Scene Graph 的场景理解方法"></a>不基于Scene Graph 的场景理解方法</h2><h3 id="隐式场景"><a href="#隐式场景" class="headerlink" title="隐式场景"></a>隐式场景</h3><p>即场景信息存储在一个神经网络中，并没有显式的结构，规划器（可以是LLM）通过query这个模型来获得信息。</p>
<ul>
<li>[[CLIP-Fields- Weakly Supervised Semantic Fields for Robotic Memory]]<ul>
<li>无结构化，只提供语义查询，定位</li>
</ul>
</li>
</ul>
<h3 id="数字表亲场景"><a href="#数字表亲场景" class="headerlink" title="数字表亲场景"></a>数字表亲场景</h3><p>核心思想是用交互更丰富的模型组合成可交互的替代场景。</p>
<ul>
<li>[[Scene Reconstruction with Functional Objects for Robot Autonomy]]</li>
<li>[[ACDC- Automated Creation of Digital Cousins for Robust Policy Learning]]<ul>
<li>通既有的精细模型库来拟合场景中的物体，可以实现更丰富的交互，对家常物品zero-shot，但是精度有限，不能应对复杂物体</li>
</ul>
</li>
</ul>
<h3 id="Contact-Graph-可以认为是Scene-Graph的扩展"><a href="#Contact-Graph-可以认为是Scene-Graph的扩展" class="headerlink" title="Contact Graph (可以认为是Scene Graph的扩展)"></a>Contact Graph (可以认为是Scene Graph的扩展)</h3><p>主要用于建模物体之间的运动学关系</p>
<ul>
<li>[[Scene Reconstruction with Functional Objects for Robot Autonomy]]</li>
<li>[[Part-level Scene Reconstruction Affords Robot Interaction]]<ul>
<li>这个涉及到了父子之间的运动学关系</li>
</ul>
</li>
</ul>
<h2 id="Scene-Graph-amp-Robots"><a href="#Scene-Graph-amp-Robots" class="headerlink" title="Scene Graph &amp; Robots"></a>Scene Graph &amp; Robots</h2><p>将 scene graph 用于机器人任务理解和规划</p>
<ul>
<li>[[SayPlan&#x3D; Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning]]</li>
<li>[[Hierarchical Open-Vocabulary 3D Scene Graphs  for Language-Grounded Robot Navigation]]</li>
<li>[[ConceptGraphs&#x3D; Open-Vocabulary 3D Scene Graphs for Perception and Planning]]</li>
</ul>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/02/15/%5BOBS%5DReconstruct%20Anything-Scene-LLM/"><img class="fill" src="/gallery/LLM.png" alt="Scene-LLM" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-15T08:17:31.000Z" title="2/15/2025, 4:17:31 PM">2025-02-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.668Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">6 minutes read (About 919 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/02/15/%5BOBS%5DReconstruct%20Anything-Scene-LLM/">Scene-LLM</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/02/15/[OBS]Reconstruct Anything-Scene-LLM/Pasted_image_20250215153945.png" title="" title=" class="gallery-item"><img src="/2025/02/15/[OBS]Reconstruct Anything-Scene-LLM/Pasted_image_20250215153945.png" alt="" title=""></a></div>
## Intro
尽管现有的视觉语言模型（VLM）在2D视觉语言的理解中取得了长足的进步，但与使用3D表示室内场景任务的人相比，它们对持续3D空间信息的掌握有限通常会使它们的有效性较小。
最近的一些文章[[3D-LLM]]以文本和其他方式桥接3D视觉信息显示出3D视觉理解和推理的潜力。但是，它们主要处理静态3D场景，这对于涉及场景变化的互动计划的适应性较低。

<p>本文提出的模型主要想解决3D密集标注和交互式规划。<br>结合</p>
<ul>
<li>egocentric（crucial for immediate updates during object interactions and for localizing the agent within the scene）</li>
<li>comprehensive（provides temporal persistent and multi-view consistent details of the entire 3D scene）<br>scene-level的信息。</li>
</ul>
<p>需要align the dense 3D visual information with the textual embedding space of a pre-trained LLM。3D点集由于其连续坐标系以及需要适应场景状态变化的表示形式而构成了一个独特的问题</p>
<h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><p>3D-VQA<br>VLN(Visual-Language Navigation)</p>
<h2 id="3D-Visual-Language-Data-Generation"><a href="#3D-Visual-Language-Data-Generation" class="headerlink" title="3D-Visual-Language Data Generation"></a>3D-Visual-Language Data Generation</h2><p>和[[3D-LLM]]一样，都是多视角采集D-RGB信息然后整合为3D frame<br>标注信息来自于Mini-GPT-V2（capable of generating captions and object descriptions from images by using caption and grounded caption identifiers）。</p>
<h3 id="3D-frame"><a href="#3D-frame" class="headerlink" title="3D-frame"></a>3D-frame</h3><p>Uses image frames and a 2D-VLM(Mini-GPT-V2) to generate frame descriptions</p>
<h3 id="Scene-Data"><a href="#Scene-Data" class="headerlink" title="Scene Data"></a>Scene Data</h3><p>3D场景数据是通过基于其相机姿势汇总的3D帧来重建<br>使用Llama-2-Chat-70B [65]生成场景的语言注释</p>
<blockquote>
<p>prompted with a mix of context data including generated frame captions, frame object descriptions, annotated object lists, and annotated bounding boxes. These prompts lead to diverse instruction-following data types like dense caption, object caption, task decomposition, functionality enhancement, question-answering, and human-robot dialogues</p>
</blockquote>
<div class="post-content"><a href="/2025/02/15/[OBS]Reconstruct Anything-Scene-LLM/Pasted_image_20250217143744.png" title="" title=" class="gallery-item"><img src="/2025/02/15/[OBS]Reconstruct Anything-Scene-LLM/Pasted_image_20250217143744.png" alt="" title=""></a></div>
From Vision Studio
对于VLM生成内容使用的self-checking: [83]

<h2 id="Scene-LLM"><a href="#Scene-LLM" class="headerlink" title="Scene-LLM"></a>Scene-LLM</h2><p>场景-LLM是一种3D视觉语言模型（VLM），具有简单而有效的体系结构，旨在理解以基于本体和场景级别的3D视觉信息，使其能够成功执行交互式计划任务。本节概述了3D视觉特征提取过程，我们的模型的体系结构，3D视觉信息与数据集的对齐以及使用Scene-LLM进行推理。</p>
<p>Employ visual language semantic features [51] to represent 3D visual semantics</p>
<ul>
<li>first extracting pixel-wise CLIP features from each image and then aggregating these into a 3D point set [[ConceptFusion]]</li>
</ul>
<p>Tokenize 3D visual features for LLM input:</p>
<ul>
<li>hybrid point-voxel representation (need for dense 3D visual information, support for interactive updates, and manageable token lengths for the LLM)</li>
</ul>
<h3 id="网络大体上分为两层："><a href="#网络大体上分为两层：" class="headerlink" title="网络大体上分为两层："></a>网络大体上分为两层：</h3><h4 id="Projection-layer"><a href="#Projection-layer" class="headerlink" title="Projection layer"></a>Projection layer</h4><p>To bridge 3D visual tokens(F) with the LLM’s tokenized space<br>FC(1030, 768)-&gt;GELU-&gt;FC(768,768)</p>
<h4 id="LLM"><a href="#LLM" class="headerlink" title="LLM"></a>LLM</h4><p>Llama-2-7b as the foundational LLM backbone</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="Stage-1-Pretraining-for-Feature-Alignment"><a href="#Stage-1-Pretraining-for-Feature-Alignment" class="headerlink" title="Stage 1: Pretraining for Feature Alignment"></a>Stage 1: Pretraining for Feature Alignment</h4><p>在两个坐标系统（camera和世界坐标）下使用3D帧数据，以确保场景-LLM理解以自我为中心和以场景为中心的观点。<br>在此阶段，仅训练了projection layer，可以有效地对齐具有文本特征的3D视觉特征，同时保持LLM参数（φ）不变。</p>
<h4 id="Stage-2-Finetuning"><a href="#Stage-2-Finetuning" class="headerlink" title="Stage 2: Finetuning"></a>Stage 2: Finetuning</h4><p>优化Scene-llm，以准确响应用户说明。我们使用标识符令牌“我看到”将3D帧语言和3D场景语言数据合并到前言。文本描述分为指令（$T_{INST}$）及其相应的响应（$T_{ANS}$）。利用转换后的3D视觉令牌（$T_{3D}$）和指令令牌（$T_{INST}$），我们的目标是微调LLM（φ）以自动生成$T_{ANS}$.<br>在这里，我们共同微调了投影层和LLM，由θ&#x3D; {ψ，φ}表示</p>
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/02/13/%5BOBS%5DReconstruct%20Anything-3D-LLM/"><img class="fill" src="/gallery/LLM.png" alt="3D-LLM" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-13T06:05:05.000Z" title="2/13/2025, 2:05:05 PM">2025-02-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.628Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">3 minutes read (About 505 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/02/13/%5BOBS%5DReconstruct%20Anything-3D-LLM/">3D-LLM</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/02/13/[OBS]Reconstruct Anything-3D-LLM/Pasted_image_20250214112500.png" title="" title=" class="gallery-item"><img src="/2025/02/13/[OBS]Reconstruct Anything-3D-LLM/Pasted_image_20250214112500.png" alt="" title=""></a></div>

<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Recent works have explored aligning images and videos with LLM for a new generation of multi-modal LLMs that equip LLMs with the ability to understand and reason about <strong>2D images</strong>.<br>但是仍缺少对于3D物理空间进行分析的模型, which involves richer concepts such as spatial relationships, affordances, physics and interaction so on.</p>
<p>由此提出了<code>inject the 3D world into large language models</code>, 介绍一个全新的3D-llm模型族，可以将3D表示（即带有功能的3D点云）作为输入，并执行一系列与3D相关的任务。<br>优势：</p>
<ul>
<li>关于整个场景的长期记忆可以存储在整体3D表示中，而不是情节的部分视图观测值</li>
<li>3D属性（如提供和空间关系）可以从3D表示形式中进行推论，远远超出了基于语言或基于2D图像的LLM的范围</li>
</ul>
<p>挑战</p>
<ul>
<li>数据获取：3D数据的稀缺性阻碍了基于3D的基础模型的发展。 3D数据与语言描述配对甚至更难获得<ul>
<li>提出了一组独特的数据生成管道，这些管道可以生成大规模的3D数据与语言配对。</li>
</ul>
</li>
<li>Obtain meaningful 3D features that could align with language features for 3D-LLMs: 一种方法是使用类似的对比性范式从头开始训练3D编码，以在2D图像和语言之间对齐。但是，该范式消耗了巨大的数据，时间和GPU资源。<ul>
<li>使用了一个3D功能提取器，该提取器构造了渲染的多视图图像的2D预处理特征的3D功能。最近，还使用了2D预训练的CLIP特征来训练其VLMS，也有很多视觉语言模型（例如Blip-2，Flamingo）。由于我们提取的3D功能与2D预处理的功能相同，因此我们可以无缝使用2D VLM作为骨架，并输入3D功能，以进行3D-LLM的有效训练。</li>
</ul>
</li>
</ul>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2></div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2025/01/06/%5BOBS%5DReconstruct%20Anything-OK-Robot-%20What%20Really%20Matters%20in%20Integrating%20Open-Knowledge%20%20Models%20for%20Robotics/"><img class="fill" src="/gallery/LLM.png" alt="OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-01-06T05:59:55.000Z" title="1/6/2025, 1:59:55 PM">2025-01-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.667Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Note/">Note</a></span><span class="level-item">6 minutes read (About 959 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/01/06/%5BOBS%5DReconstruct%20Anything-OK-Robot-%20What%20Really%20Matters%20in%20Integrating%20Open-Knowledge%20%20Models%20for%20Robotics/">OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><div class="post-content"><a href="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106151641.png" title="" title=" class="gallery-item"><img src="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106151641.png" alt="" title=""></a></div>

<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p><em>Creating a general-purpose robot has been a longstanding dream of the robotics community.</em></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>当前想要实现这一目标的系统脆弱、封闭，并且在遇到未见过的情况时会失败。即使是最大的机器人模型通常也只能部署在以前见过的环境中 [5, 6]。在机器人数据很少的环境中，例如在非结构化的家庭环境中，这些系统的脆弱性会进一步加剧。</p>
<p>虽然大型视觉模型显示出语义理解 、检测以及将视觉表示与语言联系起来的能力并且与此同时，机器人的导航、抓取和重新排列等基本机器人技能已经相当成熟。<br>但是将现代视觉模型与机器人特定基元相结合的机器人系统表现非常差。</p>
<p>这可能是因为单纯将多个不确定性的系统组合在一起会导致准确率急剧恶化。<br>所以我们需要一个将VLM和机器人primitives(导航，抓取，放置)结合在一起的细致框架，即OK-Robot。</p>
<h2 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h2><ul>
<li><strong>预训练的 VLM 对于开放词汇导航非常有效</strong>: 当前的开放词汇视觉语言模型，例如 CLIP 或 OWL-ViT，在识别现实世界中的任意对象方面提供了强大的性能，并能够以零样本的方式导航到它们。</li>
<li><strong>预训练的抓取模型可以直接应用于移动操作</strong>：与 VLM 类似，经过大量数据预训练的专用机器人模型可以立即应用于家庭中的开放词汇抓取。这些机器人模型不需要任何额外的训练或微调。</li>
<li><strong>如何组合组件至关重要</strong>：给定预训练模型，我们发现可以使用简单的状态机模型将它们组合在一起，无需训练。我们还发现，使用启发式方法来抵消机器人的物理限制可以在现实世界中获得更高的成功率。</li>
<li><strong>仍然存在一些挑战</strong>：虽然，考虑到在任意家庭中进行零样本的巨大挑战，OK-Robot 在之前的工作基础上进行了改进，通过分析故障模式，我们发现 VLM、机器人模型和机器人形态可以进行重大改进，这将直接提高开放知识操纵代理的性能。</li>
</ul>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="该框架主要完成的任务"><a href="#该框架主要完成的任务" class="headerlink" title="该框架主要完成的任务"></a>该框架主要完成的任务</h3><p>Pick up A (from B) and drop it on&#x2F;in C”, where A is an object and B and C are places in a real-world environment such as homes</p>
<h3 id="Open-home-open-vocabulary-object-navigation"><a href="#Open-home-open-vocabulary-object-navigation" class="headerlink" title="Open-home, open-vocabulary object navigation"></a>Open-home, open-vocabulary object navigation</h3><p>负责空间重建，识别物体大致位置，机器人导航<br>用到的方法:</p>
<ul>
<li>CLIP-Fields [[CLIP-Fields- Weakly Supervised Semantic Fields for Robotic Memory]] : a RGB-D video of the home -&gt; a sequence of posed ( with camera pose and positions) RGB-D images，用于重建环境，该研究还基于此获取了环境中物体和容器旁边的地板表面。</li>
<li>OWL-ViT [[Simple Open-Vocabulary Object Detection with Vision Transformers]] : 我们在每一帧上应用检测器，并提取每个对象边界框、CLIP-embedding、检测器置信度，并将这些信息传递到object memory模块中</li>
<li>SAM: 用于将ViT的检测框转化为mask</li>
<li>VoxcelMap: similar to object-centric memory of CLIP-Fields [[CLIP-Fields- Weakly Supervised Semantic Fields for Robotic Memory]], 基于点云中每一个点的CLIP semantic vector,每一个5cm的体素都包含一个CLIP-embedding的detector-confidence weighted average.</li>
<li>Querying the memory module: 先将language query 转化成CLIP semantic vector,然后基于voxelmap的clip-embeding，寻找最语义接近的那个voxel，以此定位。</li>
<li><div class="post-content"><a href="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106151051.png" title="" title=" class="gallery-item"><img src="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106151051.png" alt="" title=""></a></div></li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><div class="post-content"><a href="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106160701.png" title="" title=" class="gallery-item"><img src="/2025/01/06/[OBS]Reconstruct Anything-OK-Robot- What Really Matters in Integrating Open-Knowledge  Models for Robotics/Pasted_image_20250106160701.png" alt="" title=""></a></div>








</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2024/12/17/%5BOBS%5DReconstruct%20Anything-Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%20Mobile%20Manipulation/"><img class="fill" src="/gallery/LLM.png" alt="Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation" referrerpolicy="no-referrer"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-12-17T07:41:09.000Z" title="12/17/2024, 3:41:09 PM">2024-12-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-05-08T03:23:19.627Z" title="5/8/2025, 11:23:19 AM">2025-05-08</time></span><span class="level-item"><a class="link-muted" href="/categories/Review/">Review</a></span><span class="level-item">a few seconds read (About 42 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/12/17/%5BOBS%5DReconstruct%20Anything-Dynamic%20Open-Vocabulary%203D%20Scene%20Graphs%20for%20Long-term%20Language-Guided%20Mobile%20Manipulation/">Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation</a></p><div class="content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/css/lightgallery.min.css" /><div class=".article-gallery"><p>和我的想法非常相近，完成度也很高啊喂。可以参考他的实现思路，引用的文章等等。</p>
<div class="post-content"><a href="/2024/12/17/[OBS]Reconstruct Anything-Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation/Pasted_image_20250106151754.png" title="" title=" class="gallery-item"><img src="/2024/12/17/[OBS]Reconstruct Anything-Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation/Pasted_image_20250106151754.png" alt="" title=""></a></div>
## Intro
</div><script src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.4.0/dist/js/lightgallery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-fullscreen.js@1.2.0/dist/lg-fullscreen.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.3.0/dist/lg-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-autoplay.js@1.2.0/dist/lg-autoplay.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-video.js@1.3.0/dist/lg-video.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-hash.js@1.0.0/dist/lg-hash.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lg-pager.js@1.0.0/dist/lg-pager.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script></div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/tags/Robotics/page/0/">Previous</a></div><div class="pagination-next"><a href="/tags/Robotics/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/tags/Robotics/">1</a></li><li><a class="pagination-link" href="/tags/Robotics/page/2/">2</a></li></ul></nav></div><style>.column.column-left,.column.column-right{display:none}</style><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Chen Yulin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chen Yulin</p><p class="is-size-6 is-block">SJTU student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Manchester by the Sea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">258</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">186</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Chen-Yulin" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">May 2025</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/04/"><span class="level-start"><span class="level-item">April 2025</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/03/"><span class="level-start"><span class="level-item">March 2025</span></span><span class="level-end"><span class="level-item tag">45</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/02/"><span class="level-start"><span class="level-item">February 2025</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">January 2025</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">December 2024</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">46</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/2025/05/07/%5BOBS%5Dlinux-Write%20Latex%20in%20Neovim%20on%20Archlinux/"><img src="/thumb/Linux.png" alt="Write Latex in Neovim on Archlinux"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-06T18:23:49.000Z">2025-05-07</time></p><p class="title"><a href="/2025/05/07/%5BOBS%5Dlinux-Write%20Latex%20in%20Neovim%20on%20Archlinux/">Write Latex in Neovim on Archlinux</a></p><p class="categories"><a href="/categories/Note/">Note</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/07/%5BOBS%5Dlinux-Davinci-resolve%20on%20Archlinux/"><img src="/thumb/Linux.png" alt="Davinci-resolve on Archlinux"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-06T17:50:51.000Z">2025-05-07</time></p><p class="title"><a href="/2025/05/07/%5BOBS%5Dlinux-Davinci-resolve%20on%20Archlinux/">Davinci-resolve on Archlinux</a></p><p class="categories"><a href="/categories/Note/">Note</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/06/%5BOBS%5DDeep%20Learning-CV-Deformable%20Convolutional%20Networks/"><img src="/thumb/Research-paper.png" alt="Deformable Convolutional Networks"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-06T03:55:36.000Z">2025-05-06</time></p><p class="title"><a href="/2025/05/06/%5BOBS%5DDeep%20Learning-CV-Deformable%20Convolutional%20Networks/">Deformable Convolutional Networks</a></p><p class="categories"><a href="/categories/Review/">Review</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-05-01T11:53:16.000Z">2025-05-01</time></p><p class="title"><a href="/2025/05/01/%5BOBS%5D2025%20Summer%20Schedule/">2025 Summer Schedule</a></p><p class="categories"><a href="/categories/Schedule/">Schedule</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/04/24/%5BOBS%5DDeep%20Learning-CV-Associative%20Embedding=%20End-to-End%20Learning%20for%20Joint%20Detection%20and%20Grouping/"><img src="/thumb/Research-paper.png" alt="Associative Embedding= End-to-End Learning for Joint Detection and Grouping"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-04-24T03:00:20.000Z">2025-04-24</time></p><p class="title"><a href="/2025/04/24/%5BOBS%5DDeep%20Learning-CV-Associative%20Embedding=%20End-to-End%20Learning%20for%20Joint%20Detection%20and%20Grouping/">Associative Embedding= End-to-End Learning for Joint Detection and Grouping</a></p><p class="categories"><a href="/categories/Review/">Review</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3D-Scene/"><span class="tag">3D-Scene</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/6-D/"><span class="tag">6-D</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AIGC/"><span class="tag">AIGC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AR/"><span class="tag">AR</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Academic/"><span class="tag">Academic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Aliyun/"><span class="tag">Aliyun</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/App/"><span class="tag">App</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BS4/"><span class="tag">BS4</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Beautify/"><span class="tag">Beautify</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Behaviorism/"><span class="tag">Behaviorism</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Business/"><span class="tag">Business</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CADC/"><span class="tag">CADC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CD/"><span class="tag">CD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CLIP/"><span class="tag">CLIP</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">26</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Capstone/"><span class="tag">Capstone</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Communication/"><span class="tag">Communication</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Contrastive-Learning/"><span class="tag">Contrastive-Learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control/"><span class="tag">Control</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Csharp/"><span class="tag">Csharp</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Css/"><span class="tag">Css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cuda/"><span class="tag">Cuda</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DD/"><span class="tag">DD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DINO/"><span class="tag">DINO</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DT/"><span class="tag">DT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dataframe/"><span class="tag">Dataframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Debate/"><span class="tag">Debate</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Debugger/"><span class="tag">Debugger</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Diffusion/"><span class="tag">Diffusion</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Discrete-Mathematics/"><span class="tag">Discrete-Mathematics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docs/"><span class="tag">Docs</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dynamic-programming/"><span class="tag">Dynamic-programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ESP32/"><span class="tag">ESP32</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Education/"><span class="tag">Education</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embeded-System/"><span class="tag">Embeded-System</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embodied-AI/"><span class="tag">Embodied-AI</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emoation/"><span class="tag">Emoation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Emotion/"><span class="tag">Emotion</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ethic/"><span class="tag">Ethic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FL/"><span class="tag">FL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Family/"><span class="tag">Family</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Federated-Learning/"><span class="tag">Federated-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Foundation/"><span class="tag">Foundation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Functional-programming/"><span class="tag">Functional programming</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Game/"><span class="tag">Game</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Gated-NN/"><span class="tag">Gated-NN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Github/"><span class="tag">Github</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Godot/"><span class="tag">Godot</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HPC/"><span class="tag">HPC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/HRI/"><span class="tag">HRI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Haskell/"><span class="tag">Haskell</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Health/"><span class="tag">Health</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hierarchical/"><span class="tag">Hierarchical</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Html/"><span class="tag">Html</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Humanism/"><span class="tag">Humanism</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hyprland/"><span class="tag">Hyprland</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IK/"><span class="tag">IK</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-Grounding/"><span class="tag">Image-Grounding</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-Text/"><span class="tag">Image-Text</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Image-generation/"><span class="tag">Image-generation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ImitationLearning/"><span class="tag">ImitationLearning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Jolt/"><span class="tag">Jolt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Json/"><span class="tag">Json</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSP/"><span class="tag">LSP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Latex/"><span class="tag">Latex</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life/"><span class="tag">Life</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearAlgebra/"><span class="tag">LinearAlgebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Live2d/"><span class="tag">Live2d</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Love/"><span class="tag">Love</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Lua/"><span class="tag">Lua</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MBTI/"><span class="tag">MBTI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MR-AR/"><span class="tag">MR/AR</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mason/"><span class="tag">Mason</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Meme/"><span class="tag">Meme</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Message-Passing/"><span class="tag">Message-Passing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mod/"><span class="tag">Mod</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Motivation/"><span class="tag">Motivation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Movie/"><span class="tag">Movie</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-modal/"><span class="tag">Multi-modal</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-view/"><span class="tag">Multi-view</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Music/"><span class="tag">Music</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NN/"><span class="tag">NN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Network/"><span class="tag">Network</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nodejs/"><span class="tag">Nodejs</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Numpy/"><span class="tag">Numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nvim/"><span class="tag">Nvim</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-Detection/"><span class="tag">Object-Detection</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Open-Vocabulary/"><span class="tag">Open-Vocabulary</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenCV/"><span class="tag">OpenCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Oral/"><span class="tag">Oral</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PHD/"><span class="tag">PHD</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PSY/"><span class="tag">PSY</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pandas/"><span class="tag">Pandas</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Panoptic/"><span class="tag">Panoptic</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Path/"><span class="tag">Path</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Philosophy/"><span class="tag">Philosophy</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PhysX/"><span class="tag">PhysX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Physical-Scene/"><span class="tag">Physical-Scene</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Physics-engine/"><span class="tag">Physics-engine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pio/"><span class="tag">Pio</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Planning/"><span class="tag">Planning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Plugin/"><span class="tag">Plugin</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PoseEstimation/"><span class="tag">PoseEstimation</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Postgraduate/"><span class="tag">Postgraduate</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Prefab/"><span class="tag">Prefab</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Probability/"><span class="tag">Probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">26</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/QML/"><span class="tag">QML</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Quantum/"><span class="tag">Quantum</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS/"><span class="tag">ROS</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reading/"><span class="tag">Reading</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real2Sim/"><span class="tag">Real2Sim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reconstruct/"><span class="tag">Reconstruct</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regex/"><span class="tag">Regex</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-learning/"><span class="tag">Reinforcement-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Repository/"><span class="tag">Repository</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Representation-Learning/"><span class="tag">Representation-Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research-paper/"><span class="tag">Research-paper</span><span class="tag">85</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robot/"><span class="tag">Robot</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Robotics/"><span class="tag">Robotics</span><span class="tag">16</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SJTU-Lecture/"><span class="tag">SJTU-Lecture</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SSH/"><span class="tag">SSH</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scene-graph/"><span class="tag">Scene-graph</span><span class="tag">29</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scene-synthesis/"><span class="tag">Scene-synthesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Science-fiction/"><span class="tag">Science-fiction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Scrap/"><span class="tag">Scrap</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Script/"><span class="tag">Script</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Segmentation/"><span class="tag">Segmentation</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Semantic/"><span class="tag">Semantic</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shader/"><span class="tag">Shader</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Shell/"><span class="tag">Shell</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Signals-and-Systems/"><span class="tag">Signals and Systems</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sim2Real/"><span class="tag">Sim2Real</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sklearn/"><span class="tag">Sklearn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Snippets/"><span class="tag">Snippets</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Society/"><span class="tag">Society</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Star-rail/"><span class="tag">Star-rail</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Subgraph/"><span class="tag">Subgraph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Submodule/"><span class="tag">Submodule</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Supervised-learning/"><span class="tag">Supervised-learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Survey/"><span class="tag">Survey</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TC/"><span class="tag">TC</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TOEFL/"><span class="tag">TOEFL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Task-Planning/"><span class="tag">Task-Planning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tasks/"><span class="tag">Tasks</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tech-Communication/"><span class="tag">Tech Communication</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Torch/"><span class="tag">Torch</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Translation-Embedding/"><span class="tag">Translation-Embedding</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Travel/"><span class="tag">Travel</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unity/"><span class="tag">Unity</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-learning/"><span class="tag">Unsupervised-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLM/"><span class="tag">VLM</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VLP/"><span class="tag">VLP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Version-management/"><span class="tag">Version-management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ViT/"><span class="tag">ViT</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoEditing/"><span class="tag">VideoEditing</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vim/"><span class="tag">Vim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Visual-Relation/"><span class="tag">Visual-Relation</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WSL/"><span class="tag">WSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Waybar/"><span class="tag">Waybar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wayland/"><span class="tag">Wayland</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web/"><span class="tag">Web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Website/"><span class="tag">Website</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Well-being/"><span class="tag">Well-being</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Window-manager/"><span class="tag">Window-manager</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YKLL/"><span class="tag">YKLL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Zen/"><span class="tag">Zen</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%F0%9F%90%B1/"><span class="tag">🐱</span><span class="tag">1</span></a></div></div></div></div></div></div><style>.column.column-left,.column.column-right{display:block}</style></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/img/cyllogo.png" alt="Chen Yulin&#039;s Blog" height="28"><img class="logo-img-dark" src="/img/cyllogonight.png" alt="Chen Yulin&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 Chen Yulin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chen-Yulin"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script><script type="text/javascript" src="/js/imaegoo/falling-petals.js"></script><!-- hexo injector body_end start -->
<link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.css" />
<script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.11/index.js"></script>
<script>
  window.CHATBOT_CONFIG = {
    endpoint: "https://webchat-bot-iqu-knzhgrvznd.cn-hangzhou.fcapp.run/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
    displayByDefault: false, // 默认不展示 AI 助手聊天框
    aiChatOptions: { // aiChatOptions 中 options 会传递 aiChat 组件，自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat
      conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
        conversationStarters: [
          {prompt: '你是谁？'},
          {prompt: '博主又是谁？'},
          {prompt: '怎么使用这个网站？'},
          {prompt: '想要博主联系方式！'},
        ]
      },
      displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
        height: 600,
        width: 350,
      },
      personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
        assistant: {          name: '博主的AI助手，十四行诗参上！',
          // AI 助手的图标
          avatar: 'https://chen-yulin.github.io/thumb/14.png',
          tagline: '要不要试试问下面的问题呢？',
        }
      }
    }
  };
</script>
<style>
  :root {
    /* webchat 工具栏的颜色 */
    --webchat-toolbar-background-color: #1464E4;
    /* webchat 工具栏文字和按钮的颜色 */
    --webchat-toolbar-text-color: #FFF;
  }
  /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、left 等设置来调整位置 */
  .webchat-container {
    z-index: 100;
    bottom: 10px;
    right: 10px;
  }
  /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、left 等设置来调整位置 */
  .webchat-bubble-tip {
    z-index: 99;
    bottom: 60px;
    right: 20px;
  }
  .webchat-bubble-tip {
    overflow: visible !important;
  }
  @keyframes float {
    0% {
      transform: translateY(0px) translateX(-50%);
    }
    50% {
      transform: translateY(-10px) translateX(-50%);
    }
    100% {
      transform: translateY(0px) translateX(-50%);
    }
  }

  .webchat-bubble-tip::before {
    content: '';
    position: absolute;
    top: -25px;
    left: 70%;
    width: 40px;
    height: 40px;
    background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='white'%3E%3Cpath d='M20 2H4c-1.1 0-2 .9-2 2v18l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm0 14H6l-2 2V4h16v12z'/%3E%3C/svg%3E");
    background-repeat: no-repeat;
    background-position: center;
    background-size: contain;
    filter: drop-shadow(0 4px 6px rgba(0, 0, 0, 0.5));
    animation: float 3s ease-in-out infinite;
  }
</style><script data-pjax src="https://registry.npmmirror.com/oh-my-live2d/latest/files"></script><script>const oml2d = OML2D.loadOml2d({libraryUrls:{"complete":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/complete.js","cubism2":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism2.js","cubism5":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism5.js"},dockedPosition:"left",mobileDisplay:true,models:[{"path":"https://model.hacxy.cn/mai/model.json","mobilePosition":[25,0],"mobileScale":0.1,"mobileStageStyle":{"width":125,"height":175},"motionPreloadStrategy":"ALL","position":[50,0],"scale":0.2,"stageStyle":{"width":250,"height":350}}],parentElement:document.body,primaryColor:"var(--btn-bg)",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body></html>